{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TrainCheck: Training with Confidence TrainCheck is a lightweight tool for proactively catching silent errors in deep learning training runs. It detects correctness issues, such as code bugs and faulty hardware, early and pinpoints their root cause. TrainCheck has detected silent errors in a wide range of real-world training scenarios, from large-scale LLM pretraining (such as BLOOM-176B) to small-scale tutorial runs by deep learning beginners. \ud83d\udccc For a list of successful cases, see our Success Stories . What It Does TrainCheck uses training invariants , which are semantic rules that describe expected behavior during training, to detect bugs as they happen. These invariants can be extracted from any correct run, including those produced by official examples and tutorials. There is no need to curate inputs or write manual assertions. TrainCheck performs three core functions: Instruments your training code Inserts lightweight tracing into existing scripts (such as pytorch/examples or transformers ) with minimal code changes. Learns invariants from correct runs Discovers expected relationships across APIs, tensors, and training steps to build a model of normal behavior. Checks new or modified runs Validates behavior against the learned invariants and flags silent errors, such as missing gradient clipping, weight desynchronization, or broken mixed precision, right when they occur. This picture illustrates the TrainCheck workflow: Under the hood, TrainCheck decomposes into three CLI tools: - Instrumentor ( traincheck-collect ) Wraps target training programs with lightweight tracing logic. It produces an instrumented version of the target program that logs API calls and model states without altering training semantics. - Inference Engine ( traincheck-infer ) Consumes one or more trace logs from successful runs to infer training invariants. - Checker ( traincheck-check ) Runs alongside or after new training jobs to verify that each recorded event satisfies the inferred invariants. \ud83d\udd25 Try TrainCheck Work through 5\u2011Minute Experience with TrainCheck . You\u2019ll learn how to: - Instrument a training script and collect a trace - Automatically infer invariants - Uncover silent bugs in the training script Documentation Installation Guide Usage Guide: Scenarios and Limitations TrainCheck Technical Doc TrainCheck Dev RoadMap Status TrainCheck is under active development. Please join our \ud83d\udcac Discord server or file a GitHub issue for support. We welcome feedback and contributions from early adopters. Contributing We welcome and value any contributions and collaborations. Please check out Contributing to TrainCheck for how to get involved. License TrainCheck is licensed under the Apache License 2.0 . Citation If TrainCheck is relevant to your work, please cite our paper: @inproceedings{TrainCheckOSDI2025, author = {Jiang, Yuxuan and Zhou, Ziming and Xu, Boyu and Liu, Beijie and Xu, Runhui and Huang, Peng}, title = {Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks}, booktitle = {Proceedings of the 19th USENIX Symposium on Operating Systems Design and Implementation}, series = {OSDI '25}, month = {July}, year = {2025}, address = {Boston, MA, USA}, publisher = {USENIX Association}, } Artifact Evaluation \ud83d\udd75\ufe0f\u200d\u2640\ufe0f OSDI AE members, please see TrainCheck AE Guide .","title":"Home"},{"location":"#what-it-does","text":"TrainCheck uses training invariants , which are semantic rules that describe expected behavior during training, to detect bugs as they happen. These invariants can be extracted from any correct run, including those produced by official examples and tutorials. There is no need to curate inputs or write manual assertions. TrainCheck performs three core functions: Instruments your training code Inserts lightweight tracing into existing scripts (such as pytorch/examples or transformers ) with minimal code changes. Learns invariants from correct runs Discovers expected relationships across APIs, tensors, and training steps to build a model of normal behavior. Checks new or modified runs Validates behavior against the learned invariants and flags silent errors, such as missing gradient clipping, weight desynchronization, or broken mixed precision, right when they occur. This picture illustrates the TrainCheck workflow: Under the hood, TrainCheck decomposes into three CLI tools: - Instrumentor ( traincheck-collect ) Wraps target training programs with lightweight tracing logic. It produces an instrumented version of the target program that logs API calls and model states without altering training semantics. - Inference Engine ( traincheck-infer ) Consumes one or more trace logs from successful runs to infer training invariants. - Checker ( traincheck-check ) Runs alongside or after new training jobs to verify that each recorded event satisfies the inferred invariants.","title":"What It Does"},{"location":"#try-traincheck","text":"Work through 5\u2011Minute Experience with TrainCheck . You\u2019ll learn how to: - Instrument a training script and collect a trace - Automatically infer invariants - Uncover silent bugs in the training script","title":"\ud83d\udd25 Try TrainCheck"},{"location":"#documentation","text":"Installation Guide Usage Guide: Scenarios and Limitations TrainCheck Technical Doc TrainCheck Dev RoadMap","title":"Documentation"},{"location":"#status","text":"TrainCheck is under active development. Please join our \ud83d\udcac Discord server or file a GitHub issue for support. We welcome feedback and contributions from early adopters.","title":"Status"},{"location":"#contributing","text":"We welcome and value any contributions and collaborations. Please check out Contributing to TrainCheck for how to get involved.","title":"Contributing"},{"location":"#license","text":"TrainCheck is licensed under the Apache License 2.0 .","title":"License"},{"location":"#citation","text":"If TrainCheck is relevant to your work, please cite our paper: @inproceedings{TrainCheckOSDI2025, author = {Jiang, Yuxuan and Zhou, Ziming and Xu, Boyu and Liu, Beijie and Xu, Runhui and Huang, Peng}, title = {Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks}, booktitle = {Proceedings of the 19th USENIX Symposium on Operating Systems Design and Implementation}, series = {OSDI '25}, month = {July}, year = {2025}, address = {Boston, MA, USA}, publisher = {USENIX Association}, }","title":"Citation"},{"location":"#artifact-evaluation","text":"\ud83d\udd75\ufe0f\u200d\u2640\ufe0f OSDI AE members, please see TrainCheck AE Guide .","title":"Artifact Evaluation"},{"location":"5-min-tutorial/","text":"Quick Start: TrainCheck Tutorial In this tutorial, you will use TrainCheck to detect & diagnose the real\u2011world silent issue in PyTorch\u2011Forum\u201184911: Obtaining abnormal changes in loss and accuracy , with invariants inferred from PyTorch\u2019s official MNIST example. We\u2019ll refer to this buggy pipeline simply as '84911'. Estimated time : ~5 minutes (plus model/inference overhead) Prerequisites - A working TrainCheck installation - efficientnet_pytorch (install via pip3 install efficientnet_pytorch ) - A Linux machine with a CUDA\u2011enabled GPU - \ud83d\udca1 Tip: If you don\u2019t have a CUDA GPU, you can still run this tutorial on CPU\u2014it\u2019ll just take longer. Background: What\u2019s wrong with 84911? The author attempts to finetune a pretrained EfficientNet_b0 model for image classification but notices that\u2014even after many epochs\u2014the training loss barely improves (x\u2011axis = epoch, y\u2011axis = loss): It appears from the plot that the model is still being trained, but somehow it is just not improving meaningfully. The original issue post discussed adjusting learning rate, and training for longer epochs. However, the issue remained unresolved. We have diagnosed the root cause for you. You can look at it now or come at it yourself with the help of TrainCheck. Click here to reveal the root cause. The developer, for some reason, sets `requires_grad` to `False` for all parameters except for batch normalization layers, yet only initializes the optimizer with the final fully-connected layer. for name,param in model_transfer.module.named_parameters(): if(\"bn\" not in name): param.requires_grad = False for param in model_transfer.module._fc.parameters(): param.requires_grad = False ... optimizer_transfer = optim.Adam(model_transfer.module._fc.parameters(), lr=0.001) This freeze logic leaves virtually no trainable parameters. Since batch normalization layers still update their running mean/variance each forward pass, the loss/accuracy curves drift slightly instead of remaining flat\u2014masking the lack of actual learning. Logging metrics only once per epoch further hides the anomalies, so the initialization bug only becomes apparent after several epochs have already run. Detecting & Diagnosing 84911 We will infer invariants from the mnist.py, a very simple PyTorch-official pipeline that trains a 2-layer CNN on MNIST, to showcase TrainCheck's capability. 1. Download example scripts cd ~ mkdir traincheck-tutorial && cd traincheck-tutorial wget https://raw.githubusercontent.com/OrderLab/traincheck/main/docs/assets/code/mnist.py wget https://raw.githubusercontent.com/OrderLab/traincheck/main/docs/assets/code/84911.py \ud83d\udca1 If the wget links above fail (e.g. due to branch changes or access issues), you can also download the files manually from: - mnist.py - 84911.py 2. Instrument & collect trace from mnist.py (~1 minute) traincheck-collect \\ --pyscript mnist.py \\ --models-to-track model \\ --output-dir traincheck_mnist_trace This instruments torch and model in mnist.py , runs it with default arguments, and writes JSON trace files into traincheck_mnist_trace/ (\u2248 1 minute). You\u2019ll see the training logs and any benign PyTorch warnings on stdout. 3. Infer Invariants from mnist.py (~1-4 minutes) We will infer invariants from the trace we just collected using the command below. traincheck-infer -f ./traincheck_mnist_trace This will produce an invariants.json file (one JSON Line per invariant). Verify the count: wc -l invariants.json # should output ~913 The generated invariants capture API invocation order, event expectations, and input-output relationships. Since the trace comes from a single, simple script, some invariants may overfit\u2014we\u2019ll cover filtering in the next steps. 4. Check for silent issues in 84911.py with invariants (~5-10 minutes) Note : For this quickstart, we do offline checking for simplicity. # trace the buggy pipeline (~5 minutes) traincheck-collect \\ --pyscript 84911.py \\ --models-to-track model_transfer \\ --output-dir traincheck_84911_trace # run the checker (~2\u20136 minutes) traincheck-check \\ --trace-folders traincheck_84911_trace \\ --invariants invariants.json The output of the traincheck-check command will contain this in the end: Checking finished. 913 invariants checked Total failed invariants: 25/913 Total passed invariants: 888/913 # number here includes both passed and not triggered invariants Total invariants that are not triggered: 552/913 361 invariants were checked on 84911.py , and 25 got violated. The checker writes the full results to a folder named traincheck_checker_results_<timestamp> , containing the results ( failed.log , not_triggered.log , passed.log , depending if an invariant is violated, not checked at all, or checked and passed.), and a copy of invariants.json . 5. Detection & Diagnosis Ready to play detective? \ud83d\udd0d TrainCheck flagged 25 invariant violations right at the start of training\u2014well before the fluctuating loss/accuracy pattern was observed. Let\u2019s interpret the results first and then if you want to learn more. 1. Quick filter - Event\u2011order invariants noise (20/25 failures): - FunctionCoverRelation and FunctionLeadRelation invariants (basically specifying API invocation orders) overfit our single demo trace. - Examples: strict ordering of torch.distributed.is_initialized (6 invariants violated but we are not even doing distributed training in 84911!) or torch.cuda.is_initialized (another 7 invariants violated but shouldn't matter at all for training). - Ignore these . 2. Spot the real issues - APIContainRelation invariant violations (5/25): 1. Optimizer.zero_grad did not reset .grad from non-zero to zero/None. - Implies either no gradients were ever populated or zeroing silently failed. 2. Adadelta.step did not update .data of any parameters. - Indicates the optimizer had no trainable parameters to touch. \ud83e\udde9 Putting it all together: The optimizer wasn\u2019t updating anything because\u2026 the parameters it received had requires_grad=False. Go to Background: What\u2019s wrong in 84911? to see the full root cause confirmed and explained. \ud83d\ude4b Click here to learn how to inspect the raw results Open the `failed_*.log` file\u2014TrainCheck writes each violated invariant as a standalone JSON object. For example: { \"invariant\": { \u2026 }, \"check_passed\": false, \"triggered\": true, \"detection_time\": 18343040207314524, \"detection_time_percentage\": 0.1805434802294184, \"trace\": [ { \"func_call_id\": \"...\", \"meta_vars.step\": 1, \"function\": \"torch.optim.optimizer.Optimizer.zero_grad\", \u2026 } ... ] } - `\"invariant\"` shows the invariant that this result correspond to, and - `\"trace\"` corresponds to the specific trace that caused the violation. - `\"check_passed\": false` means that the invariant has been violated. - `\"triggered\": true` means that the invariant has been checked at least once, which is always the case if the invariant is violated. - `\"detection_time\"` is the timestamp when the violation happened. - `\"detection_percentage\"` is the percentage of this timestamp in the entire duration of the training, and gives a rough impression of how early the detection is. We are working on providing a field `\"detection_step\"` that pinpoints on which step the issue is detected. For now, to get \"step\", you can look at the `\"trace\"` field and look for step numbers in `\"meta_vars\"`. For example, the \"`optimizer.zero_grad` did **not** reset `.grad` from non-zero to zero/None\" is represented as: { \"invariant\": { \"relation\": \"APIContainRelation\", \"params\": [ { \"param_type\": \"APIParam\", \"api_full_name\": \"torch.optim.optimizer.Optimizer.zero_grad\" }, { \"param_type\": \"VarTypeParam\", \"var_type\": \"torch.nn.Parameter\", \"attr_name\": \"grad\", \"pre_value\": \"non_zero\", \"post_value\": null } ], \"precondition\": { \"parent_func_call_pre\": { \"inverted\": true, \"preconditions\": [ { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.step\", \"additional_path\": \"None\", \"prop_dtype\": \"int\", \"values\": [ 0 ] } ] }, { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.stage\", \"additional_path\": \"None\", \"prop_dtype\": \"str\", \"values\": [ \"testing\", \"init\" ] } ] } ] } }, \"num_positive_examples\": 20, \"num_negative_examples\": 1 }, \"check_passed\": false, \"triggered\": true, \"detection_time\": 18343039144178123, \"detection_time_percentage\": 0.16245728748900484, \"trace\": [ { \"func_call_id\": \"3f7265b362c34725b412cf693ceea8f3_18343039144122325\", \"thread_id\": 140156043466560, \"process_id\": 1263911, \"meta_vars.step\": 1, \"type\": \"function_call (pre)\", \"function\": \"torch.optim.optimizer.Optimizer.zero_grad\", \"is_bound_method\": true, \"obj_id\": 140152527083248, \"args\": { \"0\": { \"torch.optim.adadelta.Adadelta\": {} } }, \"kwargs\": {}, \"time\": 18343039144178123, \"return_values\": NaN, \"var_name\": NaN, \"var_type\": NaN, \"mode\": NaN, \"dump_loc\": NaN, \"attributes._ML_DAIKON_data_ID\": NaN, \"attributes.data\": NaN, \"attributes.dtype\": NaN, \"attributes.grad\": NaN, \"attributes.grad_fn\": NaN, \"attributes.is_cpu\": NaN, \"attributes.is_cuda\": NaN, \"attributes.is_ipu\": NaN, \"attributes.is_leaf\": NaN, \"attributes.is_meta\": NaN, \"attributes.is_mkldnn\": NaN, \"attributes.is_mps\": NaN, \"attributes.is_mtia\": NaN, \"attributes.is_nested\": NaN, \"attributes.is_ort\": NaN, \"attributes.is_quantized\": NaN, \"attributes.is_sparse\": NaN, \"attributes.is_sparse_csr\": NaN, \"attributes.is_vulkan\": NaN, \"attributes.is_xla\": NaN, \"attributes.is_xpu\": NaN, \"attributes.itemsize\": NaN, \"attributes.name\": NaN, \"attributes.nbytes\": NaN, \"attributes.ndim\": NaN, \"attributes.requires_grad\": NaN, \"attributes.retains_grad\": NaN, \"attributes.shape\": NaN, \"attributes._ML_DAIKON_grad_ID\": NaN, \"exception\": NaN, \"exception_msg\": NaN, \"proxy_obj_names\": NaN } ] } The invariant specifies that `torch.optim.optimizer.Optimizer.zero_grad` (*the first invariant parameter*) invocations must change `.grad` from a non-zero value to `null` (*the second invariant parameter*), except during the very first iteration (*i.e. before any backward pass when no `.grad` exists, as per the invariant precondition*). We then inspect the trace record where the invariant is violated: `meta_vars.step` is 1, indicating detection occurred in the second training iteration. You can review the other results in the same way. The `NaN` values denote missing fields and can be safely ignored. \ud83c\udf89 You just used TrainCheck to catch a real-world silent bug before it impacted training!","title":"5 Minute Quick Start"},{"location":"5-min-tutorial/#quick-start-traincheck-tutorial","text":"In this tutorial, you will use TrainCheck to detect & diagnose the real\u2011world silent issue in PyTorch\u2011Forum\u201184911: Obtaining abnormal changes in loss and accuracy , with invariants inferred from PyTorch\u2019s official MNIST example. We\u2019ll refer to this buggy pipeline simply as '84911'. Estimated time : ~5 minutes (plus model/inference overhead) Prerequisites - A working TrainCheck installation - efficientnet_pytorch (install via pip3 install efficientnet_pytorch ) - A Linux machine with a CUDA\u2011enabled GPU - \ud83d\udca1 Tip: If you don\u2019t have a CUDA GPU, you can still run this tutorial on CPU\u2014it\u2019ll just take longer.","title":"Quick Start: TrainCheck Tutorial"},{"location":"5-min-tutorial/#background-whats-wrong-with-84911","text":"The author attempts to finetune a pretrained EfficientNet_b0 model for image classification but notices that\u2014even after many epochs\u2014the training loss barely improves (x\u2011axis = epoch, y\u2011axis = loss): It appears from the plot that the model is still being trained, but somehow it is just not improving meaningfully. The original issue post discussed adjusting learning rate, and training for longer epochs. However, the issue remained unresolved. We have diagnosed the root cause for you. You can look at it now or come at it yourself with the help of TrainCheck. Click here to reveal the root cause. The developer, for some reason, sets `requires_grad` to `False` for all parameters except for batch normalization layers, yet only initializes the optimizer with the final fully-connected layer. for name,param in model_transfer.module.named_parameters(): if(\"bn\" not in name): param.requires_grad = False for param in model_transfer.module._fc.parameters(): param.requires_grad = False ... optimizer_transfer = optim.Adam(model_transfer.module._fc.parameters(), lr=0.001) This freeze logic leaves virtually no trainable parameters. Since batch normalization layers still update their running mean/variance each forward pass, the loss/accuracy curves drift slightly instead of remaining flat\u2014masking the lack of actual learning. Logging metrics only once per epoch further hides the anomalies, so the initialization bug only becomes apparent after several epochs have already run.","title":"Background: What\u2019s wrong with 84911?"},{"location":"5-min-tutorial/#detecting-diagnosing-84911","text":"We will infer invariants from the mnist.py, a very simple PyTorch-official pipeline that trains a 2-layer CNN on MNIST, to showcase TrainCheck's capability.","title":"Detecting &amp; Diagnosing 84911"},{"location":"5-min-tutorial/#1-download-example-scripts","text":"cd ~ mkdir traincheck-tutorial && cd traincheck-tutorial wget https://raw.githubusercontent.com/OrderLab/traincheck/main/docs/assets/code/mnist.py wget https://raw.githubusercontent.com/OrderLab/traincheck/main/docs/assets/code/84911.py \ud83d\udca1 If the wget links above fail (e.g. due to branch changes or access issues), you can also download the files manually from: - mnist.py - 84911.py","title":"1. Download example scripts"},{"location":"5-min-tutorial/#2-instrument-collect-trace-from-mnistpy-1-minute","text":"traincheck-collect \\ --pyscript mnist.py \\ --models-to-track model \\ --output-dir traincheck_mnist_trace This instruments torch and model in mnist.py , runs it with default arguments, and writes JSON trace files into traincheck_mnist_trace/ (\u2248 1 minute). You\u2019ll see the training logs and any benign PyTorch warnings on stdout.","title":"2. Instrument &amp; collect trace from mnist.py (~1 minute)"},{"location":"5-min-tutorial/#3-infer-invariants-from-mnistpy-1-4-minutes","text":"We will infer invariants from the trace we just collected using the command below. traincheck-infer -f ./traincheck_mnist_trace This will produce an invariants.json file (one JSON Line per invariant). Verify the count: wc -l invariants.json # should output ~913 The generated invariants capture API invocation order, event expectations, and input-output relationships. Since the trace comes from a single, simple script, some invariants may overfit\u2014we\u2019ll cover filtering in the next steps.","title":"3. Infer Invariants from mnist.py (~1-4 minutes)"},{"location":"5-min-tutorial/#4-check-for-silent-issues-in-84911py-with-invariants-5-10-minutes","text":"Note : For this quickstart, we do offline checking for simplicity. # trace the buggy pipeline (~5 minutes) traincheck-collect \\ --pyscript 84911.py \\ --models-to-track model_transfer \\ --output-dir traincheck_84911_trace # run the checker (~2\u20136 minutes) traincheck-check \\ --trace-folders traincheck_84911_trace \\ --invariants invariants.json The output of the traincheck-check command will contain this in the end: Checking finished. 913 invariants checked Total failed invariants: 25/913 Total passed invariants: 888/913 # number here includes both passed and not triggered invariants Total invariants that are not triggered: 552/913 361 invariants were checked on 84911.py , and 25 got violated. The checker writes the full results to a folder named traincheck_checker_results_<timestamp> , containing the results ( failed.log , not_triggered.log , passed.log , depending if an invariant is violated, not checked at all, or checked and passed.), and a copy of invariants.json .","title":"4. Check for silent issues in 84911.py with invariants (~5-10 minutes)"},{"location":"5-min-tutorial/#5-detection-diagnosis","text":"Ready to play detective? \ud83d\udd0d TrainCheck flagged 25 invariant violations right at the start of training\u2014well before the fluctuating loss/accuracy pattern was observed. Let\u2019s interpret the results first and then if you want to learn more. 1. Quick filter - Event\u2011order invariants noise (20/25 failures): - FunctionCoverRelation and FunctionLeadRelation invariants (basically specifying API invocation orders) overfit our single demo trace. - Examples: strict ordering of torch.distributed.is_initialized (6 invariants violated but we are not even doing distributed training in 84911!) or torch.cuda.is_initialized (another 7 invariants violated but shouldn't matter at all for training). - Ignore these . 2. Spot the real issues - APIContainRelation invariant violations (5/25): 1. Optimizer.zero_grad did not reset .grad from non-zero to zero/None. - Implies either no gradients were ever populated or zeroing silently failed. 2. Adadelta.step did not update .data of any parameters. - Indicates the optimizer had no trainable parameters to touch. \ud83e\udde9 Putting it all together: The optimizer wasn\u2019t updating anything because\u2026 the parameters it received had requires_grad=False. Go to Background: What\u2019s wrong in 84911? to see the full root cause confirmed and explained. \ud83d\ude4b Click here to learn how to inspect the raw results Open the `failed_*.log` file\u2014TrainCheck writes each violated invariant as a standalone JSON object. For example: { \"invariant\": { \u2026 }, \"check_passed\": false, \"triggered\": true, \"detection_time\": 18343040207314524, \"detection_time_percentage\": 0.1805434802294184, \"trace\": [ { \"func_call_id\": \"...\", \"meta_vars.step\": 1, \"function\": \"torch.optim.optimizer.Optimizer.zero_grad\", \u2026 } ... ] } - `\"invariant\"` shows the invariant that this result correspond to, and - `\"trace\"` corresponds to the specific trace that caused the violation. - `\"check_passed\": false` means that the invariant has been violated. - `\"triggered\": true` means that the invariant has been checked at least once, which is always the case if the invariant is violated. - `\"detection_time\"` is the timestamp when the violation happened. - `\"detection_percentage\"` is the percentage of this timestamp in the entire duration of the training, and gives a rough impression of how early the detection is. We are working on providing a field `\"detection_step\"` that pinpoints on which step the issue is detected. For now, to get \"step\", you can look at the `\"trace\"` field and look for step numbers in `\"meta_vars\"`. For example, the \"`optimizer.zero_grad` did **not** reset `.grad` from non-zero to zero/None\" is represented as: { \"invariant\": { \"relation\": \"APIContainRelation\", \"params\": [ { \"param_type\": \"APIParam\", \"api_full_name\": \"torch.optim.optimizer.Optimizer.zero_grad\" }, { \"param_type\": \"VarTypeParam\", \"var_type\": \"torch.nn.Parameter\", \"attr_name\": \"grad\", \"pre_value\": \"non_zero\", \"post_value\": null } ], \"precondition\": { \"parent_func_call_pre\": { \"inverted\": true, \"preconditions\": [ { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.step\", \"additional_path\": \"None\", \"prop_dtype\": \"int\", \"values\": [ 0 ] } ] }, { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.stage\", \"additional_path\": \"None\", \"prop_dtype\": \"str\", \"values\": [ \"testing\", \"init\" ] } ] } ] } }, \"num_positive_examples\": 20, \"num_negative_examples\": 1 }, \"check_passed\": false, \"triggered\": true, \"detection_time\": 18343039144178123, \"detection_time_percentage\": 0.16245728748900484, \"trace\": [ { \"func_call_id\": \"3f7265b362c34725b412cf693ceea8f3_18343039144122325\", \"thread_id\": 140156043466560, \"process_id\": 1263911, \"meta_vars.step\": 1, \"type\": \"function_call (pre)\", \"function\": \"torch.optim.optimizer.Optimizer.zero_grad\", \"is_bound_method\": true, \"obj_id\": 140152527083248, \"args\": { \"0\": { \"torch.optim.adadelta.Adadelta\": {} } }, \"kwargs\": {}, \"time\": 18343039144178123, \"return_values\": NaN, \"var_name\": NaN, \"var_type\": NaN, \"mode\": NaN, \"dump_loc\": NaN, \"attributes._ML_DAIKON_data_ID\": NaN, \"attributes.data\": NaN, \"attributes.dtype\": NaN, \"attributes.grad\": NaN, \"attributes.grad_fn\": NaN, \"attributes.is_cpu\": NaN, \"attributes.is_cuda\": NaN, \"attributes.is_ipu\": NaN, \"attributes.is_leaf\": NaN, \"attributes.is_meta\": NaN, \"attributes.is_mkldnn\": NaN, \"attributes.is_mps\": NaN, \"attributes.is_mtia\": NaN, \"attributes.is_nested\": NaN, \"attributes.is_ort\": NaN, \"attributes.is_quantized\": NaN, \"attributes.is_sparse\": NaN, \"attributes.is_sparse_csr\": NaN, \"attributes.is_vulkan\": NaN, \"attributes.is_xla\": NaN, \"attributes.is_xpu\": NaN, \"attributes.itemsize\": NaN, \"attributes.name\": NaN, \"attributes.nbytes\": NaN, \"attributes.ndim\": NaN, \"attributes.requires_grad\": NaN, \"attributes.retains_grad\": NaN, \"attributes.shape\": NaN, \"attributes._ML_DAIKON_grad_ID\": NaN, \"exception\": NaN, \"exception_msg\": NaN, \"proxy_obj_names\": NaN } ] } The invariant specifies that `torch.optim.optimizer.Optimizer.zero_grad` (*the first invariant parameter*) invocations must change `.grad` from a non-zero value to `null` (*the second invariant parameter*), except during the very first iteration (*i.e. before any backward pass when no `.grad` exists, as per the invariant precondition*). We then inspect the trace record where the invariant is violated: `meta_vars.step` is 1, indicating detection occurred in the second training iteration. You can review the other results in the same way. The `NaN` values denote missing fields and can be safely ignored. \ud83c\udf89 You just used TrainCheck to catch a real-world silent bug before it impacted training!","title":"5. Detection &amp; Diagnosis"},{"location":"ae-eval-s5.1-silent-issue-detection/","text":"Eval: Silent Issue Detection \u23f3 Estimated Completion Time : ~30 minutes \ud83c\udfaf Goal TrainCheck detects 18 real-world silent issues in our evaluation. Your goal in this artifact evaluation is to verify detection for the subset of issues that are currently AE-supported (see bug table below). For each supported bug, you should confirm: \u2705 TrainCheck successfully detects the issue by reporting one or more invariant violations on the provided trace. The artifact provides all necessary resources to automate this confirmation. Additional insights\u2014such as when the issue is triggered and how the violation aligns with the root cause\u2014can be explored by examining the scripts, logs, or violation reports, though they are not required for core validation. \ud83d\udcc2 Resources Provided All files are located in the TrainCheck-Evaluation-Workloads repository. Resource Description Curated Invariants Small set of known-effective invariants per bug. Pre-collected Traces Captured execution traces from the buggy pipelines. Silent Issue Reproduction Scripts and Descriptions https://github.com/OrderLab/TrainCheck-Evaluation-Workloads/tree/main/silent-issue-detection/bug-reprod-scripts \ud83d\udc1b Silent Issue Summary Table Bug ID Failure Location AE? AE Limitation (if any) baichuan2-86 HW/Driver \u2705 Yes Similar root cause as pytorch-84803, reuses pytorch-104336 trace deepspeed-1801 Framework \u2705 Yes deepspeed-5794 Framework \u274c No Invariant relation still under evaluation lightning-thunder-725 Framework \u2705 Yes mmpretrain-702 Framework \u2705 Yes pytorch-51800 Framework \u2705 Yes pytorch-84803 HW/Driver \u2705 Yes Different root cause, but low-level manifest is similar, reuses pytorch-104336 trace pytorch-96600 HW/Driver \u2705 Yes Similar root cause as pytorch-84803 reuses pytorch-104336 trace pytorch-104336 Framework \u2705 Yes pytorch-115607 Compiler \u2705 Yes pytorch-forum-84911 User Code \u2705 Yes stackoverflow-60335387 User Code \u2705 Yes stackoverflow-67180955 Framework \u274c No Requires older Python version no longer supported transformers-17877 Framework \u2705 Yes transformers-23723 Framework \u2705 Yes transformers-33844 Framework \u2705 Yes transformers-34204 Framework \u274c No Invariant support still in progress x-jxmnop-ddp-out-of-sync User Code \u2705 Yes Reuses pytorch-104336 trace We currently support 15 out of 18 bugs for artifact evaluation. You have already detected pytorch-forum-84911 in our 5-min tutorial. You will need to detect the rest of the 14 bugs. Bugs not included in this AE release typically depend on: - Unsupported or unstable library versions - Very old Python environments - Invariant support still in development Additionally, a few bugs stem from very specific issues such as faulty hardware, which are inherently difficult to reproduce. For such cases\u2014and for bugs that share the same root cause/manifest\u2014we may provide a shared/simulated trace and a shared invariant that is reused across multiple bug IDs. \ud83e\uddea Reproducing Silent Issue Detection All steps described below assumes you are already in the TrainCheck-Evaluation-Workloads repo. If not, clone the repository and go to it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide . Execute ae_detection.sh to automatically apply invariants to the pre-collected trace. This script generates results into a folder named checker_output . bash cd silent-issue-detection bash ae_detection.sh Expected Results The checker_output folder contains checkering results for each trace. (base) \u279c checker_output git:(main) tree . . \u251c\u2500\u2500 invariants.json \u251c\u2500\u2500 trace_deepspeed-1801 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_mmpretrain-702 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_pytorch-104336 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_pytorch-115607 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_pytorch-51800 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_stackoverflow-60335387 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_transformers-17877 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_transformers-23723 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_transformers-33844 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u2514\u2500\u2500 trace_x-jxmnop-ddp-out-of-sync \u251c\u2500\u2500 failed.log \u251c\u2500\u2500 not_triggered.log \u2514\u2500\u2500 passed.log You want to make sure all failed.log files are non-empty , indicating silent errors detected in these traces. Optionally, we provided a reference_checker_output folder containing the expected detection results. You can compare the checking results you get versus our reference results. When you do this comparison, do keep in mind that your results and our reference results will not be exactly the same, as TrainCheck does not enforce (1) the order of checking when multiple checkable entities are available at the same time, which may lead to invariants violated on different processes or different pairs of variables, (2) the order of fields when serializing checker results. Thus, if you do the comparison in a mechanical way (e.g. via diff -r checker_output reference_checker_output ), you might see outputs like this. The differences are expected and benign. (base) \u279c silent-issue-detection git:(main) diff -r checker_output reference_checker_output diff --color -r checker_output/trace_pytorch-104336/failed.log reference_checker_output/trace_pytorch-104336/failed.log 75,76c75,76 < \"detection_time\": 2437523672783, < \"detection_time_percentage\": 0.11841431018590723, --- > \"detection_time\": 2437539087694, > \"detection_time_percentage\": 0.11851643004648255, 81,82c81,82 < \"process_id\": 9539, < \"thread_id\": 140711397648192, --- > \"process_id\": 9591, > \"thread_id\": 140324043503424, 86c86 < \"attributes._ML_DAIKON_data_ID\": 140704882109040, --- > \"attributes._ML_DAIKON_data_ID\": 140317529048544, 116,117c116,117 < \"time\": 2437523672783, < \"meta_vars._DATA_PARALLEL_RANK\": 4.0, --- > \"time\": 2437504805077, > \"meta_vars._DATA_PARALLEL_RANK\": 5.0, 123,124c123,124 < \"process_id\": 9429, < \"thread_id\": 140050208577344, --- > \"process_id\": 9747, > \"thread_id\": 140028492969792, 128c128 < \"attributes._ML_DAIKON_data_ID\": 140043703504144, --- > \"attributes._ML_DAIKON_data_ID\": 140021978318304, 158,159c158,159 < \"time\": 2437502499438, < \"meta_vars._DATA_PARALLEL_RANK\": 2.0, --- > \"time\": 2437539087694, > \"meta_vars._DATA_PARALLEL_RANK\": 7.0, diff --color -r checker_output/trace_pytorch-115607/failed.log reference_checker_output/trace_pytorch-115607/failed.log 43,44c43,44 < \"init\", < \"testing\" --- > \"testing\", > \"init\" 78,80d77 < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 113c110,113 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN 180,182d179 < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 215c212,215 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN 261,262c261,262 < \"init\", < \"testing\" --- > \"testing\", > \"init\" 296,298d295 < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 331c328,331 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN diff --color -r checker_output/trace_pytorch-51800/failed.log reference_checker_output/trace_pytorch-51800/failed.log 34,56d33 < \"func_call_id\": \"b39a4a81b2c24473ba916ab1832fbf12_19876858668012869\", < \"thread_id\": 140254285555520, < \"process_id\": 3499981, < \"meta_vars.step\": 0, < \"type\": \"function_call (pre)\", < \"function\": \"torch.nn.modules.module.Module.eval\", < \"is_bound_method\": true, < \"obj_id\": 140250960790096, < \"args\": { < \"0\": { < \"__main__.SimpleCNN\": { < \"call_super_init\": false, < \"dump_patches\": false, < \"training\": true < } < } < }, < \"kwargs\": {}, < \"time\": 19876858668088743, < \"return_values\": NaN, < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 60a38,41 > \"process_id\": 3499981, > \"thread_id\": 140254285555520, > \"time\": 19876858668088743, > \"meta_vars.step\": 0, 89c70,89 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"type\": \"function_call (pre)\", > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"func_call_id\": \"b39a4a81b2c24473ba916ab1832fbf12_19876858668012869\", > \"function\": \"torch.nn.modules.module.Module.eval\", > \"is_bound_method\": true, > \"obj_id\": 140250960790096, > \"args\": { > \"0\": { > \"__main__.SimpleCNN\": { > \"call_super_init\": false, > \"dump_patches\": false, > \"training\": true > } > } > }, > \"kwargs\": {}, > \"return_values\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN diff --color -r checker_output/trace_transformers-33844/failed.log reference_checker_output/trace_transformers-33844/failed.log 244,246d243 < \"enabled\": { < \"bool\": true < }, 250a248,250 > \"bool\": true > }, > \"enabled\": { diff --color -r checker_output/trace_x-jxmnop-ddp-out-of-sync/failed.log reference_checker_output/trace_x-jxmnop-ddp-out-of-sync/failed.log 81,82c81,82 < \"process_id\": 89557, < \"thread_id\": 140661207828288, --- > \"process_id\": 89558, > \"thread_id\": 140625926412096, 85c85 < \"meta_vars._DATA_PARALLEL_RANK\": \"0\", --- > \"meta_vars._DATA_PARALLEL_RANK\": \"1\", 87c87 < \"attributes._ML_DAIKON_data_ID\": 140656561409856, --- > \"attributes._ML_DAIKON_data_ID\": 140621279056480, 117c117 < \"time\": 123297988837864, --- > \"time\": 123299970638648, 123,124c123,124 < \"process_id\": 89558, < \"thread_id\": 140625926412096, --- > \"process_id\": 89557, > \"thread_id\": 140661207828288, 127c127 < \"meta_vars._DATA_PARALLEL_RANK\": \"1\", --- > \"meta_vars._DATA_PARALLEL_RANK\": \"0\", 129c129 < \"attributes._ML_DAIKON_data_ID\": 140621279058160, --- > \"attributes._ML_DAIKON_data_ID\": 140656561411776, 159c159 < \"time\": 123299970638648, --- > \"time\": 123297988837864,","title":"Eval: Silent Issue Detection"},{"location":"ae-eval-s5.1-silent-issue-detection/#eval-silent-issue-detection","text":"\u23f3 Estimated Completion Time : ~30 minutes","title":"Eval: Silent Issue Detection"},{"location":"ae-eval-s5.1-silent-issue-detection/#goal","text":"TrainCheck detects 18 real-world silent issues in our evaluation. Your goal in this artifact evaluation is to verify detection for the subset of issues that are currently AE-supported (see bug table below). For each supported bug, you should confirm: \u2705 TrainCheck successfully detects the issue by reporting one or more invariant violations on the provided trace. The artifact provides all necessary resources to automate this confirmation. Additional insights\u2014such as when the issue is triggered and how the violation aligns with the root cause\u2014can be explored by examining the scripts, logs, or violation reports, though they are not required for core validation.","title":"\ud83c\udfaf Goal"},{"location":"ae-eval-s5.1-silent-issue-detection/#resources-provided","text":"All files are located in the TrainCheck-Evaluation-Workloads repository. Resource Description Curated Invariants Small set of known-effective invariants per bug. Pre-collected Traces Captured execution traces from the buggy pipelines. Silent Issue Reproduction Scripts and Descriptions https://github.com/OrderLab/TrainCheck-Evaluation-Workloads/tree/main/silent-issue-detection/bug-reprod-scripts","title":"\ud83d\udcc2 Resources Provided"},{"location":"ae-eval-s5.1-silent-issue-detection/#silent-issue-summary-table","text":"Bug ID Failure Location AE? AE Limitation (if any) baichuan2-86 HW/Driver \u2705 Yes Similar root cause as pytorch-84803, reuses pytorch-104336 trace deepspeed-1801 Framework \u2705 Yes deepspeed-5794 Framework \u274c No Invariant relation still under evaluation lightning-thunder-725 Framework \u2705 Yes mmpretrain-702 Framework \u2705 Yes pytorch-51800 Framework \u2705 Yes pytorch-84803 HW/Driver \u2705 Yes Different root cause, but low-level manifest is similar, reuses pytorch-104336 trace pytorch-96600 HW/Driver \u2705 Yes Similar root cause as pytorch-84803 reuses pytorch-104336 trace pytorch-104336 Framework \u2705 Yes pytorch-115607 Compiler \u2705 Yes pytorch-forum-84911 User Code \u2705 Yes stackoverflow-60335387 User Code \u2705 Yes stackoverflow-67180955 Framework \u274c No Requires older Python version no longer supported transformers-17877 Framework \u2705 Yes transformers-23723 Framework \u2705 Yes transformers-33844 Framework \u2705 Yes transformers-34204 Framework \u274c No Invariant support still in progress x-jxmnop-ddp-out-of-sync User Code \u2705 Yes Reuses pytorch-104336 trace We currently support 15 out of 18 bugs for artifact evaluation. You have already detected pytorch-forum-84911 in our 5-min tutorial. You will need to detect the rest of the 14 bugs. Bugs not included in this AE release typically depend on: - Unsupported or unstable library versions - Very old Python environments - Invariant support still in development Additionally, a few bugs stem from very specific issues such as faulty hardware, which are inherently difficult to reproduce. For such cases\u2014and for bugs that share the same root cause/manifest\u2014we may provide a shared/simulated trace and a shared invariant that is reused across multiple bug IDs.","title":"\ud83d\udc1b Silent Issue Summary Table"},{"location":"ae-eval-s5.1-silent-issue-detection/#reproducing-silent-issue-detection","text":"All steps described below assumes you are already in the TrainCheck-Evaluation-Workloads repo. If not, clone the repository and go to it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide . Execute ae_detection.sh to automatically apply invariants to the pre-collected trace. This script generates results into a folder named checker_output . bash cd silent-issue-detection bash ae_detection.sh","title":"\ud83e\uddea Reproducing Silent Issue Detection"},{"location":"ae-eval-s5.1-silent-issue-detection/#expected-results","text":"The checker_output folder contains checkering results for each trace. (base) \u279c checker_output git:(main) tree . . \u251c\u2500\u2500 invariants.json \u251c\u2500\u2500 trace_deepspeed-1801 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_mmpretrain-702 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_pytorch-104336 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_pytorch-115607 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_pytorch-51800 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_stackoverflow-60335387 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_transformers-17877 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_transformers-23723 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u251c\u2500\u2500 trace_transformers-33844 \u2502 \u251c\u2500\u2500 failed.log \u2502 \u251c\u2500\u2500 not_triggered.log \u2502 \u2514\u2500\u2500 passed.log \u2514\u2500\u2500 trace_x-jxmnop-ddp-out-of-sync \u251c\u2500\u2500 failed.log \u251c\u2500\u2500 not_triggered.log \u2514\u2500\u2500 passed.log You want to make sure all failed.log files are non-empty , indicating silent errors detected in these traces. Optionally, we provided a reference_checker_output folder containing the expected detection results. You can compare the checking results you get versus our reference results. When you do this comparison, do keep in mind that your results and our reference results will not be exactly the same, as TrainCheck does not enforce (1) the order of checking when multiple checkable entities are available at the same time, which may lead to invariants violated on different processes or different pairs of variables, (2) the order of fields when serializing checker results. Thus, if you do the comparison in a mechanical way (e.g. via diff -r checker_output reference_checker_output ), you might see outputs like this. The differences are expected and benign. (base) \u279c silent-issue-detection git:(main) diff -r checker_output reference_checker_output diff --color -r checker_output/trace_pytorch-104336/failed.log reference_checker_output/trace_pytorch-104336/failed.log 75,76c75,76 < \"detection_time\": 2437523672783, < \"detection_time_percentage\": 0.11841431018590723, --- > \"detection_time\": 2437539087694, > \"detection_time_percentage\": 0.11851643004648255, 81,82c81,82 < \"process_id\": 9539, < \"thread_id\": 140711397648192, --- > \"process_id\": 9591, > \"thread_id\": 140324043503424, 86c86 < \"attributes._ML_DAIKON_data_ID\": 140704882109040, --- > \"attributes._ML_DAIKON_data_ID\": 140317529048544, 116,117c116,117 < \"time\": 2437523672783, < \"meta_vars._DATA_PARALLEL_RANK\": 4.0, --- > \"time\": 2437504805077, > \"meta_vars._DATA_PARALLEL_RANK\": 5.0, 123,124c123,124 < \"process_id\": 9429, < \"thread_id\": 140050208577344, --- > \"process_id\": 9747, > \"thread_id\": 140028492969792, 128c128 < \"attributes._ML_DAIKON_data_ID\": 140043703504144, --- > \"attributes._ML_DAIKON_data_ID\": 140021978318304, 158,159c158,159 < \"time\": 2437502499438, < \"meta_vars._DATA_PARALLEL_RANK\": 2.0, --- > \"time\": 2437539087694, > \"meta_vars._DATA_PARALLEL_RANK\": 7.0, diff --color -r checker_output/trace_pytorch-115607/failed.log reference_checker_output/trace_pytorch-115607/failed.log 43,44c43,44 < \"init\", < \"testing\" --- > \"testing\", > \"init\" 78,80d77 < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 113c110,113 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN 180,182d179 < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 215c212,215 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN 261,262c261,262 < \"init\", < \"testing\" --- > \"testing\", > \"init\" 296,298d295 < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 331c328,331 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN diff --color -r checker_output/trace_pytorch-51800/failed.log reference_checker_output/trace_pytorch-51800/failed.log 34,56d33 < \"func_call_id\": \"b39a4a81b2c24473ba916ab1832fbf12_19876858668012869\", < \"thread_id\": 140254285555520, < \"process_id\": 3499981, < \"meta_vars.step\": 0, < \"type\": \"function_call (pre)\", < \"function\": \"torch.nn.modules.module.Module.eval\", < \"is_bound_method\": true, < \"obj_id\": 140250960790096, < \"args\": { < \"0\": { < \"__main__.SimpleCNN\": { < \"call_super_init\": false, < \"dump_patches\": false, < \"training\": true < } < } < }, < \"kwargs\": {}, < \"time\": 19876858668088743, < \"return_values\": NaN, < \"exception\": NaN, < \"exception_msg\": NaN, < \"proxy_obj_names\": NaN, 60a38,41 > \"process_id\": 3499981, > \"thread_id\": 140254285555520, > \"time\": 19876858668088743, > \"meta_vars.step\": 0, 89c70,89 < \"attributes._ML_DAIKON_grad_ID\": NaN --- > \"type\": \"function_call (pre)\", > \"attributes._ML_DAIKON_grad_ID\": NaN, > \"func_call_id\": \"b39a4a81b2c24473ba916ab1832fbf12_19876858668012869\", > \"function\": \"torch.nn.modules.module.Module.eval\", > \"is_bound_method\": true, > \"obj_id\": 140250960790096, > \"args\": { > \"0\": { > \"__main__.SimpleCNN\": { > \"call_super_init\": false, > \"dump_patches\": false, > \"training\": true > } > } > }, > \"kwargs\": {}, > \"return_values\": NaN, > \"exception\": NaN, > \"exception_msg\": NaN, > \"proxy_obj_names\": NaN diff --color -r checker_output/trace_transformers-33844/failed.log reference_checker_output/trace_transformers-33844/failed.log 244,246d243 < \"enabled\": { < \"bool\": true < }, 250a248,250 > \"bool\": true > }, > \"enabled\": { diff --color -r checker_output/trace_x-jxmnop-ddp-out-of-sync/failed.log reference_checker_output/trace_x-jxmnop-ddp-out-of-sync/failed.log 81,82c81,82 < \"process_id\": 89557, < \"thread_id\": 140661207828288, --- > \"process_id\": 89558, > \"thread_id\": 140625926412096, 85c85 < \"meta_vars._DATA_PARALLEL_RANK\": \"0\", --- > \"meta_vars._DATA_PARALLEL_RANK\": \"1\", 87c87 < \"attributes._ML_DAIKON_data_ID\": 140656561409856, --- > \"attributes._ML_DAIKON_data_ID\": 140621279056480, 117c117 < \"time\": 123297988837864, --- > \"time\": 123299970638648, 123,124c123,124 < \"process_id\": 89558, < \"thread_id\": 140625926412096, --- > \"process_id\": 89557, > \"thread_id\": 140661207828288, 127c127 < \"meta_vars._DATA_PARALLEL_RANK\": \"1\", --- > \"meta_vars._DATA_PARALLEL_RANK\": \"0\", 129c129 < \"attributes._ML_DAIKON_data_ID\": 140621279058160, --- > \"attributes._ML_DAIKON_data_ID\": 140656561411776, 159c159 < \"time\": 123299970638648, --- > \"time\": 123297988837864,","title":"Expected Results"},{"location":"ae-eval-s5.3-transferability/","text":"Eval: Transferability \u23f3 Estimated Completion Time : 40 minutes - Environment Setup: ~10 minutes - Trace Collection: ~10 minutes - Invariant Inference: ~20 minutes \ud83c\udfaf Goal This evaluation measures the transferability of invariants inferred by TrainCheck across library versions and training environments. The results to be reproduced correspond to the final paragraph of Section 5.3 of the paper. Other claims in Section 5.3\u2014specifically, that invariants inferred from reference pipelines can detect all known bugs\u2014are validated as part of the Silent Issue Detection Evaluation . \ud83d\udcc2 Resources & Scripts Automation Script : transferability/ae_transferability.sh Runs the full transferability evaluation pipeline described in Section 5.3 of the paper. It executes invariant inference, applies inferred invariants to other pipelines, and collects applicability (invariant should be checked and not cause false alarms) statistics. transferability/install-traincheck-torch251-cu121.sh Creates a conda environment named traincheck-torch251 with Python 3.10 and installs TrainCheck from the latest GitHub version. transferability/install-traincheck-torch251-cu118.sh Same as above but installs the CUDA 118 version of PyTorch 2.5.1. This evaluation uses the GCN training pipeline from PyTorch's official examples, tested across different PyTorch versions. The pipeline is included in the artifact repository and will be automatically handled by the script\u2014no manual setup is required. \ud83d\udee0 How to Run Go to TrainCheck-Evaluation-Workloads/transferability . Clone the repo if you do not have it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads/transferability Create a new conda environment named traincheck-torch251 , and install PyTorch 2.5.1 along with TrainCheck. Run the appropriate script based on your GPU's CUDA compatibility (likely executing either will be fine): bash bash install-traincheck-torch251-cu121.sh # for CUDA 12.1 or bash bash install-traincheck-torch251-cu118.sh # for CUDA 11.8 Run the transferability evaluation script: bash bash ae_transferability.sh This script will: - Collect traces from the GCN training pipeline using both PyTorch 2.2.2 and 2.5.1. - Infer invariants from the 2.2.2 version. - Apply them to the 2.5.1 trace to assess transferability. \u26a0\ufe0f Note: The scripts above assume that Conda is installed at ~/miniconda3 . If your installation is located elsewhere (e.g., ~/anaconda3 ), please modify the first line of the scripts to reflect your actual Conda path. We also assume that you have already installed TrainCheck in an environment named traincheck prior to running these scripts. \ud83e\uddd0 How to Verify the Results After the script finishes, it generates a file named applied_rates.csv that reports the percentage of applicable invariants. You should verify that the rate is no lower than the paper\u2019s reported value: \ud83d\udfe2 \"94.2% remain valid and applicable up to PyTorch 2.5.1\" (Section 5.3) \u26a0\ufe0f Notes & Troubleshooting If invariant inference or checking fails, please first verify that the environment is correctly set up (e.g., correct PyTorch version, dependencies installed). Then try re-running ae_transferability.py . If the issue persists, please contact us for assistance\u3002","title":"Eval: Transferability"},{"location":"ae-eval-s5.3-transferability/#eval-transferability","text":"\u23f3 Estimated Completion Time : 40 minutes - Environment Setup: ~10 minutes - Trace Collection: ~10 minutes - Invariant Inference: ~20 minutes","title":"Eval: Transferability"},{"location":"ae-eval-s5.3-transferability/#goal","text":"This evaluation measures the transferability of invariants inferred by TrainCheck across library versions and training environments. The results to be reproduced correspond to the final paragraph of Section 5.3 of the paper. Other claims in Section 5.3\u2014specifically, that invariants inferred from reference pipelines can detect all known bugs\u2014are validated as part of the Silent Issue Detection Evaluation .","title":"\ud83c\udfaf Goal"},{"location":"ae-eval-s5.3-transferability/#resources-scripts","text":"Automation Script : transferability/ae_transferability.sh Runs the full transferability evaluation pipeline described in Section 5.3 of the paper. It executes invariant inference, applies inferred invariants to other pipelines, and collects applicability (invariant should be checked and not cause false alarms) statistics. transferability/install-traincheck-torch251-cu121.sh Creates a conda environment named traincheck-torch251 with Python 3.10 and installs TrainCheck from the latest GitHub version. transferability/install-traincheck-torch251-cu118.sh Same as above but installs the CUDA 118 version of PyTorch 2.5.1. This evaluation uses the GCN training pipeline from PyTorch's official examples, tested across different PyTorch versions. The pipeline is included in the artifact repository and will be automatically handled by the script\u2014no manual setup is required.","title":"\ud83d\udcc2 Resources &amp; Scripts"},{"location":"ae-eval-s5.3-transferability/#how-to-run","text":"Go to TrainCheck-Evaluation-Workloads/transferability . Clone the repo if you do not have it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads/transferability Create a new conda environment named traincheck-torch251 , and install PyTorch 2.5.1 along with TrainCheck. Run the appropriate script based on your GPU's CUDA compatibility (likely executing either will be fine): bash bash install-traincheck-torch251-cu121.sh # for CUDA 12.1 or bash bash install-traincheck-torch251-cu118.sh # for CUDA 11.8 Run the transferability evaluation script: bash bash ae_transferability.sh This script will: - Collect traces from the GCN training pipeline using both PyTorch 2.2.2 and 2.5.1. - Infer invariants from the 2.2.2 version. - Apply them to the 2.5.1 trace to assess transferability. \u26a0\ufe0f Note: The scripts above assume that Conda is installed at ~/miniconda3 . If your installation is located elsewhere (e.g., ~/anaconda3 ), please modify the first line of the scripts to reflect your actual Conda path. We also assume that you have already installed TrainCheck in an environment named traincheck prior to running these scripts.","title":"\ud83d\udee0 How to Run"},{"location":"ae-eval-s5.3-transferability/#how-to-verify-the-results","text":"After the script finishes, it generates a file named applied_rates.csv that reports the percentage of applicable invariants. You should verify that the rate is no lower than the paper\u2019s reported value: \ud83d\udfe2 \"94.2% remain valid and applicable up to PyTorch 2.5.1\" (Section 5.3)","title":"\ud83e\uddd0 How to Verify the Results"},{"location":"ae-eval-s5.3-transferability/#notes-troubleshooting","text":"If invariant inference or checking fails, please first verify that the environment is correctly set up (e.g., correct PyTorch version, dependencies installed). Then try re-running ae_transferability.py . If the issue persists, please contact us for assistance\u3002","title":"\u26a0\ufe0f Notes &amp; Troubleshooting"},{"location":"ae-eval-s5.4-fp-rate/","text":"Eval: False Positive Rate \u23f3 Estimated Completion Time: 2 hour. - Trace Collection: ~10 minutes - Invariant Inference & Checking: ~1.5 hours \ud83c\udfaf Goal This evaluation measures the false positive rate of alarms reported by TrainCheck's invariants. The target results are discussed in the main text of Section 5.4 of the paper. \ud83d\udcc2 Resources & Scripts Automation Scripts : TrainCheck-Evaluation-Workloads/fp_rate/ae_fp.py : The script to collect traces, perform invariant inference, and check invariants on supposedly-correct programs to see if there are any false alarms. TrainCheck-Evaluation-Workloads/fp_rate/compute_fp_rate.py : The script to compute false positive rates from the invariant checking results. Workloads : The evaluation uses official PyTorch training pipelines located at TrainCheck-Evaluation-Workloads/fp_rate/workloads . We have shortened the training runs for faster execution. For AE purposes, you do not need to modify or understand the workload code\u2014 ae_fp.py will automatically handle the entire process. \ud83d\udee0 How to Run All steps described below assumes you are already in the TrainCheck-Evaluation-Workloads repo. If not, clone the repository and go to it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide . Install necessary dependencies for the false positive evaluation workloads. bash conda activate traincheck # change this if you installed TrainCheck in a different environment. cd fp_rate pip3 install -r requirements.txt Execute ae_fp.py to collect traces, perform invariant inference, and check the invariants on validation programs. The workload ddp-multigpu will need 2 GPUs. We have provided the trace for ddp-multigpu in case you do not have two GPUs. If you need to use our pre-computed trace for ddp-multigpu , remove the --overwrite-existing-results argument. bash python3 ae_fp.py --bench workloads Or, if you have a machine with 2 GPUs, execute the below command, such that the original results will be re-computed. bash python3 ae_fp.py --bench workloads --overwrite-existing-results Execute compute_fp_rate.py to compute the false positive rates. bash python3 compute_fp_rate.py \ud83d\udc40 What to Expect During Execution The ae_fp.py script is long running. It performs three tasks at same time. 1. It collects trace for all the workloads. 2. It infers invariants for three setups in Section 5.4. 3. It checks inferred invariants on the validation workloads. The experiments might fail if environment installation issues or disruption happens. When you run into problems, please refer to \u26a0\ufe0f Notes & Troubleshooting . \u26a0\ufe0f Notes & Troubleshooting The script will automatically detect any errors in any (1) trace collection, (2) inference tasks, (3) checking tasks. If you encounter any trace collection issues, please check for any missing environment dependencies. If you encounter any issues on invariant inference tasks or invariant checking tasks, please try to rerun the experiment by adding --overwrite-existing-results or delete all trace_* folders except for trace_ddp-multigpu . If you see persistent issues, it will likely be a environment issue or software bug. Please contact us for help. \ud83e\uddd0 How to verify the results? The compute_fp_rate.py script generates a file called fp_rates.csv under the current directory. Looking like this setup,fp_rate 1-input,0.3105 4-input,0.1127 6-input,0.1066 These values correspond to the results reported in Section 5.4 of the paper. You should verify that the false positive rates are similar or lower. Since the OSDI submission, we have fixed multiple bugs in TrainCheck, so the false positive rates are expected to be significantly lower in most cases. In our run of the script, we obtained the following results: setup,fp_rate 1-input,0.039 4-input,0.021 6-input,0.015","title":"Eval: False Positive Rate"},{"location":"ae-eval-s5.4-fp-rate/#eval-false-positive-rate","text":"\u23f3 Estimated Completion Time: 2 hour. - Trace Collection: ~10 minutes - Invariant Inference & Checking: ~1.5 hours","title":"Eval: False Positive Rate"},{"location":"ae-eval-s5.4-fp-rate/#goal","text":"This evaluation measures the false positive rate of alarms reported by TrainCheck's invariants. The target results are discussed in the main text of Section 5.4 of the paper.","title":"\ud83c\udfaf Goal"},{"location":"ae-eval-s5.4-fp-rate/#resources-scripts","text":"Automation Scripts : TrainCheck-Evaluation-Workloads/fp_rate/ae_fp.py : The script to collect traces, perform invariant inference, and check invariants on supposedly-correct programs to see if there are any false alarms. TrainCheck-Evaluation-Workloads/fp_rate/compute_fp_rate.py : The script to compute false positive rates from the invariant checking results. Workloads : The evaluation uses official PyTorch training pipelines located at TrainCheck-Evaluation-Workloads/fp_rate/workloads . We have shortened the training runs for faster execution. For AE purposes, you do not need to modify or understand the workload code\u2014 ae_fp.py will automatically handle the entire process.","title":"\ud83d\udcc2 Resources &amp; Scripts"},{"location":"ae-eval-s5.4-fp-rate/#how-to-run","text":"All steps described below assumes you are already in the TrainCheck-Evaluation-Workloads repo. If not, clone the repository and go to it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide . Install necessary dependencies for the false positive evaluation workloads. bash conda activate traincheck # change this if you installed TrainCheck in a different environment. cd fp_rate pip3 install -r requirements.txt Execute ae_fp.py to collect traces, perform invariant inference, and check the invariants on validation programs. The workload ddp-multigpu will need 2 GPUs. We have provided the trace for ddp-multigpu in case you do not have two GPUs. If you need to use our pre-computed trace for ddp-multigpu , remove the --overwrite-existing-results argument. bash python3 ae_fp.py --bench workloads Or, if you have a machine with 2 GPUs, execute the below command, such that the original results will be re-computed. bash python3 ae_fp.py --bench workloads --overwrite-existing-results Execute compute_fp_rate.py to compute the false positive rates. bash python3 compute_fp_rate.py","title":"\ud83d\udee0 How to Run"},{"location":"ae-eval-s5.4-fp-rate/#what-to-expect-during-execution","text":"The ae_fp.py script is long running. It performs three tasks at same time. 1. It collects trace for all the workloads. 2. It infers invariants for three setups in Section 5.4. 3. It checks inferred invariants on the validation workloads. The experiments might fail if environment installation issues or disruption happens. When you run into problems, please refer to \u26a0\ufe0f Notes & Troubleshooting .","title":"\ud83d\udc40 What to Expect During Execution"},{"location":"ae-eval-s5.4-fp-rate/#notes-troubleshooting","text":"The script will automatically detect any errors in any (1) trace collection, (2) inference tasks, (3) checking tasks. If you encounter any trace collection issues, please check for any missing environment dependencies. If you encounter any issues on invariant inference tasks or invariant checking tasks, please try to rerun the experiment by adding --overwrite-existing-results or delete all trace_* folders except for trace_ddp-multigpu . If you see persistent issues, it will likely be a environment issue or software bug. Please contact us for help.","title":"\u26a0\ufe0f Notes &amp; Troubleshooting"},{"location":"ae-eval-s5.4-fp-rate/#how-to-verify-the-results","text":"The compute_fp_rate.py script generates a file called fp_rates.csv under the current directory. Looking like this setup,fp_rate 1-input,0.3105 4-input,0.1127 6-input,0.1066 These values correspond to the results reported in Section 5.4 of the paper. You should verify that the false positive rates are similar or lower. Since the OSDI submission, we have fixed multiple bugs in TrainCheck, so the false positive rates are expected to be significantly lower in most cases. In our run of the script, we obtained the following results: setup,fp_rate 1-input,0.039 4-input,0.021 6-input,0.015","title":"\ud83e\uddd0 How to verify the results?"},{"location":"ae-eval-s5.5-perf-overhead/","text":"Eval: Performance Overhead \u23f3 Estimated Completion Time: 15 minutes. \ud83c\udfaf Goal This evaluation measures the runtime overhead introduced by TrainCheck\u2019s instrumentation compared to un-instrumented runs across a set of representative ML workloads, during the invariant checking stage. The results correspond to Section 5.5 of the paper. \ud83d\udcc2 Resources & Scripts Files described below are all in the TrainCheck-Evaluation-Workloads repo. Automation Scripts: performance_overhead/ae_perf.sh : End-to-end script for running the performance overhead benchmarks (Section 5.5) and generating Figure 7. It internally calls: run_all.xsh : Runs the experiments and collects raw data (per-iteration duration). analysis.xsh : Analyzes the raw data and prepares input for plotting. plot_e2e.py : Plots the final results. Workloads (You won't need to touch this): Located in overhead-e2e The deployed 100 invariants: eval_scripts/perf_benchmark/overhead-e2e/sampled_100_invariants.json \ud83d\udee0 How to Run Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide . All steps described below assumes you are already in the TrainCheck-Evaluation-Workloads repo. If not, clone the repository and go to it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads Execute ae_perf.sh . ```bash conda activate traincheck cd performance_overhead bash ae_perf.sh ``` \ud83e\uddd1\u200d\ud83d\udcbb Expected Output After execution completes, a plot will be generated at performance_ae.pdf . All the raw data are stored at a folder named perf_res_ae . \ud83e\uddd0 How to Verify Open the generated file performance_ae.pdf and compare it against Figure 7 in the paper. Small differences in the overhead numbers (within \u00b120%) are expected. TrainCheck\u2019s overhead is sensitive to CPU performance, since trace serialization is blocking and CPU-bound. Despite minor variations, the key takeaway should remain clear: TrainCheck\u2019s selective instrumentation incurs significantly lower overhead compared to other methods. \u26a0\ufe0f Notes & Troubleshooting Do Not Run Other GPU Tasks in Parallel For stable performance measurements, the evaluation scripts will periodically terminate all CUDA processes to ensure a clean environment. Please avoid running any other GPU workloads during this evaluation. Handling Failed Workloads If an end-to-end workload fails: - Navigate to the corresponding workload folder. - Manually rerun it using: bash traincheck-collect --use-config --config md-config-var.yml -i ../sampled_100_invariants.json - If the issue does not reproduce consistently, simply delete the result folder and rerun the full benchmark. - If the failure is consistent, please contact us for support.","title":"Eval: Performance Overhead"},{"location":"ae-eval-s5.5-perf-overhead/#eval-performance-overhead","text":"\u23f3 Estimated Completion Time: 15 minutes.","title":"Eval: Performance Overhead"},{"location":"ae-eval-s5.5-perf-overhead/#goal","text":"This evaluation measures the runtime overhead introduced by TrainCheck\u2019s instrumentation compared to un-instrumented runs across a set of representative ML workloads, during the invariant checking stage. The results correspond to Section 5.5 of the paper.","title":"\ud83c\udfaf Goal"},{"location":"ae-eval-s5.5-perf-overhead/#resources-scripts","text":"Files described below are all in the TrainCheck-Evaluation-Workloads repo. Automation Scripts: performance_overhead/ae_perf.sh : End-to-end script for running the performance overhead benchmarks (Section 5.5) and generating Figure 7. It internally calls: run_all.xsh : Runs the experiments and collects raw data (per-iteration duration). analysis.xsh : Analyzes the raw data and prepares input for plotting. plot_e2e.py : Plots the final results. Workloads (You won't need to touch this): Located in overhead-e2e The deployed 100 invariants: eval_scripts/perf_benchmark/overhead-e2e/sampled_100_invariants.json","title":"\ud83d\udcc2 Resources &amp; Scripts"},{"location":"ae-eval-s5.5-perf-overhead/#how-to-run","text":"Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide . All steps described below assumes you are already in the TrainCheck-Evaluation-Workloads repo. If not, clone the repository and go to it. bash git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git cd TrainCheck-Evaluation-Workloads Execute ae_perf.sh . ```bash conda activate traincheck cd performance_overhead bash ae_perf.sh ```","title":"\ud83d\udee0 How to Run"},{"location":"ae-eval-s5.5-perf-overhead/#expected-output","text":"After execution completes, a plot will be generated at performance_ae.pdf . All the raw data are stored at a folder named perf_res_ae .","title":"\ud83e\uddd1\u200d\ud83d\udcbb Expected Output"},{"location":"ae-eval-s5.5-perf-overhead/#how-to-verify","text":"Open the generated file performance_ae.pdf and compare it against Figure 7 in the paper. Small differences in the overhead numbers (within \u00b120%) are expected. TrainCheck\u2019s overhead is sensitive to CPU performance, since trace serialization is blocking and CPU-bound. Despite minor variations, the key takeaway should remain clear: TrainCheck\u2019s selective instrumentation incurs significantly lower overhead compared to other methods.","title":"\ud83e\uddd0 How to Verify"},{"location":"ae-eval-s5.5-perf-overhead/#notes-troubleshooting","text":"Do Not Run Other GPU Tasks in Parallel For stable performance measurements, the evaluation scripts will periodically terminate all CUDA processes to ensure a clean environment. Please avoid running any other GPU workloads during this evaluation. Handling Failed Workloads If an end-to-end workload fails: - Navigate to the corresponding workload folder. - Manually rerun it using: bash traincheck-collect --use-config --config md-config-var.yml -i ../sampled_100_invariants.json - If the issue does not reproduce consistently, simply delete the result folder and rerun the full benchmark. - If the failure is consistent, please contact us for support.","title":"\u26a0\ufe0f Notes &amp; Troubleshooting"},{"location":"ae/","text":"TrainCheck Artifact Evaluation Guide Welcome to the artifact evaluation guide for TrainCheck (OSDI'25). This document outlines the procedures needed to reproduce our results and guides you through the key experiments presented in the paper. Note: We may update both the main TrainCheck repository and the evaluation workloads repository during the evaluation period. Please make sure to pull the latest version of each repository before proceeding. \u2705 Checklist [ ] Environment set up (Python, dependencies, 2 CUDA GPUs with \u2265 12GiB memory each) [ ] Installed xonsh via pip3 install 'xonsh[full]' in the conda environment [ ] Ran Silent Issue Detection experiment [ ] Ran Invariant Transferability evaluation [ ] Ran False Positive Rate evaluation [ ] Ran Performance Overhead measurement [ ] Verified outputs match expected results (tolerances noted per experiment) \ud83d\udcce Resources You Need In addition to this guide, you will need the following resources throughout the evaluation process: 5-Minute Tutorial \u2014 A quick walkthrough that introduces TrainCheck\u2019s workflow using a real-world bug. TrainCheck Installation Guide \u2014 Step-by-step instructions for setting up TrainCheck. Technical Usage Guide \u2014 Detailed documentation on how to use TrainCheck, configure instrumentation, and interpret outputs. Evaluation Workloads Repository \u2014 Contains all evaluation workloads and automation scripts used in the experiments. Overview TrainCheck is an invariant-based tool for detecting silent correctness issues in PyTorch training pipelines. This artifact enables reproduction of the four main evaluation results from the paper: Silent Issue Detection (Section 5.1) Invariant Transferability (Section 5.3) False Positive Rate (Section 5.4) Performance Overhead (Section 5.5) To get familiar with TrainCheck, we recommend starting with the 5-Minute Tutorial , which walks you through detecting a real-world bug from Section 5.1. \u23f1\ufe0f Recommended Evaluation Order We suggest running the evaluations in the following order, based on automation level and runtime requirements: Kick the tires \u2013 5 min tutorial with TrainCheck Performance Overhead (~10 minutes) False Positive Rate (~1.5 hours) Transferability (~30 minutes) Silent Issue Detection (~ variate, should be able to finish within one day) Environment Requirements Many of our experiment scripts are written in xonsh, a shell that combines Python and Bash. Please install it with: conda activate traincheck pip3 install 'xonsh[full]' For a full and efficient AE experience, we recommend the following setup: - \ud83d\udda5 1 machine with 2\u00d7 CUDA-enabled GPUs - Each GPU should have at least 12\u202fGiB memory. - Compatible with CUDA 11.8 or 12.1 - \ud83e\udde0 32 host memory (recommended) \ud83d\udd27 Recommended Hardware: Chameleon Cloud Most experiments require 2\u00d7 CUDA-enabled GPUs with support for CUDA 11.8+ . While some workloads can run on GPUs with as little as 2\u202fGiB memory, the main experiments (e.g., Section 5.1) benefit from higher-capacity GPUs. We recommend using the compute_liqid node type on Chameleon Cloud : \u2705 liqid01 and liqid02 : These nodes each have 2\u00d7 A100 GPUs (40\u202fGiB) and allow you to reproduce all results in the paper. \ud83c\udd97 Other compute_liqid nodes with 1\u00d7 A100 GPU : These are sufficient for all single-GPU experiments and let you reproduce ~90% of results. Please consult the estimated runtimes in each evaluation section before making reservations. \u23f1\ufe0f If working full-time on the artifact, 2 days should be sufficient , but we recommend reserving at least 5 days to allow for possible setup delays or debugging. Software Notes If you\u2019re using Chameleon instances: Please start your machine with an Ubuntu 22.04 image that includes recent GPU drivers. We recommend using the CC-Ubuntu22.04-CUDA OS image. Follow Installation Guide to install TrainCheck. \u23ed\ufe0f Once your environment is set up, we recommend starting with the 5-Minute Tutorial with TrainCheck . It will help you get familiar with the workflow and also verify that your installation is working correctly. \ud83d\ude80 Kick-the-Tires: Try TrainCheck in 5 Minutes Get started quickly by using TrainCheck to detect and diagnosis a real-world bug report: PyTorch-FORUM-84911 . See details in 5-min-tutorial . \ud83d\udcca Start Full Artifact Evaluation Follow the below specific instructions to reproduce our evaluation results: Section 5.5: Performance Overhead Section 5.4: False Positives Section 5.3: Invariant Transferability Section 5.1: Silent Issue Detection","title":"TrainCheck Artifact Evaluation Guide"},{"location":"ae/#traincheck-artifact-evaluation-guide","text":"Welcome to the artifact evaluation guide for TrainCheck (OSDI'25). This document outlines the procedures needed to reproduce our results and guides you through the key experiments presented in the paper. Note: We may update both the main TrainCheck repository and the evaluation workloads repository during the evaluation period. Please make sure to pull the latest version of each repository before proceeding.","title":"TrainCheck Artifact Evaluation Guide"},{"location":"ae/#checklist","text":"[ ] Environment set up (Python, dependencies, 2 CUDA GPUs with \u2265 12GiB memory each) [ ] Installed xonsh via pip3 install 'xonsh[full]' in the conda environment [ ] Ran Silent Issue Detection experiment [ ] Ran Invariant Transferability evaluation [ ] Ran False Positive Rate evaluation [ ] Ran Performance Overhead measurement [ ] Verified outputs match expected results (tolerances noted per experiment)","title":"\u2705 Checklist"},{"location":"ae/#resources-you-need","text":"In addition to this guide, you will need the following resources throughout the evaluation process: 5-Minute Tutorial \u2014 A quick walkthrough that introduces TrainCheck\u2019s workflow using a real-world bug. TrainCheck Installation Guide \u2014 Step-by-step instructions for setting up TrainCheck. Technical Usage Guide \u2014 Detailed documentation on how to use TrainCheck, configure instrumentation, and interpret outputs. Evaluation Workloads Repository \u2014 Contains all evaluation workloads and automation scripts used in the experiments.","title":"\ud83d\udcce Resources You Need"},{"location":"ae/#overview","text":"TrainCheck is an invariant-based tool for detecting silent correctness issues in PyTorch training pipelines. This artifact enables reproduction of the four main evaluation results from the paper: Silent Issue Detection (Section 5.1) Invariant Transferability (Section 5.3) False Positive Rate (Section 5.4) Performance Overhead (Section 5.5) To get familiar with TrainCheck, we recommend starting with the 5-Minute Tutorial , which walks you through detecting a real-world bug from Section 5.1.","title":"Overview"},{"location":"ae/#recommended-evaluation-order","text":"We suggest running the evaluations in the following order, based on automation level and runtime requirements: Kick the tires \u2013 5 min tutorial with TrainCheck Performance Overhead (~10 minutes) False Positive Rate (~1.5 hours) Transferability (~30 minutes) Silent Issue Detection (~ variate, should be able to finish within one day)","title":"\u23f1\ufe0f Recommended Evaluation Order"},{"location":"ae/#environment-requirements","text":"Many of our experiment scripts are written in xonsh, a shell that combines Python and Bash. Please install it with: conda activate traincheck pip3 install 'xonsh[full]' For a full and efficient AE experience, we recommend the following setup: - \ud83d\udda5 1 machine with 2\u00d7 CUDA-enabled GPUs - Each GPU should have at least 12\u202fGiB memory. - Compatible with CUDA 11.8 or 12.1 - \ud83e\udde0 32 host memory (recommended)","title":"Environment Requirements"},{"location":"ae/#recommended-hardware-chameleon-cloud","text":"Most experiments require 2\u00d7 CUDA-enabled GPUs with support for CUDA 11.8+ . While some workloads can run on GPUs with as little as 2\u202fGiB memory, the main experiments (e.g., Section 5.1) benefit from higher-capacity GPUs. We recommend using the compute_liqid node type on Chameleon Cloud : \u2705 liqid01 and liqid02 : These nodes each have 2\u00d7 A100 GPUs (40\u202fGiB) and allow you to reproduce all results in the paper. \ud83c\udd97 Other compute_liqid nodes with 1\u00d7 A100 GPU : These are sufficient for all single-GPU experiments and let you reproduce ~90% of results. Please consult the estimated runtimes in each evaluation section before making reservations. \u23f1\ufe0f If working full-time on the artifact, 2 days should be sufficient , but we recommend reserving at least 5 days to allow for possible setup delays or debugging.","title":"\ud83d\udd27 Recommended Hardware: Chameleon Cloud"},{"location":"ae/#software-notes","text":"If you\u2019re using Chameleon instances: Please start your machine with an Ubuntu 22.04 image that includes recent GPU drivers. We recommend using the CC-Ubuntu22.04-CUDA OS image. Follow Installation Guide to install TrainCheck. \u23ed\ufe0f Once your environment is set up, we recommend starting with the 5-Minute Tutorial with TrainCheck . It will help you get familiar with the workflow and also verify that your installation is working correctly.","title":"Software Notes"},{"location":"ae/#kick-the-tires-try-traincheck-in-5-minutes","text":"Get started quickly by using TrainCheck to detect and diagnosis a real-world bug report: PyTorch-FORUM-84911 . See details in 5-min-tutorial .","title":"\ud83d\ude80 Kick-the-Tires: Try TrainCheck in 5 Minutes"},{"location":"ae/#start-full-artifact-evaluation","text":"Follow the below specific instructions to reproduce our evaluation results: Section 5.5: Performance Overhead Section 5.4: False Positives Section 5.3: Invariant Transferability Section 5.1: Silent Issue Detection","title":"\ud83d\udcca Start Full Artifact Evaluation"},{"location":"benchmarks/","text":"Performance Benchmarks Latest benchmark results (updated: 2025-07-27 18:22:13 UTC, commit: 7307782) Instrumentation Overhead - Micro Benchmarks End-to-End Performance Impact These benchmarks are automatically generated from the TrainCheck-Benchmarks repository.","title":"Performance Benchmarks"},{"location":"benchmarks/#performance-benchmarks","text":"Latest benchmark results (updated: 2025-07-27 18:22:13 UTC, commit: 7307782)","title":"Performance Benchmarks"},{"location":"benchmarks/#instrumentation-overhead-micro-benchmarks","text":"","title":"Instrumentation Overhead - Micro Benchmarks"},{"location":"benchmarks/#end-to-end-performance-impact","text":"These benchmarks are automatically generated from the TrainCheck-Benchmarks repository.","title":"End-to-End Performance Impact"},{"location":"check/","text":"TrainCheck Checker Usage Guide traincheck-check is the final stage of the TrainCheck workflow. It verifies a set of invariants against trace files or streams from target programs, reporting any detected violations\u2014helping you catch silent issues in your ML training pipelines. \ud83d\udd27 Checking Modes TrainCheck supports two checking modes: Post-training Checking ( traincheck-check ) : Perform invariant checking on completed trace files after the training job finishes. \u2705 On-the-fly Checking ( traincheck-onlinecheck ): Perform real-time checking while the target training job is running. \u2705 How to Use: On-the-fly Checking While training is in progress with traincheck-collect , run the following command: traincheck-onlinecheck -f <trace_folder> -i <path_to_invariant_file> -f <trace_folder> : Path to the folder where traces are: Already collected, or Actively being collected by traincheck-collect during the training job. -i <path_to_invariant_file> : Path to the JSON file containing inferred invariants. How to Use: Post-training Checking Run the following command: traincheck-check -f <trace_folder> -i <path_to_invariant_file> -f <trace_folder> : Path to the folder containing traces collected by traincheck-collect . -i <path_to_invariant_file> : Path to the JSON file containing inferred invariants. Interpreting the Results After running either checking mode, TrainCheck will output a summary of detected invariant violations. Each violation entry typically includes: Trace file or stream name : Identifies where the issue was found. Invariant description : Details the specific invariant that was violated. Violation details : Provides context, such as the step or epoch where the violation occurred. Review these results to pinpoint silent errors or unexpected behaviors in your ML training pipeline. For more information on result formats and how to diagnose issues, see 5. Detection & Diagnosis in the 5-Minute Tutorial .","title":"TrainCheck Checker Usage Guide"},{"location":"check/#traincheck-checker-usage-guide","text":"traincheck-check is the final stage of the TrainCheck workflow. It verifies a set of invariants against trace files or streams from target programs, reporting any detected violations\u2014helping you catch silent issues in your ML training pipelines.","title":"TrainCheck Checker Usage Guide"},{"location":"check/#checking-modes","text":"TrainCheck supports two checking modes: Post-training Checking ( traincheck-check ) : Perform invariant checking on completed trace files after the training job finishes. \u2705 On-the-fly Checking ( traincheck-onlinecheck ): Perform real-time checking while the target training job is running. \u2705","title":"\ud83d\udd27 Checking Modes"},{"location":"check/#how-to-use-on-the-fly-checking","text":"While training is in progress with traincheck-collect , run the following command: traincheck-onlinecheck -f <trace_folder> -i <path_to_invariant_file> -f <trace_folder> : Path to the folder where traces are: Already collected, or Actively being collected by traincheck-collect during the training job. -i <path_to_invariant_file> : Path to the JSON file containing inferred invariants.","title":"How to Use: On-the-fly Checking"},{"location":"check/#how-to-use-post-training-checking","text":"Run the following command: traincheck-check -f <trace_folder> -i <path_to_invariant_file> -f <trace_folder> : Path to the folder containing traces collected by traincheck-collect . -i <path_to_invariant_file> : Path to the JSON file containing inferred invariants.","title":"How to Use: Post-training Checking"},{"location":"check/#interpreting-the-results","text":"After running either checking mode, TrainCheck will output a summary of detected invariant violations. Each violation entry typically includes: Trace file or stream name : Identifies where the issue was found. Invariant description : Details the specific invariant that was violated. Violation details : Provides context, such as the step or epoch where the violation occurred. Review these results to pinpoint silent errors or unexpected behaviors in your ML training pipeline. For more information on result formats and how to diagnose issues, see 5. Detection & Diagnosis in the 5-Minute Tutorial .","title":"Interpreting the Results"},{"location":"infer/","text":"Invariant Inference & Representation traincheck-infer is part of the inference stage of the TrainCheck workflow. It consumes trace files collected from correct training runs and infers behavioral invariants that describe expected runtime behavior. These invariants are later used by traincheck-check to detect violations in other training pipelines. \ud83d\udcda Table of Contents \ud83d\udd27 Basic Usage \u2699\ufe0f Advanced Usage \ud83d\udcd8 Invariant Concepts \ud83e\uddea Guidelines: Choosing Input Pipelines \ud83e\udde0 Tips: Performance and Stability \ud83d\udd17 Next Step \ud83d\udd27 Basic Usage In most cases, you only need to specify one or more folders (generated by traincheck-collect ) containing trace files using the -f or --trace-folders flag: traincheck-infer -f ./traincheck_mnist_trace ./traincheck_84911_trace .. You can provide multiple folders to aggregate traces from different correct runs or programs. This helps TrainCheck generalize better and avoid overfitting to any single pipeline, reducing false positives during checking\u2014especially when the inferred invariants are applied to unrelated or structurally different pipelines. This command will infer invariants from all trace folders provided, and output invariants into invariants.json . \u2699\ufe0f Advanced Usage traincheck-infer provides additional flags for customization and debugging. Some concepts such as \"relation\" will be explained later. -o, --output : Specify a custom file name for the invariants. --disable-relation / --enable-relation : Control which types of invariants to infer. This is useful for reducing noise or targeting specific checks. ```bash # Disable ordering-based invariants traincheck-infer -f ./traces --disable-relation FunctionLeadRelation FunctionCoverRelation Enable only contain and variable consistency invariants traincheck-infer -f ./traces --enable-relation APIContainRelation ConsistencyRelation ``` See traincheck.invariant.relation_pool for a complete list of invariants. 3. -b, --backend : Select the data processing engine for trace handling. - pandas (default): stable and well-tested. - polars : faster for large traces (experimental) - dict : pure Python dictionary backend (experimental) Other flags (e.g. --debug , -t --traces ) are available via traincheck-infer --help, but are rarely needed unless you are debugging or developing TrainCheck itself. \ud83d\udcd8 Invariant Concepts TrainCheck infers invariants \u2014 logical properties that are consistently held during correct training runs. These invariants are used to define the expected behavior of a training pipeline, and later help detect silent issues when applied to other runs. Each invariant describes a specific pattern of behavior observed in the trace, such as: - Attribute changes during a function call (e.g., .grad becomes None in zero_grad() ) - Ordering relationships between API calls (e.g., zero_grad() should occur before step() ) - Consistency among values across different parameters (e.g., shared parameters should have the same value across devices during distributed training) Invariant Representation An invariant is defined by three things: 1. relation : the relationship this invariant encodes, can be viewed as an invariant template. Each relation has a separate inference algorithm defined (e.g., ConsistencyRelation.infer ) 2. params : descriptors for entities that should obey the relationship. 3. precondition : a logical predicate defining the context when an invariant can be applied. In the actual json representation of invariants in the traincheck-infer output, an invariant looks like this. { \"text_description\": \"torch.optim.optimizer.Optimizer.zero_grad contains VarChangeEvent torch.nn.Parameter, pre_value: non_zero, post_value: None\", \"relation\": \"APIContainRelation\", \"params\": [ { \"param_type\": \"APIParam\", \"api_full_name\": \"torch.optim.optimizer.Optimizer.zero_grad\" }, { \"param_type\": \"VarTypeParam\", \"var_type\": \"torch.nn.Parameter\", \"attr_name\": \"grad\", \"pre_value\": \"non_zero\", \"post_value\": null } ], \"precondition\": { \"parent_func_call_pre\": { \"inverted\": true, \"preconditions\": [ { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.step\", \"additional_path\": \"None\", \"prop_dtype\": \"int\", \"values\": [ 0 ] } ] }, { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.stage\", \"additional_path\": \"None\", \"prop_dtype\": \"str\", \"values\": [ \"init\", \"testing\" ] } ] } ] } }, \"num_positive_examples\": 200, \"num_negative_examples\": 1 } This invariant encodes the expectation that calling torch.optim.optimizer.Optimizer.zero_grad() should reset gradients \u2014 that is, the .grad attribute of torch.nn.Parameter objects should transition from a non-zero value to null (i.e., None or missing). - text_description: A human-readable summary of the invariant. > Note: This field is generated using a best-effort strategy and may not fully reflect the invariant\u2019s semantics. In some cases, it may be missing or incomplete. \ud83d\udcc6 We are planning to further formalize this field in the future. relation: \"APIContainRelation\" An event is expected to happen within the duration of an API invocation. params: An API call: zero_grad() on a PyTorch optimizer An attribute: .grad on a torch.nn.Parameter , which should change from a non-zero value ( \"pre_value\": \"non_zero\" ) to null ( \"post_value\": null ) during the call precondition: This invariant only applies outside the following contexts: The first step of training ( meta_vars.step == 0 ) The init or testing stages ( meta_vars.stage in {\"init\", \"testing\"} ) These are specified as inverted preconditions, meaning the invariant does not apply during those times (e.g., it\u2019s okay to not clear .grad on the first step when nothing has been backpropagated yet). num_positive_examples: 20 This behavior was observed and confirmed 200 times in the reference traces. num_negative_examples: 1 The invariant failed once \u2014 in this case, during the first training iteration, when .grad had not yet been populated before the zero_grad() call. > \ud83c\udfaf This behavior is expected and correctly handled by the precondition, which excludes step 0. Invariant Inference Workflow At a high level, TrainCheck performs invariant inference in three stages: Hypothesis Generation For each supported relation type, TrainCheck scans the provided traces and generates hypotheses by identifying patterns where a potential invariant could exist (i.e., when matching examples are observed). Example Collection For every hypothesis, TrainCheck performs a full scan across all provided traces to gather positive examples (where the hypothesized invariant holds) and negative examples (where it does not). Precondition Deduction TrainCheck analyzes the collected examples to infer a distinguishing predicate\u2014a logical condition that holds true for all positive examples and false for negative ones. This predicate becomes the invariant\u2019s precondition, reducing false positives during checking. \u2699\ufe0f For full details on the inference algorithms, please refer to our OSDI\u201925 paper (documentation is in progress). \ud83e\uddea Practical Guidelines: Choosing Input Pipelines When selecting input pipelines for invariant inference, there are two main considerations: Representativeness You want your input pipelines to be diverse enough to infer a representative set of invariants. This helps: - Avoid overfitting to specific patterns. - Ensure that inferred invariants and preconditions remain accurate across varying scenarios. For example, if none of your input pipelines use mixed precision, TrainCheck might infer invariants like: \"For mathematical operations, the output dtype must equal the input dtype.\" However, if mixed precision pipelines are included, TrainCheck will refine such invariants by adding preconditions like: \"This applies only when a torch.autocast context manager is not active.\" \u26a1 How many pipelines should you include? It depends on how different your target pipeline is from available reference pipelines: - If the target is a minor variant of a known-good pipeline, using just that reference may suffice. - If the target pipeline introduces new frameworks, tasks, or architectures, include a broader set of inputs to improve generalization. Inference Time Inference time is generally not a major concern, since inference happens offline. However, due to the repetitive nature of training loops, you can safely shorten reference runs without sacrificing invariant quality. In practice: - For all bugs detected by TrainCheck so far, we limited inference traces to at most 100 iterations. - Shortened runs have shown no significant impact on the usefulness or accuracy of inferred invariants. Core Principles \u2013 A Summary Focus on the diversity of input traces \u2014 capturing different configurations, behaviors, or modes of operation. The length or size of traces matters far less. Efficient inference is achievable with short, representative runs. Implementation Limitations TrainCheck operates on large traces with a dynamic schema, where variable types and fields can change over time. This, combined with the need for cross-trace comparisons, limits the use of typical data storage solutions like SQL databases or optimized DataFrame libraries (e.g., Polars), which require fixed schemas. To handle this, we use in-process Pandas DataFrames backed by NumPy. While effective, this approach is currently single-threaded due to Python\u2019s GIL, leaving room for future performance improvements. We are exploring options such as shared-memory DataFrames, schema standardization, or schemaless databases (e.g., MongoDB) if data transmission overhead proves manageable. Note: While data sharding could improve parallelism, it would overcomplicate cross-trace and cross-time analysis and is better handled at the storage layer rather than within inference logic.","title":"Invariant Inference &amp; Representation"},{"location":"infer/#invariant-inference-representation","text":"traincheck-infer is part of the inference stage of the TrainCheck workflow. It consumes trace files collected from correct training runs and infers behavioral invariants that describe expected runtime behavior. These invariants are later used by traincheck-check to detect violations in other training pipelines.","title":"Invariant Inference &amp; Representation"},{"location":"infer/#table-of-contents","text":"\ud83d\udd27 Basic Usage \u2699\ufe0f Advanced Usage \ud83d\udcd8 Invariant Concepts \ud83e\uddea Guidelines: Choosing Input Pipelines \ud83e\udde0 Tips: Performance and Stability \ud83d\udd17 Next Step","title":"\ud83d\udcda Table of Contents"},{"location":"infer/#basic-usage","text":"In most cases, you only need to specify one or more folders (generated by traincheck-collect ) containing trace files using the -f or --trace-folders flag: traincheck-infer -f ./traincheck_mnist_trace ./traincheck_84911_trace .. You can provide multiple folders to aggregate traces from different correct runs or programs. This helps TrainCheck generalize better and avoid overfitting to any single pipeline, reducing false positives during checking\u2014especially when the inferred invariants are applied to unrelated or structurally different pipelines. This command will infer invariants from all trace folders provided, and output invariants into invariants.json .","title":"\ud83d\udd27 Basic Usage"},{"location":"infer/#advanced-usage","text":"traincheck-infer provides additional flags for customization and debugging. Some concepts such as \"relation\" will be explained later. -o, --output : Specify a custom file name for the invariants. --disable-relation / --enable-relation : Control which types of invariants to infer. This is useful for reducing noise or targeting specific checks. ```bash # Disable ordering-based invariants traincheck-infer -f ./traces --disable-relation FunctionLeadRelation FunctionCoverRelation","title":"\u2699\ufe0f Advanced Usage"},{"location":"infer/#enable-only-contain-and-variable-consistency-invariants","text":"traincheck-infer -f ./traces --enable-relation APIContainRelation ConsistencyRelation ``` See traincheck.invariant.relation_pool for a complete list of invariants. 3. -b, --backend : Select the data processing engine for trace handling. - pandas (default): stable and well-tested. - polars : faster for large traces (experimental) - dict : pure Python dictionary backend (experimental) Other flags (e.g. --debug , -t --traces ) are available via traincheck-infer --help, but are rarely needed unless you are debugging or developing TrainCheck itself.","title":"Enable only contain and variable consistency invariants"},{"location":"infer/#invariant-concepts","text":"TrainCheck infers invariants \u2014 logical properties that are consistently held during correct training runs. These invariants are used to define the expected behavior of a training pipeline, and later help detect silent issues when applied to other runs. Each invariant describes a specific pattern of behavior observed in the trace, such as: - Attribute changes during a function call (e.g., .grad becomes None in zero_grad() ) - Ordering relationships between API calls (e.g., zero_grad() should occur before step() ) - Consistency among values across different parameters (e.g., shared parameters should have the same value across devices during distributed training)","title":"\ud83d\udcd8 Invariant Concepts"},{"location":"infer/#invariant-representation","text":"An invariant is defined by three things: 1. relation : the relationship this invariant encodes, can be viewed as an invariant template. Each relation has a separate inference algorithm defined (e.g., ConsistencyRelation.infer ) 2. params : descriptors for entities that should obey the relationship. 3. precondition : a logical predicate defining the context when an invariant can be applied. In the actual json representation of invariants in the traincheck-infer output, an invariant looks like this. { \"text_description\": \"torch.optim.optimizer.Optimizer.zero_grad contains VarChangeEvent torch.nn.Parameter, pre_value: non_zero, post_value: None\", \"relation\": \"APIContainRelation\", \"params\": [ { \"param_type\": \"APIParam\", \"api_full_name\": \"torch.optim.optimizer.Optimizer.zero_grad\" }, { \"param_type\": \"VarTypeParam\", \"var_type\": \"torch.nn.Parameter\", \"attr_name\": \"grad\", \"pre_value\": \"non_zero\", \"post_value\": null } ], \"precondition\": { \"parent_func_call_pre\": { \"inverted\": true, \"preconditions\": [ { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.step\", \"additional_path\": \"None\", \"prop_dtype\": \"int\", \"values\": [ 0 ] } ] }, { \"clauses\": [ { \"type\": \"constant\", \"prop_name\": \"meta_vars.stage\", \"additional_path\": \"None\", \"prop_dtype\": \"str\", \"values\": [ \"init\", \"testing\" ] } ] } ] } }, \"num_positive_examples\": 200, \"num_negative_examples\": 1 } This invariant encodes the expectation that calling torch.optim.optimizer.Optimizer.zero_grad() should reset gradients \u2014 that is, the .grad attribute of torch.nn.Parameter objects should transition from a non-zero value to null (i.e., None or missing). - text_description: A human-readable summary of the invariant. > Note: This field is generated using a best-effort strategy and may not fully reflect the invariant\u2019s semantics. In some cases, it may be missing or incomplete. \ud83d\udcc6 We are planning to further formalize this field in the future. relation: \"APIContainRelation\" An event is expected to happen within the duration of an API invocation. params: An API call: zero_grad() on a PyTorch optimizer An attribute: .grad on a torch.nn.Parameter , which should change from a non-zero value ( \"pre_value\": \"non_zero\" ) to null ( \"post_value\": null ) during the call precondition: This invariant only applies outside the following contexts: The first step of training ( meta_vars.step == 0 ) The init or testing stages ( meta_vars.stage in {\"init\", \"testing\"} ) These are specified as inverted preconditions, meaning the invariant does not apply during those times (e.g., it\u2019s okay to not clear .grad on the first step when nothing has been backpropagated yet). num_positive_examples: 20 This behavior was observed and confirmed 200 times in the reference traces. num_negative_examples: 1 The invariant failed once \u2014 in this case, during the first training iteration, when .grad had not yet been populated before the zero_grad() call. > \ud83c\udfaf This behavior is expected and correctly handled by the precondition, which excludes step 0.","title":"Invariant Representation"},{"location":"infer/#invariant-inference-workflow","text":"At a high level, TrainCheck performs invariant inference in three stages: Hypothesis Generation For each supported relation type, TrainCheck scans the provided traces and generates hypotheses by identifying patterns where a potential invariant could exist (i.e., when matching examples are observed). Example Collection For every hypothesis, TrainCheck performs a full scan across all provided traces to gather positive examples (where the hypothesized invariant holds) and negative examples (where it does not). Precondition Deduction TrainCheck analyzes the collected examples to infer a distinguishing predicate\u2014a logical condition that holds true for all positive examples and false for negative ones. This predicate becomes the invariant\u2019s precondition, reducing false positives during checking. \u2699\ufe0f For full details on the inference algorithms, please refer to our OSDI\u201925 paper (documentation is in progress).","title":"Invariant Inference Workflow"},{"location":"infer/#practical-guidelines-choosing-input-pipelines","text":"When selecting input pipelines for invariant inference, there are two main considerations: Representativeness You want your input pipelines to be diverse enough to infer a representative set of invariants. This helps: - Avoid overfitting to specific patterns. - Ensure that inferred invariants and preconditions remain accurate across varying scenarios. For example, if none of your input pipelines use mixed precision, TrainCheck might infer invariants like: \"For mathematical operations, the output dtype must equal the input dtype.\" However, if mixed precision pipelines are included, TrainCheck will refine such invariants by adding preconditions like: \"This applies only when a torch.autocast context manager is not active.\" \u26a1 How many pipelines should you include? It depends on how different your target pipeline is from available reference pipelines: - If the target is a minor variant of a known-good pipeline, using just that reference may suffice. - If the target pipeline introduces new frameworks, tasks, or architectures, include a broader set of inputs to improve generalization. Inference Time Inference time is generally not a major concern, since inference happens offline. However, due to the repetitive nature of training loops, you can safely shorten reference runs without sacrificing invariant quality. In practice: - For all bugs detected by TrainCheck so far, we limited inference traces to at most 100 iterations. - Shortened runs have shown no significant impact on the usefulness or accuracy of inferred invariants.","title":"\ud83e\uddea Practical Guidelines: Choosing Input Pipelines"},{"location":"infer/#core-principles-a-summary","text":"Focus on the diversity of input traces \u2014 capturing different configurations, behaviors, or modes of operation. The length or size of traces matters far less. Efficient inference is achievable with short, representative runs.","title":"Core Principles \u2013 A Summary"},{"location":"infer/#implementation-limitations","text":"TrainCheck operates on large traces with a dynamic schema, where variable types and fields can change over time. This, combined with the need for cross-trace comparisons, limits the use of typical data storage solutions like SQL databases or optimized DataFrame libraries (e.g., Polars), which require fixed schemas. To handle this, we use in-process Pandas DataFrames backed by NumPy. While effective, this approach is currently single-threaded due to Python\u2019s GIL, leaving room for future performance improvements. We are exploring options such as shared-memory DataFrames, schema standardization, or schemaless databases (e.g., MongoDB) if data transmission overhead proves manageable. Note: While data sharding could improve parallelism, it would overcomplicate cross-trace and cross-time analysis and is better handled at the storage layer rather than within inference logic.","title":"Implementation Limitations"},{"location":"installation-guide/","text":"Compatibility Python : 3.10+ (due to reliance on type annotations) PyTorch : 1.7.0\u20132.5.0 (other versions have not been tested.) CUDA : 11.2\u201312.1 (also supports MPS on macOS; see Performance note below) Operating Systems : Ubuntu 20.04+, macOS. Windows is untested but may work\u2014please file an issue if you hit a problem. Performance note: On non\u2011CUDA backends (e.g., MPS), runtime overhead can vary due to differences in tensor\u2011hashing efficiency. We\u2019re actively measuring and tuning across platforms. Installation Steps Note: Example workloads are verified on Python 3.10 and PyTorch 2.2.2 + CUDA 12.1. If you\u2019re not reproducing our benchmarks, feel free to install any supported versions. AEC note: For full artifact evaluation, we recommend Ubuntu 22.04 with two Nvidia Ampere\u2011class GPUs (\u2265 12 GiB GPU memory each). For the 5\u2011minute tutorial, any Linux or macOS (Apple Silicon) laptop will do. Install Conda Install Miniconda by following the official Miniconda guide . Create & activate a Python 3.10 Conda Env bash conda create -n traincheck python=3.10 -y conda activate traincheck Install PyTorch 2.2.2 with CUDA support bash pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121 If your GPU does not support CUDA12, CUDA11.8 is also acceptable. bash pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118 If you don't have a CUDA-enabled GPU, just install the CPU version and skip step 4. bash pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu (CUDA platforms only) Install cudatoolkit bash conda install cudatoolkit Clone & install TrainCheck bash git clone https://github.com/OrderLab/TrainCheck.git cd TrainCheck pip3 install . Verify Installation You should now have three clis installed in your system. Do a quick test to see of these commands are available and functional. bash traincheck-collect --help traincheck-infer --help traincheck-check --help Next Steps 5\u2011Minute TrainCheck Experience Follow the 5\u2011Minute Tutorial to instrument a script, infer invariants, and catch silent bugs in under five minutes. Technical Documentation Explore the TrainCheck Technical Doc for a comprehensive guide to features, configuration, and advanced workflows.","title":"Installation Guide"},{"location":"installation-guide/#compatibility","text":"Python : 3.10+ (due to reliance on type annotations) PyTorch : 1.7.0\u20132.5.0 (other versions have not been tested.) CUDA : 11.2\u201312.1 (also supports MPS on macOS; see Performance note below) Operating Systems : Ubuntu 20.04+, macOS. Windows is untested but may work\u2014please file an issue if you hit a problem. Performance note: On non\u2011CUDA backends (e.g., MPS), runtime overhead can vary due to differences in tensor\u2011hashing efficiency. We\u2019re actively measuring and tuning across platforms.","title":"Compatibility"},{"location":"installation-guide/#installation-steps","text":"Note: Example workloads are verified on Python 3.10 and PyTorch 2.2.2 + CUDA 12.1. If you\u2019re not reproducing our benchmarks, feel free to install any supported versions. AEC note: For full artifact evaluation, we recommend Ubuntu 22.04 with two Nvidia Ampere\u2011class GPUs (\u2265 12 GiB GPU memory each). For the 5\u2011minute tutorial, any Linux or macOS (Apple Silicon) laptop will do. Install Conda Install Miniconda by following the official Miniconda guide . Create & activate a Python 3.10 Conda Env bash conda create -n traincheck python=3.10 -y conda activate traincheck Install PyTorch 2.2.2 with CUDA support bash pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121 If your GPU does not support CUDA12, CUDA11.8 is also acceptable. bash pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118 If you don't have a CUDA-enabled GPU, just install the CPU version and skip step 4. bash pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu (CUDA platforms only) Install cudatoolkit bash conda install cudatoolkit Clone & install TrainCheck bash git clone https://github.com/OrderLab/TrainCheck.git cd TrainCheck pip3 install . Verify Installation You should now have three clis installed in your system. Do a quick test to see of these commands are available and functional. bash traincheck-collect --help traincheck-infer --help traincheck-check --help","title":"Installation Steps"},{"location":"installation-guide/#next-steps","text":"5\u2011Minute TrainCheck Experience Follow the 5\u2011Minute Tutorial to instrument a script, infer invariants, and catch silent bugs in under five minutes. Technical Documentation Explore the TrainCheck Technical Doc for a comprehensive guide to features, configuration, and advanced workflows.","title":"Next Steps"},{"location":"instr/","text":"Instrumentation & Trace Representation traincheck-collect is the starting point of TrainCheck's workflow. It instruments your PyTorch training script to capture runtime behavior, generating detailed execution traces for later invariant inference and issue detection. This document explains how to use traincheck-collect effectively. TrainCheck dynamically wraps key PyTorch APIs and monitors model states\u2014 no modifications to your original training code are required . Use traincheck-collect when you need to: - Generate traces from reference pipelines for invariant inference. - Collect traces from target pipelines to detect silent issues using pre-inferred invariants. Table of Contents Introduction \ud83d\udd27 Basic Usage Configuration File Example Running traincheck-collect Selective Instrumentation for Checking Output Structure Overriding Configuration via CLI Adding Meta Variables to Traces How Meta Variables Improve Inference Examples of Useful Meta Variables How to Annotate Meta Variables Trace Representation Instrumentation Mechanisms Advanced Usage Algorithms Overview Troubleshooting & FAQs \ud83d\udd27 Basic Usage traincheck-collect requires three types of input: Python script to instrument. Launch arguments (if any) for executing the script. Instrumentation-specific configurations . You can provide these inputs either directly via the command line or through a configuration file. \u25b6\ufe0f Recommendation : Use a configuration file for clarity and reusability. Here\u2019s an example configuration: pyscript: ./mnist.py # Python entry point of your training program. shscript: ./run.sh # [Optional] Shell script to launch with custom arguments or environment setup. modules_to_instr: # Libraries to instrument. Defaults to ['torch'] if omitted. - torch models_to_track: # [Optional] Variable names of models to track. Leave empty to disable model tracking. - model model_tracker_style: proxy # [Optional] Tracking method: \"proxy\" (default) or \"sampler\". copy_all_files: false # [Optional] Set true if your code relies on relative paths (e.g., local datasets/configs). You can find example configurations and training programs in: \u2022 MNIST Example \u2022 GPT-2 Pretrain Example Run TrainCheck trace collection with: traincheck-collect --use-config --config <path-to-config-file> This command instruments the specified libraries and model variables, then executes your program. (Details on instrumentation mechanisms and limitations will follow in the next section. TODO) Selective Instrumentation for Checking When checking for silent issues, traincheck-collect supports selective instrumentation to improve efficiency. Simply provide the invariants file: traincheck-collect --use-config --config <path-to-config> --invariants <path-to-inv-file> TrainCheck will automatically adjust instrumentation granularity based on the provided invariants. Output Structure By default, TrainCheck creates a folder named: traincheck_run_<pyscript_name>_<instr_libs>_<timestamp> This folder contains: - Collected traces - Instrumented scripts and execution logs (if the program completes successfully) You can also provide any additional arguments not specified in the configuration through the commandline interface, such as Overriding Configuration via CLI You can override or supplement configuration settings by providing additional arguments directly via the command line. For example: # Write trace files to ./trace_training instead of using the default auto-generated folder name traincheck-collect --use-config --config <path-to-config-file> --output-dir trace_training To view all available command-line arguments and configuration options, run: traincheck-collect --help Note : When using a configuration file, replace hyphens (-) in argument names with underscores (_). For example: - Command-line: --output-dir trace_training - Configuration file: output_dir: trace_training Adding Meta Variables to Traces You can enhance your traces by providing custom meta variables \u2014semantic information about your program's execution. These annotations improve the quality and precision of inferred invariants by offering context that might not be directly observable from raw traces. Learn how meta variables improve invariant inference TrainCheck infers **preconditions** for each invariant\u2014these are predicates that distinguish between positive and negative examples in the trace. - A **positive example** is a trace segment where the invariant holds. - A **negative example** is where it is violated. Many invariants are inherently **conditional**, meaning they only hold true under certain contexts (e.g., during training but not initialization). TrainCheck tries to automatically discover such conditions. However, trace data alone may lack sufficient context. This is where **meta variables** come in\u2014they inject semantic hints (like execution phase or step number) to guide smarter inference. \u2728 Examples of Useful Meta Variables stage \u2014 Indicates whether a trace record belongs to initialization, training, or evaluation. step_id \u2014 The current training step or iteration number. Custom arguments \u2014 Any domain-specific flags or parameters relevant to your training logic. How to Annotate Meta Variables \ud83d\udccc [To Be Documented] Instructions for defining and injecting meta variables into traces will be provided in a future update. Trace Representation \ud83d\udccc [To Be Documented] Instrumentation Mechanisms \ud83d\udccc [To Be Documented] Details about TrainCheck\u2019s instrumentation strategies, supported APIs, and limitations will be covered here later.","title":"Instrumentation &amp; Trace Representation"},{"location":"instr/#instrumentation-trace-representation","text":"traincheck-collect is the starting point of TrainCheck's workflow. It instruments your PyTorch training script to capture runtime behavior, generating detailed execution traces for later invariant inference and issue detection. This document explains how to use traincheck-collect effectively. TrainCheck dynamically wraps key PyTorch APIs and monitors model states\u2014 no modifications to your original training code are required . Use traincheck-collect when you need to: - Generate traces from reference pipelines for invariant inference. - Collect traces from target pipelines to detect silent issues using pre-inferred invariants.","title":"Instrumentation &amp; Trace Representation"},{"location":"instr/#table-of-contents","text":"Introduction \ud83d\udd27 Basic Usage Configuration File Example Running traincheck-collect Selective Instrumentation for Checking Output Structure Overriding Configuration via CLI Adding Meta Variables to Traces How Meta Variables Improve Inference Examples of Useful Meta Variables How to Annotate Meta Variables Trace Representation Instrumentation Mechanisms Advanced Usage Algorithms Overview Troubleshooting & FAQs","title":"Table of Contents"},{"location":"instr/#basic-usage","text":"traincheck-collect requires three types of input: Python script to instrument. Launch arguments (if any) for executing the script. Instrumentation-specific configurations . You can provide these inputs either directly via the command line or through a configuration file. \u25b6\ufe0f Recommendation : Use a configuration file for clarity and reusability. Here\u2019s an example configuration: pyscript: ./mnist.py # Python entry point of your training program. shscript: ./run.sh # [Optional] Shell script to launch with custom arguments or environment setup. modules_to_instr: # Libraries to instrument. Defaults to ['torch'] if omitted. - torch models_to_track: # [Optional] Variable names of models to track. Leave empty to disable model tracking. - model model_tracker_style: proxy # [Optional] Tracking method: \"proxy\" (default) or \"sampler\". copy_all_files: false # [Optional] Set true if your code relies on relative paths (e.g., local datasets/configs). You can find example configurations and training programs in: \u2022 MNIST Example \u2022 GPT-2 Pretrain Example Run TrainCheck trace collection with: traincheck-collect --use-config --config <path-to-config-file> This command instruments the specified libraries and model variables, then executes your program. (Details on instrumentation mechanisms and limitations will follow in the next section. TODO)","title":"\ud83d\udd27 Basic Usage"},{"location":"instr/#selective-instrumentation-for-checking","text":"When checking for silent issues, traincheck-collect supports selective instrumentation to improve efficiency. Simply provide the invariants file: traincheck-collect --use-config --config <path-to-config> --invariants <path-to-inv-file> TrainCheck will automatically adjust instrumentation granularity based on the provided invariants.","title":"Selective Instrumentation for Checking"},{"location":"instr/#output-structure","text":"By default, TrainCheck creates a folder named: traincheck_run_<pyscript_name>_<instr_libs>_<timestamp> This folder contains: - Collected traces - Instrumented scripts and execution logs (if the program completes successfully) You can also provide any additional arguments not specified in the configuration through the commandline interface, such as","title":"Output Structure"},{"location":"instr/#overriding-configuration-via-cli","text":"You can override or supplement configuration settings by providing additional arguments directly via the command line. For example: # Write trace files to ./trace_training instead of using the default auto-generated folder name traincheck-collect --use-config --config <path-to-config-file> --output-dir trace_training To view all available command-line arguments and configuration options, run: traincheck-collect --help Note : When using a configuration file, replace hyphens (-) in argument names with underscores (_). For example: - Command-line: --output-dir trace_training - Configuration file: output_dir: trace_training","title":"Overriding Configuration via CLI"},{"location":"instr/#adding-meta-variables-to-traces","text":"You can enhance your traces by providing custom meta variables \u2014semantic information about your program's execution. These annotations improve the quality and precision of inferred invariants by offering context that might not be directly observable from raw traces. Learn how meta variables improve invariant inference TrainCheck infers **preconditions** for each invariant\u2014these are predicates that distinguish between positive and negative examples in the trace. - A **positive example** is a trace segment where the invariant holds. - A **negative example** is where it is violated. Many invariants are inherently **conditional**, meaning they only hold true under certain contexts (e.g., during training but not initialization). TrainCheck tries to automatically discover such conditions. However, trace data alone may lack sufficient context. This is where **meta variables** come in\u2014they inject semantic hints (like execution phase or step number) to guide smarter inference.","title":"Adding Meta Variables to Traces"},{"location":"instr/#examples-of-useful-meta-variables","text":"stage \u2014 Indicates whether a trace record belongs to initialization, training, or evaluation. step_id \u2014 The current training step or iteration number. Custom arguments \u2014 Any domain-specific flags or parameters relevant to your training logic.","title":"\u2728 Examples of Useful Meta Variables"},{"location":"instr/#how-to-annotate-meta-variables","text":"\ud83d\udccc [To Be Documented] Instructions for defining and injecting meta variables into traces will be provided in a future update.","title":"How to Annotate Meta Variables"},{"location":"instr/#trace-representation","text":"\ud83d\udccc [To Be Documented]","title":"Trace Representation"},{"location":"instr/#instrumentation-mechanisms","text":"\ud83d\udccc [To Be Documented] Details about TrainCheck\u2019s instrumentation strategies, supported APIs, and limitations will be covered here later.","title":"Instrumentation Mechanisms"},{"location":"successful-stories/","text":"\u2705 TrainCheck: Real-World Success Stories TrainCheck proactively detects silent failures in deep learning training by inferring and checking invariants. Below are real-world cases where TrainCheck caught critical bugs that would have otherwise wasted months of compute and effort. This page highlights several silent errors that TrainCheck detected in real-world scenarios. For a comprehensive list of issues and detailed analysis, see our research paper: Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks . \ud83e\udde8 Case 1: Silent Weight Divergence in BLOOM-176B The Story: While training the BLOOM-176B model, a subtle optimizer bug caused model weights to silently diverge across GPUs. All standard metrics and logs appeared normal, masking the critical issue. The Risk: 3.5 months of training time on 384 A100 GPUs, with invalid checkpoints. The Delay: It took developers 15 days to notice and diagnose the problem. TrainCheck's Role: TrainCheck would have instantly detected this divergence with its parameter consistency invariant, saving the project from a massive setback. Source: BigScience BLOOM-176B Training Chronicles \ud83e\udde0 Case 2: Silent Gradient Application Failure The Story: A user reported their model performance degrading over time, even though the gradient norm seemed stable. The community suspected issues with learning rates, data, or hardware. The Root Cause: Gradients were not being applied to the model weights due to incorrect logic in a multi-GPU wrapper. TrainCheck's Role: TrainCheck immediately flagged the root cause, revealing that despite gradient calculations, no actual model updates were happening. Source: Community Discussion on X \u2753 Case 3: The Flat Loss Mystery The Story: A user experienced a completely flat loss curve, indicating the model was not learning at all. The cause was unclear, with suspicions pointing to the model architecture or optimizer configuration. The Root Cause: The model and optimizer were incorrectly wrapped for Fully Sharded Data Parallel (FSDP) training, preventing optimizer.step() from updating model parameters. TrainCheck's Role: TrainCheck identified the problem instantly by verifying that zero_grad() and step() calls resulted in zero actual model changes . Source: HuggingFace Accelerate Issue #2665","title":"Success Stories"},{"location":"successful-stories/#traincheck-real-world-success-stories","text":"TrainCheck proactively detects silent failures in deep learning training by inferring and checking invariants. Below are real-world cases where TrainCheck caught critical bugs that would have otherwise wasted months of compute and effort. This page highlights several silent errors that TrainCheck detected in real-world scenarios. For a comprehensive list of issues and detailed analysis, see our research paper: Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks .","title":"\u2705 TrainCheck: Real-World Success Stories"},{"location":"successful-stories/#case-1-silent-weight-divergence-in-bloom-176b","text":"The Story: While training the BLOOM-176B model, a subtle optimizer bug caused model weights to silently diverge across GPUs. All standard metrics and logs appeared normal, masking the critical issue. The Risk: 3.5 months of training time on 384 A100 GPUs, with invalid checkpoints. The Delay: It took developers 15 days to notice and diagnose the problem. TrainCheck's Role: TrainCheck would have instantly detected this divergence with its parameter consistency invariant, saving the project from a massive setback. Source: BigScience BLOOM-176B Training Chronicles","title":"\ud83e\udde8 Case 1: Silent Weight Divergence in BLOOM-176B"},{"location":"successful-stories/#case-2-silent-gradient-application-failure","text":"The Story: A user reported their model performance degrading over time, even though the gradient norm seemed stable. The community suspected issues with learning rates, data, or hardware. The Root Cause: Gradients were not being applied to the model weights due to incorrect logic in a multi-GPU wrapper. TrainCheck's Role: TrainCheck immediately flagged the root cause, revealing that despite gradient calculations, no actual model updates were happening. Source: Community Discussion on X","title":"\ud83e\udde0 Case 2: Silent Gradient Application Failure"},{"location":"successful-stories/#case-3-the-flat-loss-mystery","text":"The Story: A user experienced a completely flat loss curve, indicating the model was not learning at all. The cause was unclear, with suspicions pointing to the model architecture or optimizer configuration. The Root Cause: The model and optimizer were incorrectly wrapped for Fully Sharded Data Parallel (FSDP) training, preventing optimizer.step() from updating model parameters. TrainCheck's Role: TrainCheck identified the problem instantly by verifying that zero_grad() and step() calls resulted in zero actual model changes . Source: HuggingFace Accelerate Issue #2665","title":"\u2753 Case 3: The Flat Loss Mystery"},{"location":"technical-doc/","text":"TrainCheck Documentation \ud83d\ude9c This documentation is under construction. We welcome any feedback or questions through GitHub Issues or our Discord server . TrainCheck is a lightweight, invariant-based instrumentation and analysis tool for identifying silent correctness issues in PyTorch training pipelines. It infers behavioral invariants from correct reference runs (e.g., official examples or clean configurations), then checks other scripts for behavioral violations. TrainCheck is designed to be minimally intrusive\u2014requiring no code modifications or rewrites of training logic. \ud83d\udd27 System Overview TrainCheck consists of three core command-line utilities: traincheck-collect \u2013 Instruments a training pipeline and collects trace logs. traincheck-infer \u2013 Infers behavioral invariants from the collected traces. traincheck-check \u2013 Checks new traces against a set of inferred invariants to detect silent issues. TrainCheck workflows are organized into two stages: \ud83e\uddea Inference Stage traincheck-collect collects execution traces from reference training pipelines. traincheck-infer analyzes traces and produces invariants that describe correct/expected runtime behavior. \ud83d\udea8 Checking Stage traincheck-collect is used again to trace the target (possibly buggy) pipeline. traincheck-check verifies whether the collected trace violates any of the known invariants. \ud83d\udce6 Pre-Inferred Invariants (On the Roadmap) In common use cases, users typically do not need to infer invariants manually. TrainCheck provides a high-quality set of pre-inferred invariants that work out-of-the-box with popular libraries such as PyTorch, HuggingFace Transformers, and DeepSpeed. You may still want to run inference in the following cases: - When using certain niche or uncommon features not covered by the default invariants. - When working with custom training stacks outside supported libraries. - When you want to increase specificity by inferring invariants from a set of related, known-good pipelines (e.g. in industrial settings). \ud83d\udcda Component Documentation Each utility is documented separately: Collecting Traces with traincheck-collect Usage, instrumentation caveats, and trace file format. Inferring Invariants with traincheck-infer CLI usage, performance considerations, invariant format, and the inference algorithm (relations, preconditions, etc.). Checking Violations with traincheck-check How to apply invariants to new traces, result interpretation, and result file formats.","title":"Technical Documentation"},{"location":"technical-doc/#traincheck-documentation","text":"\ud83d\ude9c This documentation is under construction. We welcome any feedback or questions through GitHub Issues or our Discord server . TrainCheck is a lightweight, invariant-based instrumentation and analysis tool for identifying silent correctness issues in PyTorch training pipelines. It infers behavioral invariants from correct reference runs (e.g., official examples or clean configurations), then checks other scripts for behavioral violations. TrainCheck is designed to be minimally intrusive\u2014requiring no code modifications or rewrites of training logic.","title":"TrainCheck Documentation"},{"location":"technical-doc/#system-overview","text":"TrainCheck consists of three core command-line utilities: traincheck-collect \u2013 Instruments a training pipeline and collects trace logs. traincheck-infer \u2013 Infers behavioral invariants from the collected traces. traincheck-check \u2013 Checks new traces against a set of inferred invariants to detect silent issues. TrainCheck workflows are organized into two stages: \ud83e\uddea Inference Stage traincheck-collect collects execution traces from reference training pipelines. traincheck-infer analyzes traces and produces invariants that describe correct/expected runtime behavior. \ud83d\udea8 Checking Stage traincheck-collect is used again to trace the target (possibly buggy) pipeline. traincheck-check verifies whether the collected trace violates any of the known invariants.","title":"\ud83d\udd27 System Overview"},{"location":"technical-doc/#pre-inferred-invariants-on-the-roadmap","text":"In common use cases, users typically do not need to infer invariants manually. TrainCheck provides a high-quality set of pre-inferred invariants that work out-of-the-box with popular libraries such as PyTorch, HuggingFace Transformers, and DeepSpeed. You may still want to run inference in the following cases: - When using certain niche or uncommon features not covered by the default invariants. - When working with custom training stacks outside supported libraries. - When you want to increase specificity by inferring invariants from a set of related, known-good pipelines (e.g. in industrial settings).","title":"\ud83d\udce6 Pre-Inferred Invariants (On the Roadmap)"},{"location":"technical-doc/#component-documentation","text":"Each utility is documented separately: Collecting Traces with traincheck-collect Usage, instrumentation caveats, and trace file format. Inferring Invariants with traincheck-infer CLI usage, performance considerations, invariant format, and the inference algorithm (relations, preconditions, etc.). Checking Violations with traincheck-check How to apply invariants to new traces, result interpretation, and result file formats.","title":"\ud83d\udcda Component Documentation"},{"location":"usage-guide/","text":"\ud83e\uddea TrainCheck: Usage Guide TrainCheck helps detect and diagnose silent errors in deep learning training runs\u2014issues that don't crash your code but silently break correctness. \ud83d\ude80 Quick Start Check out the 5-minute guide for a minimal working example. \u2705 Common Use Cases TrainCheck is useful when your training process doesn\u2019t converge, behaves inconsistently, or silently fails. It can help you: Monitor long-running training jobs and catch issues early Debug finished runs and pinpoint where things went wrong Sanity-check new pipelines, code changes, or infrastructure upgrades TrainCheck detects a range of correctness issues\u2014like misused APIs, incorrect training logic, or hardware faults\u2014without requiring labels or modifications to your training code. While TrainCheck focuses on correctness, it\u2019s also useful for ruling out bugs so you can focus on algorithm design with confidence. \ud83e\udde0 Tips for Effective Use Use short runs to reduce overhead. If your hardware is stable, you can validate just the beginning of training. Use smaller models and fewer iterations to speed up turnaround time. Choose good reference runs for inference. If you have a past run of the same code that worked well, just use that. You can also use small-scale example pipelines that cover different features of the framework (e.g., various optimizers, mixed precision, optional flags). If you're debugging a new or niche feature with limited history, try using the official example as a reference. Even if the example is not bug-free, invariant violations can still highlight behavioral differences between your run and the example, helping you debug faster. Minimize scale when collecting traces. Shrink the pipeline by using a smaller model, running for only ~10 iterations, and using the minimal necessary compute setup (e.g., 2 nodes for distributed training). \ud83d\udea7 Current Limitations Eager mode only. TrainCheck instrumentor currently works only in PyTorch eager mode. Features like torch.compile are disabled during instrumentation. Not fully real-time (yet). Invariant checking is semi-online. Full real-time support is planned but not yet available.","title":"Usage Tips"},{"location":"usage-guide/#traincheck-usage-guide","text":"TrainCheck helps detect and diagnose silent errors in deep learning training runs\u2014issues that don't crash your code but silently break correctness.","title":"\ud83e\uddea TrainCheck: Usage Guide"},{"location":"usage-guide/#quick-start","text":"Check out the 5-minute guide for a minimal working example.","title":"\ud83d\ude80 Quick Start"},{"location":"usage-guide/#common-use-cases","text":"TrainCheck is useful when your training process doesn\u2019t converge, behaves inconsistently, or silently fails. It can help you: Monitor long-running training jobs and catch issues early Debug finished runs and pinpoint where things went wrong Sanity-check new pipelines, code changes, or infrastructure upgrades TrainCheck detects a range of correctness issues\u2014like misused APIs, incorrect training logic, or hardware faults\u2014without requiring labels or modifications to your training code. While TrainCheck focuses on correctness, it\u2019s also useful for ruling out bugs so you can focus on algorithm design with confidence.","title":"\u2705 Common Use Cases"},{"location":"usage-guide/#tips-for-effective-use","text":"Use short runs to reduce overhead. If your hardware is stable, you can validate just the beginning of training. Use smaller models and fewer iterations to speed up turnaround time. Choose good reference runs for inference. If you have a past run of the same code that worked well, just use that. You can also use small-scale example pipelines that cover different features of the framework (e.g., various optimizers, mixed precision, optional flags). If you're debugging a new or niche feature with limited history, try using the official example as a reference. Even if the example is not bug-free, invariant violations can still highlight behavioral differences between your run and the example, helping you debug faster. Minimize scale when collecting traces. Shrink the pipeline by using a smaller model, running for only ~10 iterations, and using the minimal necessary compute setup (e.g., 2 nodes for distributed training).","title":"\ud83e\udde0 Tips for Effective Use"},{"location":"usage-guide/#current-limitations","text":"Eager mode only. TrainCheck instrumentor currently works only in PyTorch eager mode. Features like torch.compile are disabled during instrumentation. Not fully real-time (yet). Invariant checking is semi-online. Full real-time support is planned but not yet available.","title":"\ud83d\udea7 Current Limitations"},{"location":"assets/examples/traincheck-collect/gpt2-pretrain-config/","text":"Language model pretraining script from the official examples of the transformers library. Trains GPT-2 on Modifications: 1. 10 steps per training/testing epoch. 2. stage annotations 3. skip instrumentation for the tokenization step","title":"Index"}]}