{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Invariant Checking &amp; Observability for AI Training</p> <p>Stop flying blind. Validate training dynamics, catch silent errors, and debug with confidence automatically.</p> <p>Get Started 5-Min Tutorial View on GitHub</p> \u2705 Continuous Invariant Checking <p>TrainCheck validates the \"physics\" of your training process in real-time. It ensures your model adheres to learned invariants (such as gradient norms, tensor shapes, and update magnitudes) effectively catching silent corruption before it wastes GPU hours.</p> \ud83d\ude80 Holistic Observability <p>Traditional tools only show you if your model crashed. TrainCheck shows you why it's degrading, analyzing internal state dynamics that loss curves miss.</p> \ud83e\udde0 Zero-Config Validation <p>No manual tests required. TrainCheck automatically learns the invariants of your specific model from healthy runs and flags deviations instantly.</p> \u26a1 Universal Compatibility <p>Drop-in support for PyTorch, Hugging Face, and industry-class workloads using DeepSpeed/Megatron and more.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<ol> <li>Instrument: We wrap your training loop with lightweight probes. No code changes needed.</li> <li>Learn: We analyze correct runs to infer invariants (mathematical rules of healthy training).</li> <li>Check: We monitor new runs in real-time, verifying every step against learned invariants to catch silent logic bugs and hardware faults.</li> </ol>"},{"location":"#try-traincheck","title":"\ud83d\udd25 Try TrainCheck","text":"<p>Work through 5\u2011Minute Experience with TrainCheck. You\u2019ll learn how to:    - Instrument a training script and collect a trace    - Automatically infer invariants    - Uncover silent bugs in the training script</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation Guide</li> <li>Usage Guide: Scenarios and Limitations</li> <li>TrainCheck Technical Doc</li> <li>TrainCheck Dev RoadMap</li> </ul>"},{"location":"#status","title":"Status","text":"<p>TrainCheck is under active development. Please join our \ud83d\udcac Discord server or file a GitHub issue for support.  We welcome feedback and contributions from early adopters.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome and value any contributions and collaborations. Please check out Contributing to TrainCheck for how to get involved.</p>"},{"location":"#license","title":"License","text":"<p>TrainCheck is licensed under the Apache License 2.0.</p>"},{"location":"#citation","title":"Citation","text":"<p>If TrainCheck is relevant to your work, please cite our paper: <pre><code>@inproceedings{TrainCheckOSDI2025,\n  author = {Jiang, Yuxuan and Zhou, Ziming and Xu, Boyu and Liu, Beijie and Xu, Runhui and Huang, Peng},\n  title = {Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks},\n  booktitle = {Proceedings of the 19th USENIX Symposium on Operating Systems Design and Implementation},\n  series = {OSDI '25},\n  month = {July},\n  year = {2025},\n  address = {Boston, MA, USA},\n  publisher = {USENIX Association},\n}\n</code></pre></p>"},{"location":"#artifact-evaluation","title":"Artifact Evaluation","text":"<p>\ud83d\udd75\ufe0f\u200d\u2640\ufe0f OSDI AE members, please see TrainCheck AE Guide.</p>"},{"location":"5-min-tutorial/","title":"Quick Start: TrainCheck Tutorial","text":"<p>In this tutorial, you will use TrainCheck to detect &amp; diagnose the real\u2011world silent issue in PyTorch\u2011Forum\u201184911: Obtaining abnormal changes in loss and accuracy, with invariants inferred from PyTorch\u2019s official MNIST example. We\u2019ll refer to this buggy pipeline simply as '84911'.</p> <p>Estimated time: ~5\u00a0minutes (plus model/inference overhead)</p> <p>Prerequisites - A working TrainCheck installation - <code>efficientnet_pytorch</code> and <code>torchvision</code> (install via <code>pip3 install efficientnet_pytorch torchvision</code>) - A Linux machine with a CUDA\u2011enabled GPU   - \ud83d\udca1 Tip: If you don\u2019t have a CUDA GPU, you can still run this tutorial on CPU\u2014it\u2019ll just take longer.</p>"},{"location":"5-min-tutorial/#background-whats-wrong-with-84911","title":"Background: What\u2019s wrong with 84911?","text":"<p>The author attempts to finetune a pretrained <code>EfficientNet_b0</code> model for image classification but notices that\u2014even after many epochs\u2014the training loss barely improves (x\u2011axis = epoch, y\u2011axis = loss):</p> <p>It appears from the plot that the model is still being trained, but somehow it is just not improving meaningfully.  The original issue post discussed adjusting learning rate, and training for longer epochs. However, the issue remained unresolved.</p> <p>We have diagnosed the root cause for you. You can look at it now or come at it yourself with the help of TrainCheck.</p> Click here to reveal the root cause.  The developer, for some reason, sets `requires_grad` to `False` for all parameters except for batch normalization layers, yet only initializes the optimizer with the final fully-connected layer.  <pre><code>for name,param in model_transfer.module.named_parameters():\n    if(\"bn\" not in name):\n        param.requires_grad = False\n\nfor param in model_transfer.module._fc.parameters():\n    param.requires_grad = False\n\n...\noptimizer_transfer = optim.Adam(model_transfer.module._fc.parameters(), lr=0.001)\n</code></pre>  This freeze logic leaves virtually no trainable parameters. Since batch normalization layers still update their running mean/variance each forward pass, the loss/accuracy curves drift slightly instead of remaining flat\u2014masking the lack of actual learning. Logging metrics only once per epoch further hides the anomalies, so the initialization bug only becomes apparent after several epochs have already run."},{"location":"5-min-tutorial/#detecting-diagnosing-84911","title":"Detecting &amp; Diagnosing 84911","text":"<p>We will infer invariants from the mnist.py, a very simple PyTorch-official pipeline that trains a 2-layer CNN on MNIST, to showcase TrainCheck's capability.</p>"},{"location":"5-min-tutorial/#1-download-example-scripts","title":"1. Download example scripts","text":"<pre><code>cd ~\nmkdir traincheck-tutorial &amp;&amp; cd traincheck-tutorial\nwget https://raw.githubusercontent.com/OrderLab/traincheck/main/docs/assets/code/mnist.py\nwget https://raw.githubusercontent.com/OrderLab/traincheck/main/docs/assets/code/84911.py\n</code></pre> <p>\ud83d\udca1 If the wget links above fail (e.g. due to branch changes or access issues), you can also download the files manually from: - mnist.py - 84911.py</p>"},{"location":"5-min-tutorial/#2-instrument-collect-trace-from-mnistpy-1-minute","title":"2. Instrument &amp; collect trace from <code>mnist.py</code> (~1 minute)","text":"<pre><code>traincheck-collect \\\n  --pyscript mnist.py \\\n  --models-to-track model \\\n  --output-dir traincheck_mnist_trace\n</code></pre> <p>This instruments torch and model in <code>mnist.py</code>, runs it with default arguments, and writes JSON trace files into <code>traincheck_mnist_trace/</code> (\u2248\u00a01\u00a0minute). You\u2019ll see the training logs and any benign PyTorch warnings on stdout.</p>"},{"location":"5-min-tutorial/#3-infer-invariants-from-mnistpy-1-4-minutes","title":"3. Infer Invariants from <code>mnist.py</code> (~1-4 minutes)","text":"<p>We will infer invariants from the trace we just collected using the command below.</p> <p><pre><code>traincheck-infer -f ./traincheck_mnist_trace\n</code></pre> This will produce an invariants.json file (one JSON\u00a0Line per invariant). Verify the count:</p> <pre><code>wc -l invariants.json  # should output ~913\n</code></pre> <p>The generated invariants capture API invocation order, event expectations, and input-output relationships. Since the trace comes from a single, simple script, some invariants may overfit\u2014we\u2019ll cover filtering in the next steps.</p>"},{"location":"5-min-tutorial/#4-check-for-silent-issues-in-84911py-with-invariants-5-10-minutes","title":"4. Check for silent issues in 84911.py with invariants (~5-10 minutes)","text":"<p>Note: For this quickstart, we do offline checking for simplicity.</p> <pre><code># trace the buggy pipeline (~5\u00a0minutes)\ntraincheck-collect \\\n  --pyscript 84911.py \\\n  --models-to-track model_transfer \\\n  --output-dir traincheck_84911_trace\n\n# run the checker (~2\u20136\u00a0minutes)\ntraincheck-check \\\n  --trace-folders traincheck_84911_trace \\\n  --invariants invariants.json\n</code></pre> <p>The output of the traincheck-check command will contain this in the end: <pre><code>Checking finished. 913 invariants checked\nTotal failed invariants: 25/913\nTotal passed invariants: 888/913 # number here includes both passed and not triggered invariants\nTotal invariants that are not triggered: 552/913\n</code></pre></p> <p>361 invariants were checked on <code>84911.py</code>, and 25 got violated.</p> <p>The checker writes the full results to a folder named <code>traincheck_checker_results_&lt;timestamp&gt;</code>, containing the results (<code>failed.log</code>, <code>not_triggered.log</code>, <code>passed.log</code>, depending if an invariant is violated, not checked at all, or checked and passed.), and a copy of <code>invariants.json</code>.</p>"},{"location":"5-min-tutorial/#5-detection-diagnosis","title":"5. Detection &amp; Diagnosis","text":"<p>Ready to play detective? \ud83d\udd0d TrainCheck flagged 25 invariant violations right at the start of training\u2014well before the fluctuating loss/accuracy pattern was observed. Let\u2019s interpret the results first and then if you want to learn more.</p> <p>1. Quick filter - Event\u2011order invariants noise (20/25 failures):   - <code>FunctionCoverRelation</code> and <code>FunctionLeadRelation</code> invariants (basically specifying API invocation orders) overfit our single demo trace.   - Examples: strict ordering of <code>torch.distributed.is_initialized</code> (6 invariants violated but we are not even doing distributed training in 84911!) or <code>torch.cuda.is_initialized</code> (another 7 invariants violated but shouldn't matter at all for training).   - Ignore these.</p> <p>2. Spot the real issues - APIContainRelation invariant violations (5/25):   1. <code>Optimizer.zero_grad</code> did not reset <code>.grad</code> from non-zero to zero/None.      - Implies either no gradients were ever populated or zeroing silently failed.   2. <code>Adadelta.step</code> did not update <code>.data</code> of any parameters.      - Indicates the optimizer had no trainable parameters to touch.  </p> <p>\ud83e\udde9 Putting it all together: The optimizer wasn\u2019t updating anything because\u2026 the parameters it received had requires_grad=False. Go to Background: What\u2019s wrong in 84911? to see the full root cause confirmed and explained.</p> \ud83d\ude4b Click here to learn how to inspect the raw results  Open the `failed_*.log` file\u2014TrainCheck writes each violated invariant as a standalone JSON object. For example:  <pre><code>{\n  \"invariant\": { \u2026 },\n  \"check_passed\": false,\n  \"triggered\": true,\n  \"detection_time\": 18343040207314524,\n  \"detection_time_percentage\": 0.1805434802294184,\n  \"trace\": [\n    {\n      \"func_call_id\": \"...\",\n      \"meta_vars.step\": 1,\n      \"function\": \"torch.optim.optimizer.Optimizer.zero_grad\",\n      \u2026\n    }\n    ...\n  ]\n}\n</code></pre>  - `\"invariant\"` shows the invariant that this result correspond to, and  - `\"trace\"` corresponds to the specific trace that caused the violation. - `\"check_passed\": false` means that the invariant has been violated. - `\"triggered\": true` means that the invariant has been checked at least once, which is always the case if the invariant is violated. - `\"detection_time\"` is the timestamp when the violation happened. - `\"detection_percentage\"` is the percentage of this timestamp in the entire duration of the training, and gives a rough impression of how early the detection is. We are working on providing a field `\"detection_step\"` that pinpoints on which step the issue is detected. For now, to get \"step\", you can look at the `\"trace\"` field and look for step numbers in `\"meta_vars\"`.  For example, the \"`optimizer.zero_grad` did **not** reset `.grad` from non-zero to zero/None\" is represented as:  <pre><code>{\n    \"invariant\": {\n        \"relation\": \"APIContainRelation\",\n        \"params\": [\n            {\n                \"param_type\": \"APIParam\",\n                \"api_full_name\": \"torch.optim.optimizer.Optimizer.zero_grad\"\n            },\n            {\n                \"param_type\": \"VarTypeParam\",\n                \"var_type\": \"torch.nn.Parameter\",\n                \"attr_name\": \"grad\",\n                \"pre_value\": \"non_zero\",\n                \"post_value\": null\n            }\n        ],\n        \"precondition\": {\n            \"parent_func_call_pre\": {\n                \"inverted\": true,\n                \"preconditions\": [\n                    {\n                        \"clauses\": [\n                            {\n                                \"type\": \"constant\",\n                                \"prop_name\": \"meta_vars.step\",\n                                \"additional_path\": \"None\",\n                                \"prop_dtype\": \"int\",\n                                \"values\": [\n                                    0\n                                ]\n                            }\n                        ]\n                    },\n                    {\n                        \"clauses\": [\n                            {\n                                \"type\": \"constant\",\n                                \"prop_name\": \"meta_vars.stage\",\n                                \"additional_path\": \"None\",\n                                \"prop_dtype\": \"str\",\n                                \"values\": [\n                                    \"testing\",\n                                    \"init\"\n                                ]\n                            }\n                        ]\n                    }\n                ]\n            }\n        },\n        \"num_positive_examples\": 20,\n        \"num_negative_examples\": 1\n    },\n    \"check_passed\": false,\n    \"triggered\": true,\n    \"detection_time\": 18343039144178123,\n    \"detection_time_percentage\": 0.16245728748900484,\n    \"trace\": [\n        {\n            \"func_call_id\": \"3f7265b362c34725b412cf693ceea8f3_18343039144122325\",\n            \"thread_id\": 140156043466560,\n            \"process_id\": 1263911,\n            \"meta_vars.step\": 1,\n            \"type\": \"function_call (pre)\",\n            \"function\": \"torch.optim.optimizer.Optimizer.zero_grad\",\n            \"is_bound_method\": true,\n            \"obj_id\": 140152527083248,\n            \"args\": {\n                \"0\": {\n                    \"torch.optim.adadelta.Adadelta\": {}\n                }\n            },\n            \"kwargs\": {},\n            \"time\": 18343039144178123,\n            \"return_values\": NaN,\n            \"var_name\": NaN,\n            \"var_type\": NaN,\n            \"mode\": NaN,\n            \"dump_loc\": NaN,\n            \"attributes._TRAINCHECK_data_ID\": NaN,\n            \"attributes.data\": NaN,\n            \"attributes.dtype\": NaN,\n            \"attributes.grad\": NaN,\n            \"attributes.grad_fn\": NaN,\n            \"attributes.is_cpu\": NaN,\n            \"attributes.is_cuda\": NaN,\n            \"attributes.is_ipu\": NaN,\n            \"attributes.is_leaf\": NaN,\n            \"attributes.is_meta\": NaN,\n            \"attributes.is_mkldnn\": NaN,\n            \"attributes.is_mps\": NaN,\n            \"attributes.is_mtia\": NaN,\n            \"attributes.is_nested\": NaN,\n            \"attributes.is_ort\": NaN,\n            \"attributes.is_quantized\": NaN,\n            \"attributes.is_sparse\": NaN,\n            \"attributes.is_sparse_csr\": NaN,\n            \"attributes.is_vulkan\": NaN,\n            \"attributes.is_xla\": NaN,\n            \"attributes.is_xpu\": NaN,\n            \"attributes.itemsize\": NaN,\n            \"attributes.name\": NaN,\n            \"attributes.nbytes\": NaN,\n            \"attributes.ndim\": NaN,\n            \"attributes.requires_grad\": NaN,\n            \"attributes.retains_grad\": NaN,\n            \"attributes.shape\": NaN,\n            \"attributes._TRAINCHECK_grad_ID\": NaN,\n            \"exception\": NaN,\n            \"exception_msg\": NaN,\n            \"proxy_obj_names\": NaN\n        }\n    ]\n}\n</code></pre>  The invariant specifies that `torch.optim.optimizer.Optimizer.zero_grad` (*the first invariant parameter*) invocations must change `.grad` from a non-zero value to `null` (*the second invariant parameter*), except during the very first iteration (*i.e. before any backward pass when no `.grad` exists, as per the invariant precondition*). We then inspect the trace record where the invariant is violated: `meta_vars.step` is 1, indicating detection occurred in the second training iteration. You can review the other results in the same way.  The `NaN` values denote missing fields and can be safely ignored.   <p>\ud83c\udf89 You just used TrainCheck to catch a real-world silent bug before it impacted training!</p>"},{"location":"ae-eval-s5.1-silent-issue-detection/","title":"Eval: Silent Issue Detection","text":"<p>\u23f3 Estimated Completion Time: ~30 minutes</p>"},{"location":"ae-eval-s5.1-silent-issue-detection/#goal","title":"\ud83c\udfaf Goal","text":"<p>TrainCheck detects 18 real-world silent issues in our evaluation. Your goal in this artifact evaluation is to verify detection for the subset of issues that are currently AE-supported (see bug table below).</p> <p>For each supported bug, you should confirm:</p> <p>\u2705 TrainCheck successfully detects the issue by reporting one or more invariant violations on the provided trace.</p> <p>The artifact provides all necessary resources to automate this confirmation. Additional insights\u2014such as when the issue is triggered and how the violation aligns with the root cause\u2014can be explored by examining the scripts, logs, or violation reports, though they are not required for core validation.</p>"},{"location":"ae-eval-s5.1-silent-issue-detection/#resources-provided","title":"\ud83d\udcc2 Resources Provided","text":"<p>All files are located in the <code>TrainCheck-Evaluation-Workloads</code> repository.</p> Resource Description Curated Invariants Small set of known-effective invariants per bug. Pre-collected Traces Captured execution traces from the buggy pipelines. Silent Issue Reproduction Scripts and Descriptions https://github.com/OrderLab/TrainCheck-Evaluation-Workloads/tree/main/silent-issue-detection/bug-reprod-scripts"},{"location":"ae-eval-s5.1-silent-issue-detection/#silent-issue-summary-table","title":"\ud83d\udc1b Silent Issue Summary Table","text":"Bug ID Failure Location AE? AE Limitation (if any) <code>baichuan2-86</code> HW/Driver \u2705 Yes Similar root cause as pytorch-84803, reuses pytorch-104336 trace <code>deepspeed-1801</code> Framework \u2705 Yes <code>deepspeed-5794</code> Framework \u274c No Invariant relation still under evaluation <code>lightning-thunder-725</code> Framework \u2705 Yes <code>mmpretrain-702</code> Framework \u2705 Yes <code>pytorch-51800</code> Framework \u2705 Yes <code>pytorch-84803</code> HW/Driver \u2705 Yes Different root cause, but low-level manifest is similar, reuses pytorch-104336 trace <code>pytorch-96600</code> HW/Driver \u2705 Yes Similar root cause as pytorch-84803 reuses pytorch-104336 trace <code>pytorch-104336</code> Framework \u2705 Yes <code>pytorch-115607</code> Compiler \u2705 Yes <code>pytorch-forum-84911</code> User Code \u2705 Yes <code>stackoverflow-60335387</code> User Code \u2705 Yes <code>stackoverflow-67180955</code> Framework \u274c No Requires older Python version no longer supported <code>transformers-17877</code> Framework \u2705 Yes <code>transformers-23723</code> Framework \u2705 Yes <code>transformers-33844</code> Framework \u2705 Yes <code>transformers-34204</code> Framework \u274c No Invariant support still in progress <code>x-jxmnop-ddp-out-of-sync</code> User Code \u2705 Yes Reuses pytorch-104336 trace <p>We currently support 15 out of 18 bugs for artifact evaluation. You have already detected <code>pytorch-forum-84911</code> in our 5-min tutorial. You will need to detect the rest of the 14 bugs.</p> <p>Bugs not included in this AE release typically depend on: - Unsupported or unstable library versions - Very old Python environments - Invariant support still in development</p> <p>Additionally, a few bugs stem from very specific issues such as faulty hardware, which are inherently difficult to reproduce. For such cases\u2014and for bugs that share the same root cause/manifest\u2014we may provide a shared/simulated trace and a shared invariant that is reused across multiple bug IDs.</p>"},{"location":"ae-eval-s5.1-silent-issue-detection/#reproducing-silent-issue-detection","title":"\ud83e\uddea Reproducing Silent Issue Detection","text":"<p>All steps described below assumes you are already in the <code>TrainCheck-Evaluation-Workloads</code> repo. If not, clone the repository and go to it. <pre><code>git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git\ncd TrainCheck-Evaluation-Workloads\n</code></pre></p> <ol> <li> <p>Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide.</p> </li> <li> <p>Execute <code>ae_detection.sh</code> to automatically apply invariants to the pre-collected trace. This script generates results into a folder named <code>checker_output</code>.    <pre><code>cd silent-issue-detection\nbash ae_detection.sh\n</code></pre></p> </li> </ol>"},{"location":"ae-eval-s5.1-silent-issue-detection/#expected-results","title":"Expected Results","text":"<p>The <code>checker_output</code> folder contains checkering results for each trace. <pre><code>(base) \u279c  checker_output git:(main) tree .\n.\n\u251c\u2500\u2500 invariants.json\n\u251c\u2500\u2500 trace_deepspeed-1801\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_mmpretrain-702\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_pytorch-104336\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_pytorch-115607\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_pytorch-51800\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_stackoverflow-60335387\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_transformers-17877\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_transformers-23723\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u251c\u2500\u2500 trace_transformers-33844\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failed.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 not_triggered.log\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 passed.log\n\u2514\u2500\u2500 trace_x-jxmnop-ddp-out-of-sync\n    \u251c\u2500\u2500 failed.log\n    \u251c\u2500\u2500 not_triggered.log\n    \u2514\u2500\u2500 passed.log\n</code></pre> You want to make sure all <code>failed.log</code> files are non-empty, indicating silent errors detected in these traces.</p> <p>Optionally, we provided a <code>reference_checker_output</code> folder containing the expected detection results. You can compare the checking results you get versus our reference results.</p> <p>When you do this comparison, do keep in mind that your results and our reference results will not be exactly the same, as TrainCheck does not enforce (1) the order of checking when multiple checkable entities are available at the same time, which may lead to invariants violated on different processes or different pairs of variables, (2) the order of fields when serializing checker results. </p> <p>Thus, if you do the comparison in a mechanical way (e.g. via <code>diff -r checker_output reference_checker_output</code>), you might see outputs like this. The differences are expected and benign. <pre><code>(base) \u279c  silent-issue-detection git:(main) diff -r checker_output reference_checker_output\ndiff --color -r checker_output/trace_pytorch-104336/failed.log reference_checker_output/trace_pytorch-104336/failed.log\n75,76c75,76\n&lt;     \"detection_time\": 2437523672783,\n&lt;     \"detection_time_percentage\": 0.11841431018590723,\n---\n&gt;     \"detection_time\": 2437539087694,\n&gt;     \"detection_time_percentage\": 0.11851643004648255,\n81,82c81,82\n&lt;             \"process_id\": 9539,\n&lt;             \"thread_id\": 140711397648192,\n---\n&gt;             \"process_id\": 9591,\n&gt;             \"thread_id\": 140324043503424,\n86c86\n&lt;             \"attributes._TRAINCHECK_data_ID\": 140704882109040,\n---\n&gt;             \"attributes._TRAINCHECK_data_ID\": 140317529048544,\n116,117c116,117\n&lt;             \"time\": 2437523672783,\n&lt;             \"meta_vars._DATA_PARALLEL_RANK\": 4.0,\n---\n&gt;             \"time\": 2437504805077,\n&gt;             \"meta_vars._DATA_PARALLEL_RANK\": 5.0,\n123,124c123,124\n&lt;             \"process_id\": 9429,\n&lt;             \"thread_id\": 140050208577344,\n---\n&gt;             \"process_id\": 9747,\n&gt;             \"thread_id\": 140028492969792,\n128c128\n&lt;             \"attributes._TRAINCHECK_data_ID\": 140043703504144,\n---\n&gt;             \"attributes._TRAINCHECK_data_ID\": 140021978318304,\n158,159c158,159\n&lt;             \"time\": 2437502499438,\n&lt;             \"meta_vars._DATA_PARALLEL_RANK\": 2.0,\n---\n&gt;             \"time\": 2437539087694,\n&gt;             \"meta_vars._DATA_PARALLEL_RANK\": 7.0,\ndiff --color -r checker_output/trace_pytorch-115607/failed.log reference_checker_output/trace_pytorch-115607/failed.log\n43,44c43,44\n&lt;                                     \"init\",\n&lt;                                     \"testing\"\n---\n&gt;                                     \"testing\",\n&gt;                                     \"init\"\n78,80d77\n&lt;             \"exception\": NaN,\n&lt;             \"exception_msg\": NaN,\n&lt;             \"proxy_obj_names\": NaN,\n113c110,113\n&lt;             \"attributes._TRAINCHECK_grad_ID\": NaN\n---\n&gt;             \"attributes._TRAINCHECK_grad_ID\": NaN,\n&gt;             \"exception\": NaN,\n&gt;             \"exception_msg\": NaN,\n&gt;             \"proxy_obj_names\": NaN\n180,182d179\n&lt;             \"exception\": NaN,\n&lt;             \"exception_msg\": NaN,\n&lt;             \"proxy_obj_names\": NaN,\n215c212,215\n&lt;             \"attributes._TRAINCHECK_grad_ID\": NaN\n---\n&gt;             \"attributes._TRAINCHECK_grad_ID\": NaN,\n&gt;             \"exception\": NaN,\n&gt;             \"exception_msg\": NaN,\n&gt;             \"proxy_obj_names\": NaN\n261,262c261,262\n&lt;                                     \"init\",\n&lt;                                     \"testing\"\n---\n&gt;                                     \"testing\",\n&gt;                                     \"init\"\n296,298d295\n&lt;             \"exception\": NaN,\n&lt;             \"exception_msg\": NaN,\n&lt;             \"proxy_obj_names\": NaN,\n331c328,331\n&lt;             \"attributes._TRAINCHECK_grad_ID\": NaN\n---\n&gt;             \"attributes._TRAINCHECK_grad_ID\": NaN,\n&gt;             \"exception\": NaN,\n&gt;             \"exception_msg\": NaN,\n&gt;             \"proxy_obj_names\": NaN\ndiff --color -r checker_output/trace_pytorch-51800/failed.log reference_checker_output/trace_pytorch-51800/failed.log\n34,56d33\n&lt;             \"func_call_id\": \"b39a4a81b2c24473ba916ab1832fbf12_19876858668012869\",\n&lt;             \"thread_id\": 140254285555520,\n&lt;             \"process_id\": 3499981,\n&lt;             \"meta_vars.step\": 0,\n&lt;             \"type\": \"function_call (pre)\",\n&lt;             \"function\": \"torch.nn.modules.module.Module.eval\",\n&lt;             \"is_bound_method\": true,\n&lt;             \"obj_id\": 140250960790096,\n&lt;             \"args\": {\n&lt;                 \"0\": {\n&lt;                     \"__main__.SimpleCNN\": {\n&lt;                         \"call_super_init\": false,\n&lt;                         \"dump_patches\": false,\n&lt;                         \"training\": true\n&lt;                     }\n&lt;                 }\n&lt;             },\n&lt;             \"kwargs\": {},\n&lt;             \"time\": 19876858668088743,\n&lt;             \"return_values\": NaN,\n&lt;             \"exception\": NaN,\n&lt;             \"exception_msg\": NaN,\n&lt;             \"proxy_obj_names\": NaN,\n60a38,41\n&gt;             \"process_id\": 3499981,\n&gt;             \"thread_id\": 140254285555520,\n&gt;             \"time\": 19876858668088743,\n&gt;             \"meta_vars.step\": 0,\n89c70,89\n&lt;             \"attributes._TRAINCHECK_grad_ID\": NaN\n---\n&gt;             \"type\": \"function_call (pre)\",\n&gt;             \"attributes._TRAINCHECK_grad_ID\": NaN,\n&gt;             \"func_call_id\": \"b39a4a81b2c24473ba916ab1832fbf12_19876858668012869\",\n&gt;             \"function\": \"torch.nn.modules.module.Module.eval\",\n&gt;             \"is_bound_method\": true,\n&gt;             \"obj_id\": 140250960790096,\n&gt;             \"args\": {\n&gt;                 \"0\": {\n&gt;                     \"__main__.SimpleCNN\": {\n&gt;                         \"call_super_init\": false,\n&gt;                         \"dump_patches\": false,\n&gt;                         \"training\": true\n&gt;                     }\n&gt;                 }\n&gt;             },\n&gt;             \"kwargs\": {},\n&gt;             \"return_values\": NaN,\n&gt;             \"exception\": NaN,\n&gt;             \"exception_msg\": NaN,\n&gt;             \"proxy_obj_names\": NaN\ndiff --color -r checker_output/trace_transformers-33844/failed.log reference_checker_output/trace_transformers-33844/failed.log\n244,246d243\n&lt;                 \"enabled\": {\n&lt;                     \"bool\": true\n&lt;                 },\n250a248,250\n&gt;                     \"bool\": true\n&gt;                 },\n&gt;                 \"enabled\": {\ndiff --color -r checker_output/trace_x-jxmnop-ddp-out-of-sync/failed.log reference_checker_output/trace_x-jxmnop-ddp-out-of-sync/failed.log\n81,82c81,82\n&lt;             \"process_id\": 89557,\n&lt;             \"thread_id\": 140661207828288,\n---\n&gt;             \"process_id\": 89558,\n&gt;             \"thread_id\": 140625926412096,\n85c85\n&lt;             \"meta_vars._DATA_PARALLEL_RANK\": \"0\",\n---\n&gt;             \"meta_vars._DATA_PARALLEL_RANK\": \"1\",\n87c87\n&lt;             \"attributes._TRAINCHECK_data_ID\": 140656561409856,\n---\n&gt;             \"attributes._TRAINCHECK_data_ID\": 140621279056480,\n117c117\n&lt;             \"time\": 123297988837864,\n---\n&gt;             \"time\": 123299970638648,\n123,124c123,124\n&lt;             \"process_id\": 89558,\n&lt;             \"thread_id\": 140625926412096,\n---\n&gt;             \"process_id\": 89557,\n&gt;             \"thread_id\": 140661207828288,\n127c127\n&lt;             \"meta_vars._DATA_PARALLEL_RANK\": \"1\",\n---\n&gt;             \"meta_vars._DATA_PARALLEL_RANK\": \"0\",\n129c129\n&lt;             \"attributes._TRAINCHECK_data_ID\": 140621279058160,\n---\n&gt;             \"attributes._TRAINCHECK_data_ID\": 140656561411776,\n159c159\n&lt;             \"time\": 123299970638648,\n---\n&gt;             \"time\": 123297988837864,\n</code></pre></p>"},{"location":"ae-eval-s5.3-transferability/","title":"Eval: Transferability","text":"<p>\u23f3 Estimated Completion Time: 40 minutes - Environment Setup: ~10 minutes - Trace Collection: ~10 minutes - Invariant Inference: ~20 minutes</p>"},{"location":"ae-eval-s5.3-transferability/#goal","title":"\ud83c\udfaf Goal","text":"<p>This evaluation measures the transferability of invariants inferred by TrainCheck across library versions and training environments. The results to be reproduced correspond to the final paragraph of Section 5.3 of the paper.</p> <p>Other claims in Section 5.3\u2014specifically, that invariants inferred from reference pipelines can detect all known bugs\u2014are validated as part of the Silent Issue Detection Evaluation.</p>"},{"location":"ae-eval-s5.3-transferability/#resources-scripts","title":"\ud83d\udcc2 Resources &amp; Scripts","text":"<ul> <li>Automation Script:  </li> <li><code>transferability/ae_transferability.sh</code> Runs the full transferability evaluation pipeline described in Section 5.3 of the paper. It executes invariant inference, applies inferred invariants to other pipelines, and collects applicability (invariant should be checked and not cause false alarms) statistics.</li> <li><code>transferability/install-traincheck-torch251-cu121.sh</code> Creates a conda environment named traincheck-torch251 with Python 3.10 and installs TrainCheck from the latest GitHub version.</li> <li><code>transferability/install-traincheck-torch251-cu118.sh</code> Same as above but installs the CUDA 118 version of PyTorch 2.5.1.</li> </ul> <p>This evaluation uses the GCN training pipeline from PyTorch's official examples, tested across different PyTorch versions. The pipeline is included in the artifact repository and will be automatically handled by the script\u2014no manual setup is required.</p>"},{"location":"ae-eval-s5.3-transferability/#how-to-run","title":"\ud83d\udee0 How to Run","text":"<ol> <li> <p>Go to TrainCheck-Evaluation-Workloads/transferability. Clone the repo if you do not have it.     <pre><code>git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git\ncd TrainCheck-Evaluation-Workloads/transferability\n</code></pre></p> </li> <li> <p>Create a new conda environment named <code>traincheck-torch251</code>, and install PyTorch 2.5.1 along with TrainCheck.  </p> <p>Run the appropriate script based on your GPU's CUDA compatibility (likely executing either will be fine): <pre><code>bash install-traincheck-torch251-cu121.sh  # for CUDA 12.1\n</code></pre> or <pre><code>bash install-traincheck-torch251-cu118.sh  # for CUDA 11.8\n</code></pre></p> </li> <li> <p>Run the transferability evaluation script:     <pre><code>bash ae_transferability.sh\n</code></pre></p> <p>This script will:   - Collect traces from the GCN training pipeline using both PyTorch 2.2.2 and 2.5.1.   - Infer invariants from the 2.2.2 version.   - Apply them to the 2.5.1 trace to assess transferability.</p> </li> </ol> <p>\u26a0\ufe0f Note: The scripts above assume that Conda is installed at <code>~/miniconda3</code>. If your installation is located elsewhere (e.g., <code>~/anaconda3</code>), please modify the first line of the scripts to reflect your actual Conda path.</p> <p>We also assume that you have already installed TrainCheck in an environment named <code>traincheck</code> prior to running these scripts.</p>"},{"location":"ae-eval-s5.3-transferability/#how-to-verify-the-results","title":"\ud83e\uddd0 How to Verify the Results","text":"<p>After the script finishes, it generates a file named <code>applied_rates.csv</code> that reports the percentage of applicable invariants. You should verify that the rate is no lower than the paper\u2019s reported value:</p> <p>\ud83d\udfe2 \"94.2% remain valid and applicable up to PyTorch 2.5.1\" (Section 5.3)</p>"},{"location":"ae-eval-s5.3-transferability/#notes-troubleshooting","title":"\u26a0\ufe0f Notes &amp; Troubleshooting","text":"<p>If invariant inference or checking fails, please first verify that the environment is correctly set up (e.g., correct PyTorch version, dependencies installed). Then try re-running <code>ae_transferability.py</code>.</p> <p>If the issue persists, please contact us for assistance\u3002</p>"},{"location":"ae-eval-s5.4-fp-rate/","title":"Eval: False Positive Rate","text":"<p>\u23f3 Estimated Completion Time: 2 hour. - Trace Collection: ~10 minutes - Invariant Inference &amp; Checking: ~1.5 hours</p>"},{"location":"ae-eval-s5.4-fp-rate/#goal","title":"\ud83c\udfaf Goal","text":"<p>This evaluation measures the false positive rate of alarms reported by TrainCheck's invariants. The target results are discussed in the main text of Section 5.4 of the paper.</p>"},{"location":"ae-eval-s5.4-fp-rate/#resources-scripts","title":"\ud83d\udcc2 Resources &amp; Scripts","text":"<ul> <li>Automation Scripts:</li> <li><code>TrainCheck-Evaluation-Workloads/fp_rate/ae_fp.py</code>: The script to collect traces, perform invariant inference, and check invariants on supposedly-correct programs to see if there are any false alarms.</li> <li> <p><code>TrainCheck-Evaluation-Workloads/fp_rate/compute_fp_rate.py</code>: The script to compute false positive rates from the invariant checking results.</p> </li> <li> <p>Workloads:</p> </li> <li>The evaluation uses official PyTorch training pipelines located at <code>TrainCheck-Evaluation-Workloads/fp_rate/workloads</code>.     We have shortened the training runs for faster execution.     For AE purposes, you do not need to modify or understand the workload code\u2014<code>ae_fp.py</code> will automatically handle the entire process.</li> </ul>"},{"location":"ae-eval-s5.4-fp-rate/#how-to-run","title":"\ud83d\udee0 How to Run","text":"<p>All steps described below assumes you are already in the <code>TrainCheck-Evaluation-Workloads</code> repo. If not, clone the repository and go to it. <pre><code>git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git\ncd TrainCheck-Evaluation-Workloads\n</code></pre></p> <ol> <li> <p>Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide.</p> </li> <li> <p>Install necessary dependencies for the false positive evaluation workloads.     <pre><code>conda activate traincheck # change this if you installed TrainCheck in a different environment.\ncd fp_rate\npip3 install -r requirements.txt\n</code></pre></p> </li> <li> <p>Execute <code>ae_fp.py</code> to collect traces, perform invariant inference, and check the invariants on validation programs.</p> <p>The workload <code>ddp-multigpu</code> will need 2 GPUs. We have provided the trace for <code>ddp-multigpu</code> in case you do not have two GPUs.</p> <p>If you need to use our pre-computed trace for <code>ddp-multigpu</code>, remove the <code>--overwrite-existing-results</code> argument. <pre><code>python3 ae_fp.py --bench workloads\n</code></pre></p> <p>Or, if you have a machine with 2 GPUs, execute the below command, such that the original results will be re-computed. <pre><code>python3 ae_fp.py --bench workloads --overwrite-existing-results\n</code></pre></p> </li> <li> <p>Execute <code>compute_fp_rate.py</code> to compute the false positive rates.</p> <pre><code>python3 compute_fp_rate.py\n</code></pre> </li> </ol>"},{"location":"ae-eval-s5.4-fp-rate/#what-to-expect-during-execution","title":"\ud83d\udc40 What to Expect During Execution","text":"<p>The <code>ae_fp.py</code> script is long running. It performs three tasks at same time.  1. It collects trace for all the workloads. 2. It infers invariants for three setups in Section 5.4. 3. It checks inferred invariants on the validation workloads.</p> <p>The experiments might fail if environment installation issues or disruption happens. When you run into problems, please refer to \u26a0\ufe0f Notes &amp; Troubleshooting.</p>"},{"location":"ae-eval-s5.4-fp-rate/#notes-troubleshooting","title":"\u26a0\ufe0f Notes &amp; Troubleshooting","text":"<p>The script will automatically detect any errors in any (1) trace collection, (2) inference tasks, (3) checking tasks. If you encounter any trace collection issues, please check for any missing environment dependencies.</p> <p>If you encounter any issues on invariant inference tasks or invariant checking tasks, please try to rerun the experiment by adding <code>--overwrite-existing-results</code> or delete all <code>trace_*</code> folders except for <code>trace_ddp-multigpu</code>.</p> <p>If you see persistent issues, it will likely be a environment issue or software bug. Please contact us for help.</p>"},{"location":"ae-eval-s5.4-fp-rate/#how-to-verify-the-results","title":"\ud83e\uddd0 How to verify the results?","text":"<p>The <code>compute_fp_rate.py</code> script generates a file called <code>fp_rates.csv</code> under the current directory. Looking like this</p> <pre><code>setup,fp_rate\n1-input,0.3105\n4-input,0.1127\n6-input,0.1066\n</code></pre> <p>These values correspond to the results reported in Section 5.4 of the paper. You should verify that the false positive rates are similar or lower. Since the OSDI submission, we have fixed multiple bugs in TrainCheck, so the false positive rates are expected to be significantly lower in most cases.</p> <p>In our run of the script, we obtained the following results: <pre><code>setup,fp_rate\n1-input,0.039\n4-input,0.021\n6-input,0.015\n</code></pre></p>"},{"location":"ae-eval-s5.5-perf-overhead/","title":"Eval: Performance Overhead","text":"<p>\u23f3 Estimated Completion Time: 15 minutes.</p>"},{"location":"ae-eval-s5.5-perf-overhead/#goal","title":"\ud83c\udfaf Goal","text":"<p>This evaluation measures the runtime overhead introduced by TrainCheck\u2019s instrumentation compared to un-instrumented runs across a set of representative ML workloads, during the invariant checking stage. The results correspond to Section 5.5 of the paper.</p>"},{"location":"ae-eval-s5.5-perf-overhead/#resources-scripts","title":"\ud83d\udcc2 Resources &amp; Scripts","text":"<p>Files described below are all in the TrainCheck-Evaluation-Workloads repo.</p> <ul> <li>Automation Scripts:</li> <li> <p><code>performance_overhead/ae_perf.sh</code>: End-to-end script for running the performance overhead benchmarks (Section 5.5) and generating Figure 7. It internally calls:</p> <ul> <li><code>run_all.xsh</code>: Runs the experiments and collects raw data (per-iteration duration).</li> <li><code>analysis.xsh</code>: Analyzes the raw data and prepares input for plotting.</li> <li><code>plot_e2e.py</code>: Plots the final results.</li> </ul> </li> <li> <p>Workloads (You won't need to touch this):</p> <ul> <li>Located in overhead-e2e</li> </ul> </li> <li> <p>The deployed 100 invariants:     eval_scripts/perf_benchmark/overhead-e2e/sampled_100_invariants.json</p> </li> </ul>"},{"location":"ae-eval-s5.5-perf-overhead/#how-to-run","title":"\ud83d\udee0 How to Run","text":"<ol> <li>Make sure you have a working TrainCheck installation by following TrainCheck Installation Guide.</li> </ol> <p>All steps described below assumes you are already in the <code>TrainCheck-Evaluation-Workloads</code> repo. If not, clone the repository and go to it. <pre><code>git clone https://github.com/OrderLab/TrainCheck-Evaluation-Workloads.git\ncd TrainCheck-Evaluation-Workloads\n</code></pre></p> <ol> <li> <p>Execute <code>ae_perf.sh</code>.</p> <pre><code>conda activate traincheck\ncd performance_overhead\n\nbash ae_perf.sh\n</code></pre> </li> </ol>"},{"location":"ae-eval-s5.5-perf-overhead/#expected-output","title":"\ud83e\uddd1\u200d\ud83d\udcbb Expected Output","text":"<p>After execution completes, a plot will be generated at <code>performance_ae.pdf</code>. All the raw data are stored at a folder named <code>perf_res_ae</code>.</p>"},{"location":"ae-eval-s5.5-perf-overhead/#how-to-verify","title":"\ud83e\uddd0 How to Verify","text":"<ul> <li>Open the generated file performance_ae.pdf and compare it against Figure 7 in the paper.</li> <li>Small differences in the overhead numbers (within \u00b120%) are expected. TrainCheck\u2019s overhead is sensitive to CPU performance, since trace serialization is blocking and CPU-bound.</li> <li>Despite minor variations, the key takeaway should remain clear: TrainCheck\u2019s selective instrumentation incurs significantly lower overhead compared to other methods.</li> </ul>"},{"location":"ae-eval-s5.5-perf-overhead/#notes-troubleshooting","title":"\u26a0\ufe0f Notes &amp; Troubleshooting","text":"<ol> <li> <p>Do Not Run Other GPU Tasks in Parallel</p> <p>For stable performance measurements, the evaluation scripts will periodically terminate all CUDA processes to ensure a clean environment.  Please avoid running any other GPU workloads during this evaluation.</p> </li> <li> <p>Handling Failed Workloads</p> <p>If an end-to-end workload fails: - Navigate to the corresponding workload folder. - Manually rerun it using: <pre><code>traincheck-collect --use-config --config md-config-var.yml -i ../sampled_100_invariants.json\n</code></pre> - If the issue does not reproduce consistently, simply delete the result folder and rerun the full benchmark. - If the failure is consistent, please contact us for support.</p> </li> </ol>"},{"location":"ae/","title":"TrainCheck Artifact Evaluation Guide","text":"<p>Welcome to the artifact evaluation guide for TrainCheck (OSDI'25). This document outlines the procedures needed to reproduce our results and guides you through the key experiments presented in the paper.</p> <p>Note: We may update both the main TrainCheck repository and the evaluation workloads repository during the evaluation period. Please make sure to pull the latest version of each repository before proceeding.</p>"},{"location":"ae/#checklist","title":"\u2705 Checklist","text":"<ul> <li>[ ] Environment set up (Python, dependencies, 2 CUDA GPUs with \u2265 12GiB memory each)</li> <li>[ ] Installed <code>xonsh</code> via <code>pip3 install 'xonsh[full]'</code> in the conda environment</li> <li>[ ] Ran Silent Issue Detection experiment</li> <li>[ ] Ran Invariant Transferability evaluation</li> <li>[ ] Ran False Positive Rate evaluation</li> <li>[ ] Ran Performance Overhead measurement</li> <li>[ ] Verified outputs match expected results (tolerances noted per experiment)</li> </ul>"},{"location":"ae/#resources-you-need","title":"\ud83d\udcce Resources You Need","text":"<p>In addition to this guide, you will need the following resources throughout the evaluation process:</p> <ol> <li>5-Minute Tutorial \u2014 A quick walkthrough that introduces TrainCheck\u2019s workflow using a real-world bug.</li> <li>TrainCheck Installation Guide \u2014 Step-by-step instructions for setting up TrainCheck.</li> <li>Technical Usage Guide \u2014 Detailed documentation on how to use TrainCheck, configure instrumentation, and interpret outputs.</li> <li>Evaluation Workloads Repository \u2014 Contains all evaluation workloads and automation scripts used in the experiments.</li> </ol>"},{"location":"ae/#overview","title":"Overview","text":"<p>TrainCheck is an invariant-based tool for detecting silent correctness issues in PyTorch training pipelines.</p> <p>This artifact enables reproduction of the four main evaluation results from the paper:</p> <ul> <li>Silent Issue Detection (Section 5.1)</li> <li>Invariant Transferability (Section 5.3)</li> <li>False Positive Rate (Section 5.4)</li> <li>Performance Overhead (Section 5.5)</li> </ul> <p>To get familiar with TrainCheck, we recommend starting with the 5-Minute Tutorial, which walks you through detecting a real-world bug from Section 5.1.</p>"},{"location":"ae/#recommended-evaluation-order","title":"\u23f1\ufe0f Recommended Evaluation Order","text":"<p>We suggest running the evaluations in the following order, based on automation level and runtime requirements:</p> <ol> <li>Kick the tires \u2013 5 min tutorial with TrainCheck</li> <li>Performance Overhead (~10 minutes)</li> <li>False Positive Rate (~1.5 hours)</li> <li>Transferability (~30 minutes)</li> <li>Silent Issue Detection (~ variate, should be able to finish within one day)</li> </ol>"},{"location":"ae/#environment-requirements","title":"Environment Requirements","text":"<p>Many of our experiment scripts are written in xonsh, a shell that combines Python and Bash. Please install it with:</p> <pre><code>conda activate traincheck\npip3 install 'xonsh[full]'\n</code></pre> <p>For a full and efficient AE experience, we recommend the following setup: - \ud83d\udda5 1 machine with 2\u00d7 CUDA-enabled GPUs - Each GPU should have at least 12\u202fGiB memory. - Compatible with CUDA 11.8 or 12.1 - \ud83e\udde0 32 host memory (recommended)</p>"},{"location":"ae/#recommended-hardware-chameleon-cloud","title":"\ud83d\udd27 Recommended Hardware: Chameleon Cloud","text":"<p>Most experiments require 2\u00d7 CUDA-enabled GPUs with support for CUDA 11.8+. While some workloads can run on GPUs with as little as 2\u202fGiB memory, the main experiments (e.g., Section 5.1) benefit from higher-capacity GPUs.</p> <p>We recommend using the <code>compute_liqid</code> node type on Chameleon Cloud:</p> <ul> <li> <p>\u2705 <code>liqid01</code> and <code>liqid02</code>:   These nodes each have 2\u00d7 A100 GPUs (40\u202fGiB) and allow you to reproduce all results in the paper.</p> </li> <li> <p>\ud83c\udd97 Other <code>compute_liqid</code> nodes with 1\u00d7 A100 GPU:   These are sufficient for all single-GPU experiments and let you reproduce ~90% of results.</p> </li> </ul> <p>Please consult the estimated runtimes in each evaluation section before making reservations. \u23f1\ufe0f If working full-time on the artifact, 2 days should be sufficient, but we recommend reserving at least 5 days to allow for possible setup delays or debugging.</p>"},{"location":"ae/#software-notes","title":"Software Notes","text":"<ol> <li> <p>If you\u2019re using Chameleon instances:</p> <ul> <li>Please start your machine with an Ubuntu 22.04 image that includes recent GPU drivers.</li> <li>We recommend using the <code>CC-Ubuntu22.04-CUDA</code> OS image.</li> </ul> </li> <li> <p>Follow Installation Guide to install TrainCheck.</p> </li> </ol> <p>\u23ed\ufe0f Once your environment is set up, we recommend starting with the 5-Minute Tutorial with TrainCheck. It will help you get familiar with the workflow and also verify that your installation is working correctly.</p>"},{"location":"ae/#kick-the-tires-try-traincheck-in-5-minutes","title":"\ud83d\ude80 Kick-the-Tires: Try TrainCheck in 5 Minutes","text":"<p>Get started quickly by using TrainCheck to detect and diagnosis a real-world bug report: PyTorch-FORUM-84911.</p> <p>See details in 5-min-tutorial.</p>"},{"location":"ae/#start-full-artifact-evaluation","title":"\ud83d\udcca Start Full Artifact Evaluation","text":"<p>Follow the below specific instructions to reproduce our evaluation results:</p> <ol> <li>Section 5.5: Performance Overhead</li> <li>Section 5.4: False Positives</li> <li>Section 5.3: Invariant Transferability</li> <li>Section 5.1: Silent Issue Detection</li> </ol>"},{"location":"benchmarks/","title":"Performance Benchmarks","text":"<p>Latest benchmark results (updated: 2026-02-13 03:22:28 UTC, commit: a9fe391)</p>"},{"location":"benchmarks/#end-to-end-performance-impact","title":"End-to-End Performance Impact","text":"<p>These benchmarks are automatically generated from the TrainCheck-Benchmarks repository.</p>"},{"location":"check/","title":"TrainCheck Checker Usage Guide","text":"<p><code>traincheck-check</code> is the final stage of the TrainCheck workflow. It verifies a set of invariants against trace files or streams from target programs, reporting any detected violations\u2014helping you catch silent issues in your ML training pipelines.</p>"},{"location":"check/#checking-modes","title":"\ud83d\udd27 Checking Modes","text":"<p>TrainCheck supports two checking modes:</p> <ul> <li> <p>Post-training Checking (<code>traincheck-check</code>):    Perform invariant checking on completed trace files after the training job finishes. \u2705</p> </li> <li> <p>On-the-fly Checking (<code>traincheck-onlinecheck</code>):    Perform real-time checking while the target training job is running. \u2705</p> </li> </ul>"},{"location":"check/#how-to-use-on-the-fly-checking","title":"How to Use: On-the-fly Checking","text":"<p>While training is in progress with <code>traincheck-collect</code>, run the following command:</p> <pre><code>traincheck-onlinecheck -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt;\n</code></pre> <ul> <li><code>-f &lt;trace_folder&gt;</code>: Path to the folder where traces are:</li> <li>Already collected, or</li> <li> <p>Actively being collected by <code>traincheck-collect</code> during the training job.</p> </li> <li> <p><code>-i &lt;path_to_invariant_file&gt;</code>: Path to the JSON file containing inferred invariants.</p> </li> </ul>"},{"location":"check/#how-to-use-post-training-checking","title":"How to Use: Post-training Checking","text":"<p>Run the following command:</p> <pre><code>traincheck-check -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt;\n</code></pre> <ul> <li><code>-f &lt;trace_folder&gt;</code>: Path to the folder containing traces collected by <code>traincheck-collect</code>.</li> <li><code>-i &lt;path_to_invariant_file&gt;</code>: Path to the JSON file containing inferred invariants.</li> </ul>"},{"location":"check/#report-visualization-options","title":"Report Visualization Options","text":"<p>Both checkers can produce a standalone HTML report and optionally log summary metrics to external monitoring tools.</p>"},{"location":"check/#standalone-html-report-default","title":"Standalone HTML Report (default)","text":"<ul> <li>Output: <code>&lt;output_dir&gt;/report.html</code></li> <li>Includes summary counts, relation breakdown, and top violations.</li> <li>Disable with <code>--no-html-report</code>.</li> </ul> <p>Offline example <pre><code>traincheck-check -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt;\n</code></pre></p> <p>Online example <pre><code>traincheck-onlinecheck -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt;\n</code></pre></p>"},{"location":"check/#wb-integration","title":"W&amp;B Integration","text":"<p>Enable with <code>--report-wandb</code>. You can also pass: <code>--wandb-project</code>, <code>--wandb-entity</code>, <code>--wandb-run-name</code>, <code>--wandb-group</code>, <code>--wandb-tags</code>.</p> <pre><code>traincheck-check -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt; \\\n  --report-wandb --wandb-project &lt;project&gt;\n</code></pre> <pre><code>traincheck-onlinecheck -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt; \\\n  --report-wandb --wandb-project &lt;project&gt;\n</code></pre>"},{"location":"check/#mlflow-integration","title":"MLflow Integration","text":"<p>Enable with <code>--report-mlflow</code>. Optional: <code>--mlflow-experiment</code>, <code>--mlflow-run-name</code>.</p> <pre><code>traincheck-check -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt; \\\n  --report-mlflow --mlflow-experiment &lt;experiment&gt;\n</code></pre> <pre><code>traincheck-onlinecheck -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt; \\\n  --report-mlflow --mlflow-experiment &lt;experiment&gt;\n</code></pre>"},{"location":"check/#online-report-refresh","title":"Online Report Refresh","text":"<p>The online checker refreshes the report when violations change, and also on a periodic timer. Control the interval with <code>--report-interval-seconds</code> (default: 10).</p> <pre><code>traincheck-onlinecheck -f &lt;trace_folder&gt; -i &lt;path_to_invariant_file&gt; \\\n  --report-interval-seconds 30\n</code></pre> <p>Note: W&amp;B and MLflow logging are optional. If the packages are not installed, TrainCheck will skip logging and emit a warning.</p>"},{"location":"check/#interpreting-the-results","title":"Interpreting the Results","text":"<p>After running either checking mode, TrainCheck will output a summary of detected invariant violations. Each violation entry typically includes:</p> <ul> <li>Trace file or stream name: Identifies where the issue was found.</li> <li>Invariant description: Details the specific invariant that was violated.</li> <li>Violation details: Provides context, such as the step or epoch where the violation occurred.</li> </ul> <p>Review these results to pinpoint silent errors or unexpected behaviors in your ML training pipeline. For more information on result formats and how to diagnose issues, see 5. Detection &amp; Diagnosis in the 5-Minute Tutorial.</p>"},{"location":"infer/","title":"Invariant Inference &amp; Representation","text":"<p><code>traincheck-infer</code> is part of the inference stage of the TrainCheck workflow. It consumes trace files collected from correct training runs and infers behavioral invariants that describe expected runtime behavior. These invariants are later used by <code>traincheck-check</code> to detect violations in other training pipelines.</p>"},{"location":"infer/#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<ul> <li>\ud83d\udd27 Basic Usage</li> <li>\u2699\ufe0f Advanced Usage</li> <li>\ud83d\udcd8 Invariant Concepts</li> <li>\ud83e\uddea Guidelines: Choosing Input Pipelines</li> <li>\ud83e\udde0 Tips: Performance and Stability</li> <li>\ud83d\udd17 Next Step</li> </ul>"},{"location":"infer/#basic-usage","title":"\ud83d\udd27 Basic Usage","text":"<p>In most cases, you only need to specify one or more folders (generated by <code>traincheck-collect</code>) containing trace files using the <code>-f</code> or <code>--trace-folders</code> flag:</p> <pre><code>traincheck-infer -f ./traincheck_mnist_trace ./traincheck_84911_trace ..\n</code></pre> <p>You can provide multiple folders to aggregate traces from different correct runs or programs. This helps TrainCheck generalize better and avoid overfitting to any single pipeline, reducing false positives during checking\u2014especially when the inferred invariants are applied to unrelated or structurally different pipelines.</p> <p>This command will infer invariants from all trace folders provided, and output invariants into <code>invariants.json</code>.</p>"},{"location":"infer/#advanced-usage","title":"\u2699\ufe0f Advanced Usage","text":"<p>traincheck-infer provides additional flags for customization and debugging. Some concepts such as \"relation\" will be explained later.</p> <ol> <li><code>-o, --output</code>: Specify a custom file name for the invariants.</li> <li><code>--disable-relation</code> / <code>--enable-relation</code>: Control which types of invariants to infer. This is useful for reducing noise or targeting specific checks.     <pre><code># Disable ordering-based invariants\ntraincheck-infer -f ./traces --disable-relation FunctionLeadRelation FunctionCoverRelation\n\n# Enable only contain and variable consistency invariants\ntraincheck-infer -f ./traces --enable-relation APIContainRelation ConsistencyRelation\n</code></pre>     &gt; See traincheck.invariant.relation_pool for a complete list of invariants.</li> <li><code>-b, --backend</code>: Select the data processing engine for trace handling.<ul> <li><code>pandas</code> (default): stable and well-tested.</li> <li><code>polars</code>: faster for large traces (experimental)</li> <li><code>dict</code>: pure Python dictionary backend (experimental)</li> </ul> </li> </ol> <p>Other flags (e.g. <code>--debug</code>, <code>-t --traces</code>) are available via traincheck-infer --help, but are rarely needed unless you are debugging or developing TrainCheck itself.</p>"},{"location":"infer/#invariant-concepts","title":"\ud83d\udcd8 Invariant Concepts","text":"<p>TrainCheck infers invariants \u2014 logical properties that are consistently held during correct training runs. These invariants are used to define the expected behavior of a training pipeline, and later help detect silent issues when applied to other runs.</p> <p>Each invariant describes a specific pattern of behavior observed in the trace, such as: - Attribute changes during a function call (e.g., <code>.grad</code> becomes <code>None</code> in <code>zero_grad()</code>) - Ordering relationships between API calls (e.g., <code>zero_grad()</code> should occur before <code>step()</code>) - Consistency among values across different parameters (e.g., shared parameters should have the same value across devices during distributed training)</p>"},{"location":"infer/#invariant-representation","title":"Invariant Representation","text":"<p>An invariant is defined by three things: 1. relation: the relationship this invariant encodes, can be viewed as an invariant template. Each relation has a separate inference algorithm defined (e.g., ConsistencyRelation.infer) 2. params: descriptors for entities that should obey the relationship. 3. precondition: a logical predicate defining the context when an invariant can be applied.</p> <p>In the actual json representation of invariants in the <code>traincheck-infer</code> output, an invariant looks like this.</p> <pre><code>{\n  \"text_description\": \"torch.optim.optimizer.Optimizer.zero_grad contains VarChangeEvent torch.nn.Parameter, pre_value: non_zero, post_value: None\",\n  \"relation\": \"APIContainRelation\",\n  \"params\": [\n    {\n      \"param_type\": \"APIParam\",\n      \"api_full_name\": \"torch.optim.optimizer.Optimizer.zero_grad\"\n    },\n    {\n      \"param_type\": \"VarTypeParam\",\n      \"var_type\": \"torch.nn.Parameter\",\n      \"attr_name\": \"grad\",\n      \"pre_value\": \"non_zero\",\n      \"post_value\": null\n    }\n  ],\n  \"precondition\": {\n    \"parent_func_call_pre\": {\n      \"inverted\": true,\n      \"preconditions\": [\n        {\n          \"clauses\": [\n            {\n              \"type\": \"constant\",\n              \"prop_name\": \"meta_vars.step\",\n              \"additional_path\": \"None\",\n              \"prop_dtype\": \"int\",\n              \"values\": [\n                0\n              ]\n            }\n          ]\n        },\n        {\n          \"clauses\": [\n            {\n              \"type\": \"constant\",\n              \"prop_name\": \"meta_vars.stage\",\n              \"additional_path\": \"None\",\n              \"prop_dtype\": \"str\",\n              \"values\": [\n                \"init\",\n                \"testing\"\n              ]\n            }\n          ]\n        }\n      ]\n    }\n  },\n  \"num_positive_examples\": 200,\n  \"num_negative_examples\": 1\n}\n</code></pre> <p>This invariant encodes the expectation that calling torch.optim.optimizer.Optimizer.zero_grad() should reset gradients \u2014 that is, the .grad attribute of torch.nn.Parameter objects should transition from a non-zero value to null (i.e., None or missing). - text_description:</p> <pre><code>A human-readable summary of the invariant.\n&gt; Note: This field is generated using a best-effort strategy and may not fully reflect the invariant\u2019s semantics. In some cases, it may be missing or incomplete. \ud83d\udcc6 We are planning to further formalize this field in the future.\n</code></pre> <ul> <li> <p>relation: \"APIContainRelation\"</p> <p>An event is expected to happen within the duration of an API invocation.</p> </li> <li> <p>params:</p> <ul> <li>An API call: <code>zero_grad()</code> on a PyTorch optimizer</li> <li>An attribute: <code>.grad</code> on a <code>torch.nn.Parameter</code>, which should change from a non-zero value (<code>\"pre_value\": \"non_zero\"</code>) to null (<code>\"post_value\": null</code>) during the call</li> </ul> </li> <li> <p>precondition:     This invariant only applies outside the following contexts:</p> <ul> <li>The first step of training (<code>meta_vars.step == 0</code>)</li> <li>The init or testing stages (<code>meta_vars.stage in {\"init\", \"testing\"}</code>) <p>These are specified as inverted preconditions, meaning the invariant does not apply during those times (e.g., it\u2019s okay to not clear .grad on the first step when nothing has been backpropagated yet).</p> </li> </ul> </li> <li> <p>num_positive_examples: 20     This behavior was observed and confirmed 200 times in the reference traces.</p> </li> <li> <p>num_negative_examples: 1     The invariant failed once \u2014 in this case, during the first training iteration, when .grad had not yet been populated before the zero_grad() call.     &gt; \ud83c\udfaf This behavior is expected and correctly handled by the precondition, which excludes step 0.</p> </li> </ul>"},{"location":"infer/#invariant-inference-workflow","title":"Invariant Inference Workflow","text":"<p>At a high level, TrainCheck performs invariant inference in three stages:</p> <ol> <li> <p>Hypothesis Generation</p> <p>For each supported relation type, TrainCheck scans the provided traces and generates hypotheses by identifying patterns where a potential invariant could exist (i.e., when matching examples are observed).</p> </li> <li> <p>Example Collection</p> <p>For every hypothesis, TrainCheck performs a full scan across all provided traces to gather positive examples (where the hypothesized invariant holds) and negative examples (where it does not).</p> </li> <li> <p>Precondition Deduction</p> <p>TrainCheck analyzes the collected examples to infer a distinguishing predicate\u2014a logical condition that holds true for all positive examples and false for negative ones. This predicate becomes the invariant\u2019s precondition, reducing false positives during checking.</p> </li> </ol> <p>\u2699\ufe0f For full details on the inference algorithms, please refer to our OSDI\u201925 paper (documentation is in progress).</p>"},{"location":"infer/#practical-guidelines-choosing-input-pipelines","title":"\ud83e\uddea Practical Guidelines: Choosing Input Pipelines","text":"<p>When selecting input pipelines for invariant inference, there are two main considerations:</p> <ol> <li> <p>Representativeness</p> <p>You want your input pipelines to be diverse enough to infer a representative set of invariants. This helps: - Avoid overfitting to specific patterns. - Ensure that inferred invariants and preconditions remain accurate across varying scenarios.</p> <p>For example, if none of your input pipelines use mixed precision, TrainCheck might infer invariants like:</p> <p>\"For mathematical operations, the output dtype must equal the input dtype.\"</p> <p>However, if mixed precision pipelines are included, TrainCheck will refine such invariants by adding preconditions like:</p> <p>\"This applies only when a torch.autocast context manager is not active.\"</p> <p>\u26a1 How many pipelines should you include? It depends on how different your target pipeline is from available reference pipelines: - If the target is a minor variant of a known-good pipeline, using just that reference may suffice. - If the target pipeline introduces new frameworks, tasks, or architectures, include a broader set of inputs to improve generalization.</p> </li> <li> <p>Inference Time</p> <p>Inference time is generally not a major concern, since inference happens offline. However, due to the repetitive nature of training loops, you can safely shorten reference runs without sacrificing invariant quality.</p> <p>In practice: - For all bugs detected by TrainCheck so far, we limited inference traces to at most 100 iterations. - Shortened runs have shown no significant impact on the usefulness or accuracy of inferred invariants.</p> </li> </ol>"},{"location":"infer/#core-principles-a-summary","title":"Core Principles \u2013 A Summary","text":"<ul> <li>Focus on the diversity of input traces \u2014 capturing different configurations, behaviors, or modes of operation.</li> <li>The length or size of traces matters far less.</li> <li>Efficient inference is achievable with short, representative runs.</li> </ul>"},{"location":"infer/#implementation-limitations","title":"Implementation Limitations","text":"<p>TrainCheck operates on large traces with a dynamic schema, where variable types and fields can change over time. This, combined with the need for cross-trace comparisons, limits the use of typical data storage solutions like SQL databases or optimized DataFrame libraries (e.g., Polars), which require fixed schemas.</p> <p>To handle this, we use in-process Pandas DataFrames backed by NumPy. While effective, this approach is currently single-threaded due to Python\u2019s GIL, leaving room for future performance improvements.</p> <p>We are exploring options such as shared-memory DataFrames, schema standardization, or schemaless databases (e.g., MongoDB) if data transmission overhead proves manageable.</p> <p>Note: While data sharding could improve parallelism, it would overcomplicate cross-trace and cross-time analysis and is better handled at the storage layer rather than within inference logic.</p>"},{"location":"installation-guide/","title":"Installation Guide","text":""},{"location":"installation-guide/#compatibility","title":"Compatibility","text":"<ul> <li>Python: 3.10+ (due to reliance on type annotations)</li> <li>PyTorch: 1.7.0\u20132.5.0 (other versions have not been tested.)</li> <li>CUDA: 11.2\u201312.1 (also supports MPS on macOS; see Performance note below)  </li> <li>Operating Systems: Ubuntu\u00a020.04+, macOS. Windows is untested but may work\u2014please file an issue if you hit a problem.</li> </ul> <p>Performance note: On non\u2011CUDA backends (e.g., MPS), runtime overhead can vary due to differences in tensor\u2011hashing efficiency. We\u2019re actively measuring and tuning across platforms.</p>"},{"location":"installation-guide/#installation-steps","title":"Installation Steps","text":"<p>Note: Example workloads are verified on Python\u00a03.10 and PyTorch\u00a02.2.2\u00a0+\u00a0CUDA\u00a012.1. If you\u2019re not reproducing our benchmarks, feel free to install any supported versions.</p> <p>AEC note: For full artifact evaluation, we recommend Ubuntu\u00a022.04 with two Nvidia Ampere\u2011class GPUs (\u2265\u00a012\u00a0GiB GPU memory each). For the 5\u2011minute tutorial, any Linux or macOS (Apple Silicon) laptop will do.</p> <ol> <li> <p>Install Conda   Install Miniconda by following the official Miniconda guide.</p> </li> <li> <p>Create &amp; activate a Python 3.10 Conda Env <pre><code>conda create -n traincheck python=3.10 -y\nconda activate traincheck\n</code></pre></p> </li> <li> <p>Install PyTorch 2.2.2 with CUDA support <pre><code>pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n</code></pre></p> <p>If your GPU does not support CUDA12, CUDA11.8 is also acceptable.</p> <pre><code>pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>If you don't have a CUDA-enabled GPU, just install the CPU version and skip step 4.</p> <pre><code>pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu\n</code></pre> </li> <li> <p>(CUDA platforms only) Install cudatoolkit <pre><code>conda install cudatoolkit\n</code></pre></p> </li> <li> <p>Clone &amp; install TrainCheck <pre><code>git clone https://github.com/OrderLab/TrainCheck.git\ncd TrainCheck\npip3 install .\n</code></pre></p> </li> <li> <p>Verify Installation     You should now have three clis installed in your system. Do a quick test to see of these commands are available and functional.     <pre><code>traincheck-collect --help\ntraincheck-infer --help\ntraincheck-check --help\n</code></pre></p> </li> </ol>"},{"location":"installation-guide/#next-steps","title":"Next Steps","text":"<ul> <li> <p>5\u2011Minute TrainCheck Experience   Follow the 5\u2011Minute Tutorial to instrument a script, infer invariants, and catch silent bugs in under five minutes.</p> </li> <li> <p>Technical Documentation   Explore the TrainCheck Technical Doc for a comprehensive guide to features, configuration, and advanced workflows.</p> </li> </ul>"},{"location":"instr/","title":"Instrumentation &amp; Trace Representation","text":"<p><code>traincheck-collect</code> is the starting point of TrainCheck's workflow. It instruments your PyTorch training script to capture runtime behavior, generating detailed execution traces for later invariant inference and issue detection.</p> <p>This document explains how to use <code>traincheck-collect</code> effectively. TrainCheck dynamically wraps key PyTorch APIs and monitors model states\u2014no modifications to your original training code are required.</p> <p>Use <code>traincheck-collect</code> when you need to: - Generate traces from reference pipelines for invariant inference. - Collect traces from target pipelines to detect silent issues using pre-inferred invariants.</p>"},{"location":"instr/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>\ud83d\udd27 Basic Usage</li> <li>Configuration File Example</li> <li>Running traincheck-collect</li> <li>Selective Instrumentation for Checking</li> <li>Output Structure</li> <li>Overriding Configuration via CLI</li> <li>Adding Meta Variables to Traces</li> <li>How Meta Variables Improve Inference</li> <li>Examples of Useful Meta Variables</li> <li>How to Annotate Meta Variables</li> <li>Trace Representation</li> <li>Instrumentation Mechanisms</li> <li>Advanced Usage </li> <li>Algorithms Overview </li> <li>Troubleshooting &amp; FAQs </li> </ol>"},{"location":"instr/#basic-usage","title":"\ud83d\udd27 Basic Usage","text":"<p><code>traincheck-collect</code> requires three types of input:</p> <ol> <li>Python script to instrument.</li> <li>Launch arguments (if any) for executing the script.</li> <li>Instrumentation-specific configurations.</li> </ol> <p>You can provide these inputs either directly via the command line or through a configuration file. \u25b6\ufe0f Recommendation: Use a configuration file for clarity and reusability.</p> <p>Here\u2019s an example configuration:</p> <pre><code>pyscript: ./mnist.py        # Python entry point of your training program.\nshscript: ./run.sh          # [Optional] Shell script to launch with custom arguments or environment setup.\nmodules_to_instr:           # Libraries to instrument. Defaults to ['torch'] if omitted.\n  - torch\nmodels_to_track:            # [Optional] Variable names of models to track. Leave empty to disable model tracking.\n  - model\nmodel_tracker_style: proxy  # [Optional] Tracking method: \"proxy\" (default), \"subclass\", or \"sampler\".\ncopy_all_files: false       # [Optional] Set true if your code relies on relative paths (e.g., local datasets/configs).\n</code></pre> <p>You can find example configurations and training programs in:     \u2022   MNIST Example     \u2022   GPT-2 Pretrain Example</p> <p>Run TrainCheck trace collection with:  </p> <pre><code>traincheck-collect --use-config --config &lt;path-to-config-file&gt;\n</code></pre> <p>This command instruments the specified libraries and model variables, then executes your program. (Details on instrumentation mechanisms and limitations will follow in the next section. TODO)</p>"},{"location":"instr/#selective-instrumentation-for-checking","title":"Selective Instrumentation for Checking","text":"<p>When checking for silent issues, <code>traincheck-collect</code> supports selective instrumentation to improve efficiency. Simply provide the invariants file:</p> <pre><code>traincheck-collect --use-config --config &lt;path-to-config&gt; --invariants &lt;path-to-inv-file&gt;\n</code></pre> <p>TrainCheck will automatically adjust instrumentation granularity based on the provided invariants.</p>"},{"location":"instr/#output-structure","title":"Output Structure","text":"<p>By default, TrainCheck creates a folder named:</p> <pre><code>traincheck_run_&lt;pyscript_name&gt;_&lt;instr_libs&gt;_&lt;timestamp&gt;\n</code></pre> <p>This folder contains: - Collected traces - Instrumented scripts and execution logs (if the program completes successfully)</p> <p>You can also provide any additional arguments not specified in the configuration through the commandline interface, such as</p>"},{"location":"instr/#overriding-configuration-via-cli","title":"Overriding Configuration via CLI","text":"<p>You can override or supplement configuration settings by providing additional arguments directly via the command line. For example:</p> <pre><code># Write trace files to ./trace_training instead of using the default auto-generated folder name\ntraincheck-collect --use-config --config &lt;path-to-config-file&gt; --output-dir trace_training\n</code></pre> <p>To view all available command-line arguments and configuration options, run:</p> <pre><code>traincheck-collect --help\n</code></pre> <p>Note: When using a configuration file, replace hyphens (-) in argument names with underscores (_). For example: - Command-line: <code>--output-dir trace_training</code> - Configuration file: <code>output_dir: trace_training</code></p>"},{"location":"instr/#adding-meta-variables-to-traces","title":"Adding Meta Variables to Traces","text":"<p>You can enhance your traces by providing custom meta variables\u2014semantic information about your program's execution. These annotations improve the quality and precision of inferred invariants by offering context that might not be directly observable from raw traces.</p> Learn how meta variables improve invariant inference  TrainCheck infers **preconditions** for each invariant\u2014these are predicates that distinguish between positive and negative examples in the trace.   - A **positive example** is a trace segment where the invariant holds.   - A **negative example** is where it is violated.  Many invariants are inherently **conditional**, meaning they only hold true under certain contexts (e.g., during training but not initialization). TrainCheck tries to automatically discover such conditions.  However, trace data alone may lack sufficient context. This is where **meta variables** come in\u2014they inject semantic hints (like execution phase or step number) to guide smarter inference."},{"location":"instr/#examples-of-useful-meta-variables","title":"\u2728 Examples of Useful Meta Variables","text":"<ol> <li><code>stage</code> \u2014 Indicates whether a trace record belongs to initialization, training, or evaluation.</li> <li><code>step_id</code> \u2014 The current training step or iteration number.</li> <li>Custom arguments \u2014 Any domain-specific flags or parameters relevant to your training logic.</li> </ol>"},{"location":"instr/#how-to-annotate-meta-variables","title":"How to Annotate Meta Variables","text":"<p>\ud83d\udccc [To Be Documented] Instructions for defining and injecting meta variables into traces will be provided in a future update.</p>"},{"location":"instr/#trace-representation","title":"Trace Representation","text":"<p>\ud83d\udccc [To Be Documented] </p>"},{"location":"instr/#instrumentation-mechanisms","title":"Instrumentation Mechanisms","text":"<p>\ud83d\udccc [To Be Documented] Details about TrainCheck\u2019s instrumentation strategies, supported APIs, and limitations will be covered here later.</p>"},{"location":"successful-stories/","title":"\u2705 TrainCheck: Real-World Success Stories","text":"<p>TrainCheck proactively detects silent failures in deep learning training by inferring and checking invariants. Below are real-world cases where TrainCheck caught critical bugs that would have otherwise wasted months of compute and effort.</p> <p>This page highlights several silent errors that TrainCheck detected in real-world scenarios. For a comprehensive list of issues and detailed analysis, see our research paper: Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks.</p>"},{"location":"successful-stories/#case-1-silent-weight-divergence-in-bloom-176b","title":"\ud83e\udde8 Case 1: Silent Weight Divergence in BLOOM-176B","text":"<p>The Story: While training the BLOOM-176B model, a subtle optimizer bug caused model weights to silently diverge across GPUs. All standard metrics and logs appeared normal, masking the critical issue.</p> <ul> <li>The Risk: 3.5 months of training time on 384 A100 GPUs, with invalid checkpoints.</li> <li>The Delay: It took developers 15 days to notice and diagnose the problem.</li> <li>TrainCheck's Role: TrainCheck would have instantly detected this divergence with its parameter consistency invariant, saving the project from a massive setback.</li> </ul> <p>Source: BigScience BLOOM-176B Training Chronicles</p>"},{"location":"successful-stories/#case-2-silent-gradient-application-failure","title":"\ud83e\udde0 Case 2: Silent Gradient Application Failure","text":"<p>The Story: A user reported their model performance degrading over time, even though the gradient norm seemed stable. The community suspected issues with learning rates, data, or hardware.</p> <ul> <li>The Root Cause: Gradients were not being applied to the model weights due to incorrect logic in a multi-GPU wrapper.</li> <li>TrainCheck's Role: TrainCheck immediately flagged the root cause, revealing that despite gradient calculations, no actual model updates were happening.</li> </ul> <p>Source: Community Discussion on X</p>"},{"location":"successful-stories/#case-3-the-flat-loss-mystery","title":"\u2753 Case 3: The Flat Loss Mystery","text":"<p>The Story: A user experienced a completely flat loss curve, indicating the model was not learning at all. The cause was unclear, with suspicions pointing to the model architecture or optimizer configuration.</p> <ul> <li>The Root Cause: The model and optimizer were incorrectly wrapped for Fully Sharded Data Parallel (FSDP) training, preventing <code>optimizer.step()</code> from updating model parameters.</li> <li>TrainCheck's Role: TrainCheck identified the problem instantly by verifying that <code>zero_grad()</code> and <code>step()</code> calls resulted in zero actual model changes.</li> </ul> <p>Source: HuggingFace Accelerate Issue #2665</p>"},{"location":"successful-stories/#case-4-the-bug-that-taught-me-pytorch","title":"\ud83e\udde9 Case 4: The Bug That Taught Me PyTorch","text":"<p>The Story: A training run looked healthy at first glance, but the model was not learning as expected. The symptoms were subtle and resembled the classic 84911-style failure mode.</p> <ul> <li>The Root Cause: A silent training logic issue that prevented expected parameter updates under certain conditions.</li> <li>TrainCheck's Role: TrainCheck can surface this by checking that critical API calls actually lead to parameter state changes.</li> </ul> <p>Source: the bug that taught me more about PyTorch than years of using it</p>"},{"location":"technical-doc/","title":"TrainCheck Documentation","text":"<p>\ud83d\ude9c This documentation is under construction. We welcome any feedback or questions through GitHub Issues or our Discord server.</p> <p>TrainCheck is a lightweight, invariant-based instrumentation and analysis tool for identifying silent correctness issues in PyTorch training pipelines. It infers behavioral invariants from correct reference runs (e.g., official examples or clean configurations), then checks other scripts for behavioral violations. TrainCheck is designed to be minimally intrusive\u2014requiring no code modifications or rewrites of training logic.</p>"},{"location":"technical-doc/#system-overview","title":"\ud83d\udd27 System Overview","text":"<p>TrainCheck consists of three core command-line utilities:</p> <ol> <li>traincheck-collect \u2013 Instruments a training pipeline and collects trace logs.</li> <li>traincheck-infer \u2013 Infers behavioral invariants from the collected traces.</li> <li>traincheck-check \u2013 Checks new traces against a set of inferred invariants to detect silent issues.</li> </ol> <p>TrainCheck workflows are organized into two stages:</p> <ol> <li> <p>\ud83e\uddea Inference Stage</p> <ul> <li>traincheck-collect collects execution traces from reference training pipelines.</li> <li>traincheck-infer analyzes traces and produces invariants that describe correct/expected runtime behavior.</li> </ul> </li> <li> <p>\ud83d\udea8 Checking Stage</p> <ul> <li>traincheck-collect is used again to trace the target (possibly buggy) pipeline.</li> <li>traincheck-check verifies whether the collected trace violates any of the known invariants.</li> </ul> </li> </ol>"},{"location":"technical-doc/#pre-inferred-invariants-on-the-roadmap","title":"\ud83d\udce6 Pre-Inferred Invariants (On the Roadmap)","text":"<p>In common use cases, users typically do not need to infer invariants manually. TrainCheck provides a high-quality set of pre-inferred invariants that work out-of-the-box with popular libraries such as PyTorch, HuggingFace Transformers, and DeepSpeed.</p> <p>You may still want to run inference in the following cases: - When using certain niche or uncommon features not covered by the default invariants. - When working with custom training stacks outside supported libraries. - When you want to increase specificity by inferring invariants from a set of related, known-good pipelines (e.g. in industrial settings).</p>"},{"location":"technical-doc/#component-documentation","title":"\ud83d\udcda Component Documentation","text":"<p>Each utility is documented separately:</p> <ul> <li> <p>Collecting Traces with traincheck-collect     Usage, instrumentation caveats, and trace file format.</p> </li> <li> <p>Inferring Invariants with traincheck-infer CLI usage, performance considerations, invariant format, and the inference algorithm (relations, preconditions, etc.).</p> </li> <li> <p>Checking Violations with traincheck-check How to apply invariants to new traces, result interpretation, and result file formats.</p> </li> </ul>"},{"location":"usage-guide/","title":"\ud83e\uddea TrainCheck: Usage Guide","text":"<p>TrainCheck helps detect and diagnose silent errors in deep learning training runs\u2014issues that don't crash your code but silently break correctness.</p>"},{"location":"usage-guide/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Check out the 5-minute guide for a minimal working example.</p>"},{"location":"usage-guide/#common-use-cases","title":"\u2705 Common Use Cases","text":"<p>TrainCheck is useful when your training process doesn\u2019t converge, behaves inconsistently, or silently fails. It can help you:</p> <ul> <li>Monitor long-running training jobs and catch issues early</li> <li>Debug finished runs and pinpoint where things went wrong</li> <li>Sanity-check new pipelines, code changes, or infrastructure upgrades</li> </ul> <p>TrainCheck detects a range of correctness issues\u2014like misused APIs, incorrect training logic, or hardware faults\u2014without requiring labels or modifications to your training code.</p> <p>While TrainCheck focuses on correctness, it\u2019s also useful for ruling out bugs so you can focus on algorithm design with confidence.</p>"},{"location":"usage-guide/#tips-for-effective-use","title":"\ud83e\udde0 Tips for Effective Use","text":"<ol> <li> <p>Use short runs to reduce overhead.    If your hardware is stable, you can validate just the beginning of training. Use smaller models and fewer iterations to speed up turnaround time.</p> </li> <li> <p>Choose good reference runs for inference. </p> </li> <li>If you have a past run of the same code that worked well, just use that.</li> <li>You can also use small-scale example pipelines that cover different features of the framework (e.g., various optimizers, mixed precision, optional flags).</li> <li> <p>If you're debugging a new or niche feature with limited history, try using the official example as a reference. Even if the example is not bug-free, invariant violations can still highlight behavioral differences between your run and the example, helping you debug faster.</p> </li> <li> <p>Minimize scale when collecting traces. </p> </li> <li>Shrink the pipeline by using a smaller model, running for only ~10 iterations, and using the minimal necessary compute setup (e.g., 2 nodes for distributed training).</li> </ol>"},{"location":"usage-guide/#current-limitations","title":"\ud83d\udea7 Current Limitations","text":"<ul> <li> <p>Eager mode only. TrainCheck instrumentor currently works only in PyTorch eager mode. Features like <code>torch.compile</code> are disabled during instrumentation.</p> </li> <li> <p>Not fully real-time (yet). Invariant checking is semi-online. Full real-time support is planned but not yet available.</p> </li> </ul>"},{"location":"assets/examples/traincheck-collect/gpt2-pretrain-config/","title":"Index","text":"<p>Language model pretraining script from the official examples of the transformers library. Trains GPT-2 on </p> <p>Modifications: 1. 10 steps per training/testing epoch. 2. stage annotations 3. skip instrumentation for the tokenization step</p>"}]}